<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlece17f0a456a89a36.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Segmentation | Y. Liu </title> <meta name="author" content="Y. Liu"> <meta name="description" content="Image Segmentation, including Hough Transform"> <meta name="keywords" content="ku, ucph, copenhagen, diku, portfolio-website, liuying, dk, yingliu, ying liu, liu ying"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?157bbd74cef60250fd5a67a2e078966e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liuying-1.github.io/blog/2023/segmentation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Segmentation",
            "description": "Image Segmentation, including Hough Transform",
            "published": "March 21, 2023",
            "authors": [
              
              {
                "author": "Ying Liu",
                "authorURL": "https://di.ku.dk/Ansatte/forskere/?pure=da/persons/762476",
                "affiliations": [
                  {
                    "name": "DIKU, UCPH",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Y.</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Segmentation</h1> <p>Image Segmentation, including Hough Transform</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#segmentation-overview">Segmentation Overview</a> </div> <ul> <li> <a href="#segmentation-approaches">Segmentation Approaches</a> </li> </ul> <div> <a href="#hough-transform">Hough Transform</a> </div> <ul> <li> <a href="#line">Line</a> </li> <li> <a href="#circle">Circle</a> </li> <li> <a href="#segmentation-evaluation">Segmentation Evaluation</a> </li> </ul> <div> <a href="#machine-learning-based-segmentation">Machine Learning based segmentation</a> </div> <ul> <li> <a href="#supervised-learning">Supervised Learning</a> </li> <li> <a href="#convolution-neural-network">Convolution Neural Network</a> </li> <li> <a href="#fully-convolutional-networks">Fully Convolutional Networks</a> </li> </ul> </nav> </d-contents> <div align="center"><img src="https://i.imgur.com/z0885iZ.jpeg" alt="1551680875610_.pic" style="zoom:80%;"></div> <div class="reference" style="color: #999; font-size: 0.8em; margin-top: 1em; margin-bottom: 1em; ">Reference: Lecture from <a href="https://di.ku.dk/english/staff/?pure=en/persons/254908" _target="blank" rel="external nofollow noopener" target="_blank">Stefan Horst Sommer</a> </div> <h2 id="segmentation-overview">Segmentation Overview</h2> <div align="center"><img src="https://i.imgur.com/IUmbCe9.png" alt="image-20230321111305719" style="zoom:33%;"></div> <p>In this example, you have an image that is filled with a lot of triangles. And basically, we wanna <strong><u>extract those coherent structures from the image</u></strong> and there are multiple ways of doing that.</p> <p>Another example is an image with a collection of coins. Here you can actually see that you have a uniform background where the foreground actually varies in intensity.</p> <div align="center"><img src="https://i.imgur.com/cekguer.png" alt="image-20230321111514252" style="zoom:25%;"></div> <p>More complicated patterns are textures. Here, for instance, this is a collection of trees trunk.</p> <div align="center"><img src="https://i.imgur.com/OtCGHmA.png" alt="image-20230321111704852" style="zoom: 25%;"></div> <p>And in many cases, some of these scenes are quite complicated. So here is an example over here. You can see that you have a complicated scene of a horse back rider with a horse. There’s a flag here. There is a selection of cars up here. There’s a shrubbery and so on.</p> <div align="center"><img src="https://i.imgur.com/rFKCqS9.png" alt="image-20230321111900741" style="zoom: 50%;"></div> <p>So there’s of course different interpretations of this.</p> <p>The ground truth here annotated you can see that you regard the shorse as a separate object and the rider is separate object. The cars are separate objects but the same type. And you can also see this a spectator or something marked here. (Just a person here)</p> <p>The results from two different types of segmentation FCN-8s and the other one.</p> <p>From FCN-8s, you can see that you very clearly get the horse and the rider. It doesn’t have the same fidelity as a ground truth, but it’s pretty close.</p> <p>While SDS here, you can see that you get some of the horse, you get some of the rider, and you also get the cars, but it’s not clear. So =&gt; It is not easy.</p> <hr> <p>The next example here you can see two motorbikes.</p> <p>And again you have the bikes same class. And then, you have the humans. Spectators, the riders and the spectators. You can see here that the two methods, the convolution neural network here does a really good job. But it defines the bikes and the riders whereas spectators are more shady.</p> <p>In SDS case, you can see that the riders and the bikes are melded together and you find some structure in the background, but that is not really the spectators.</p> <hr> <p>The next one here you can see another difficult example you have a sheep. And a wire fence.</p> <p>Whether this method is capable of reasonably delineating the sheep but completely misses out on the wireframe. And the other method, here is a capable of capturing most of the sheep but due to the wire for the wire frame, it’s simply cutoffs some stuff here.</p> <hr> <p>The final one is a boat.</p> <p>In the FCD you can see that you have three classes and curious where they come from but at least given the ground truth.</p> <p>Here you can see that the top of the boat which is basically water here. =&gt; SDS not work.</p> <hr> <p>Other examples include segmentation of anatomical structures. So this is really common in medical image analysis. So in this case, we are interested in two specific regions of the brain.</p> <p>One is Ventricle and the other one is the left cuneus and what you can see here is that it’s not necessarily an easy task depending on what kind of method that you use.</p> <div align="center"><img src="https://i.imgur.com/p6K72fU.png" alt="image-20230321114210885" style="zoom: 33%;"></div> <p>A more complicated scenarios when you have multiclasses in this case we know there’s 132 classes.</p> <div align="center"><img src="https://i.imgur.com/XD2y0pv.png" alt="image-20230321114656540" style="zoom: 33%;"></div> <p>This is a more complicated nature.</p> <p>Another example which is also well described in the literature is the segmentation of Hippocampus for AD diagnosis/prognosis.</p> <div align="center"><img src="https://i.imgur.com/JrWU27S.png" alt="image-20230321114906910" style="zoom: 33%;"></div> <p>So the hippocampus is the structure that is related to memory. When you get Alzheimer’s, what will happen is that you can observe that people basically use the ability to remember things and it’s strongly correlated with a reduction in the hippocampus.</p> <p>Another example here is knee segmentation and here we are interested in the cartilage. Actually this is the particular intersection between the two cartilage sheets and either side of the knee.</p> <div align="center"><img src="https://i.imgur.com/VJzDmXj.png" alt="image-20230321115242548" style="zoom: 33%;"></div> <p>And here you can see a complete reconstruction of the two different sheets of cartilage.</p> <p>Another example is segmentation of airways.</p> <div align="center"><img src="https://i.imgur.com/4fejT0w.png" alt="image-20230321115530862" style="zoom:30%;"></div> <p>So, from a CT image, basically extracting the airways and classify them into branches. This is the evaluation of people belong to disease. This is probably very hot with respect to the current Corona virus.</p> <hr> <p>So the learning goal is defining the segmentation problem, so actually sort of confining, what is it really that you want to do?</p> <ul> <li>Constrast local/edge/region-based and semantic segmentation</li> <li>Select appropriate segmentation methods</li> <li>implement simple segmentation algorithms</li> <li>define and implement line Hough transform</li> <li>formulate the segmentation problem in a machine learning setting</li> <li>use machine learning based segmentation</li> <li>solve a specific real-life segmentation problem</li> </ul> <hr> <h3 id="segmentation-approaches">Segmentation Approaches</h3> <p>So a really good example is …..</p> <p>If you think of edges of zebras here, the segmentation was used to animal is a inherentely diffcult problem. Because you can see we have a lot of it, just not only the outline of the animal itself, but due to the texture of the animals. Region based methods or methods of general are easily fool and you can even see that’s a background here has been intuitive in the segmentation.</p> <div align="center"><img src="https://i.imgur.com/zA2zF86.png" alt="image-20230321120729252" style="zoom: 25%;"></div> <h4 id="intensity-based-segmentation">Intensity based Segmentation</h4> <p>So, if we turn ourselves to a grayscale. If we just assume a simple thresholding.</p> <div align="center"><img src="https://i.imgur.com/1woCqA6.png" alt="image-20230321121144138" style="zoom: 25%;"></div> <p>You can see here that if there is a bias through the image, we have a problem right? Because then some of the foreground pixels as you can see here were falling into the intensity space of the background pixels.</p> <p>So what will happen is that we will have basically get a misclassfication over here.</p> <p>A way to get rid of this is simply using some of the methodology that you already know.</p> <p>I will rectify the image, that is basically model this distortion like you see, so that the intensity of the different levels are leveled. In this case, you are interested in the background and foreground.</p> <p>And you can use that to sort of estimate a function that will intensity correct the image so that you have a uniform intensity distribution.</p> <p>If you do that, you will end up with examples more like this.</p> <div align="center"><img src="https://i.imgur.com/shaOKyJ.png" alt="image-20230321121910947" style="zoom: 33%;"></div> <p>But what you can see from this is that, if this is your image, and you blur it. You can see here that you have two distinct peaks which is the background here where the intensities are zero. And you can see here you have the foreground which are very distinct and then you need to choose some sort of threshold in between here.</p> <p>When you select this, it has an effect. So in this case, due to the blurring, you get rounded corners. The objects become smaller and smaller if you go to the right part to choose $\tau$, while if you go to the left part, the objects become larger and larger but it will not sort of reflect the real size.</p> <p>So this is one of the basic problems with intensity based and this is of course based on the resolution of different images.</p> <p>So what you really can consider this as is this will be the original image and this is sort of a low resolution because if that’s been smoothed of this one. And when you do the segmentation, you get artifacts so these are you can see that these convex parts of the shape sort of gets blurred away and concave parts is sort of diminished right. So things tend to be rounded.</p> <p>Another way to do this is to use K-means clustering.</p> <div align="center"><img src="https://i.imgur.com/t7IzaY3.png" alt="image-20230321124355857" style="zoom: 33%;"></div> <p>So assume that you have corrected your image so that you don’t have any bias. You can run K-means clustering on the intensity space.</p> <p>In this case you can see that if we choose 9 clusters, we can see some artifacts of the background out here. And there’s some speckle noise. Well, this brain is divided into this background and in two classes and then you would have seven different classes in here.</p> <p>And here you can see one of the clusters and another output of clusters.</p> <h4 id="edge-based-segmentation">Edge-based Segmentation</h4> <div align="center"><img src="https://i.imgur.com/m15SYk6.png" alt="image-20230321125146780" style="zoom:50%;"></div> <p>So if we take this original image what we have here is a binary image, and there’s a very nicely geives these two peaks. But if you have like we saw in the green image, sort of that biased throught the image, you can see that stalled distribution very clearly.</p> <p>However, there are methods that relies on edge detection that can be used to mitigate this.</p> <p>So, by running an edge filtering, you can see that we can actually get almost the same kind of accuracy in the segmentations through here and this is because the transition become from background to foreground. Can be described by the derivatives of the image and exactly at the edge of this is where the derivatives has the largest magnitude. And if you think of the second order derivative, then that’s the change of the derivative, the ridge of that would be exactly on the edge. So what you are basically get is a sort of a bump right here.</p> <p>And you can use this to extract the edges.</p> <p>So, to extract the contours of the object that relent to edge tracking and what is described int the text you need to read is you will basically search for the highest magnitude of the gradient or this edge detector and then you will basically trace it around. Just searching for the largest and you stop once and you can’t find the neighbor.</p> <div align="center"><img src="https://i.imgur.com/V9c3dHK.png" alt="image-20230321132310162" style="zoom:33%;"></div> <p>So if you want to do the whole outline you have to do like edge tracking, another example of doing this is the watershed, which also will basically track the edges of the object.</p> <div align="center"><img src="https://i.imgur.com/d1gVI4I.png" alt="image-20230321132638404" style="zoom:25%;"></div> <p>Pyramid linking is also a way to sort of create coherence across scales and the way to do this is simply to ….</p> <p>So if you imagine that you first create the gaussian pyramid and the way it’s described in the book is that you have sort of an intrinsic linking right.</p> <p>So at the highest level here, it is result of the blurring of the next level.</p> <div align="center"><img src="https://i.imgur.com/LjJL9Yd.png" alt="image-20230321133416869" style="zoom: 25%;"></div> <font face="noto serif sc">金字塔链接（pyramid linking）是一种在图像和信号处理中常用的技术，用于构建图像金字塔或信号金字塔。金字塔是一种多尺度表示，用于在不同的尺度上分析信号或图像。通过金字塔链接技术，可以将不同尺度的金字塔层级相互连接起来，以提高图像或信号的分析能力。它可以将不同分辨率的图像或信号组合在一起，从而创建一个具有不同分辨率的层次结构。在这个层次结构中，每个级别的图像或信号都是前一个级别的下采样版本。</font> <font face="noto serif sc">在图像处理中，金字塔链接通常是通过将图像进行降采样来实现的，即通过将图像缩小到不同的尺度来构建金字塔层级。每个金字塔层级都包含一个缩小版本的原始图像，而缩小的尺度越高，图像的分辨率就越低。金字塔层级可以通过卷积、滤波、差分等方法来计算，以提取不同尺度的图像特征。Pyramid linking通常用于图像处理中的特征提取和匹配，以及信号处理中的滤波和分析。它可以帮助识别图像中的不同特征，并在不同分辨率的图像或信号中找到相应的特征。这使得它在许多计算机视觉和图像处理应用程序中非常有用，例如目标检测、图像匹配和纹理分析。</font> <h4 id="region-based-segmentation">Region based segmentation</h4> <p>Another method is the <strong>Chan-Vese</strong> which is a segmentation based on active contours, and what you are basically have is a function that continuously moves through the instruments so we start out here by initially a circle.</p> <p>This is actually in fact an implicit function so it’s positive inside and negative outside. That gives us a binary segmentation and what it minimizes is a variant in the outside and the variance in the inside. And because it’s an implicit function, it gets sort of crawl across boundaries as you can see here and feel this. So the solution here found by the Chan-Vese, the one that minimizes the variance outside plus the variance inside.</p> <div align="center"><img src="https://i.imgur.com/GSTfAau.png" alt="image-20230321134734510" style="zoom: 33%;"></div> <p><strong><em>Active contour is the predecessor of Chan-Vese method.</em></strong></p> <div align="center"><img src="https://i.imgur.com/35iV3Vf.png" alt="image-20230321134840903" style="zoom:50%;"></div> <p>The idea was to have a line or contour thta could cling to edges in order to be able to model both concave and convex of the objects and separate them.</p> <p>There is an example that the contours clinging to the edge here and you can then extract it and use a prior that requires a certain smoothness which will make it collapsed back onto the edge of the object.</p> <p>But we usually use Chan-Vese as it already does a very excellent job.</p> <h4 id="registration-based">Registration based</h4> <div align="center"><img src="https://i.imgur.com/iPDuhtQ.png" alt="image-20230321135312444" style="zoom: 33%;"></div> <p>We can think of this as an example method in terms of machine learning. So what you have is you have a collection of atlases.</p> <p>So basically images that have already been segmented at all parts of the structure. Then you perform the registration between the target image with the structure that you’re interested in and the atlases. And then you do a label propagation so that means once these things are placed on top of each other sufficiently. Optimal you propagate the labels from the addresses to the target image and then you can use multiple techniques to combine the labels from the atlases to the final segmentation.</p> <h2 id="hough-transform">Hough Transform</h2> <h3 id="line">Line</h3> <div align="center"><img src="https://i.imgur.com/qBJu2XD.png" alt="image-20230321142325992" style="zoom: 33%;"></div> <p>So the Hough Transform is a mapping of lines from the first domain to the second domain.</p> <p>It’s basically you draw lines in this space, measure the intensity (plot in the second image), that’s the parameterized by the distance of the line to the center of this image plus the slope which you can see here.</p> <p>And from that, when you do that a lot of times, you get poles in the second image which corresponds to an angle.</p> <font face="noto serif sc">霍夫变换（Hough Transform）是一种图像处理技术，用于检测形状在图像中的出现，并将其表示为数学形式。它最初是由霍夫在1962年提出的用于检测直线形状的算法，但后来被拓展用于检测其他形状，如圆形和椭圆形等。</font> <font face="noto serif sc">在第一个图中，白线组成的方框图包含了若干条直线。霍夫变换的主要思想是将这些直线从原始图像空间（x,y）映射到另一个参数空间（θ，ρ），其中θ是直线的角度，ρ是直线到原点的距离。因此，第二个图展示了这些直线在参数空间中的分布情况，每个点代表一条直线。</font> <font face="noto serif sc">在第二个图中，可以看到一些明显的聚集点（也称为“极值”），这些聚集点对应于原始图像中的直线。这些聚集点可以通过设定适当的阈值来筛选出来，从而得到第三个图中的检测到的直线。</font> <font face="noto serif sc">总之，霍夫变换是一种用于检测形状在图像中的出现的技术，它通过将形状从图像空间映射到参数空间来实现。在参数空间中，形状出现的位置可以用极值来表示，这些极值可以用于检测形状并提取相关信息。</font> <div align="center"><img src="https://i.imgur.com/OqrWeSA.png" alt="image-20230321144427216" style="zoom: 22%;"></div> <p>Have an image space looks like this, and through all the points you can draw multiple lines. So, for each line, you get a slope $m$ and a bias $b$. That gives you a unique mapping to the parameter space.</p> <p>What is really interesting in the second image is that the places where these lines intersect is exactly the coefficient of the line that passes through these two points.</p> <p>So, in practice, what is done is similar to what is up (previous example) here where this paramemterized in terms of the distance to the center and the slope in degrees.</p> <p><strong>Steps to do Hough Transformation</strong></p> <p><strong><em>Step 1</em></strong>: Edge detection</p> <p><strong><em>Step 2</em></strong>: Then you need to decide on the number of angles $(ntheta)$ and distances $nd$. So basically parameterize your Hough space. Create accumulator image $H[ntheta, nd]$.</p> <p><strong><em>Step 3</em></strong>: And then you simply start voting for each edge and theta calculate $d$ and increment $(theta, d)$ pixel in accumulator.</p> <p>So at the end, <strong><em>you find peaks in accumulator image.</em></strong></p> <h3 id="circle">Circle</h3> <p><strong>Extension on circle</strong></p> <div align="center"><img src="https://i.imgur.com/mKOcI4N.png" alt="image-20230321150955054" style="zoom:30%;"></div> <p>So, here you extract boundaries of the coins using an edge filter. So you get these here. For instance, you can parameterise this by the distance to the center and the size of the circle.</p> <h3 id="segmentation-evaluation">Segmentation Evaluation</h3> <p><strong><em>Once you have a segmentation, how do we actually evaluate it in this case?</em></strong></p> <p><strong><em>Dice Measure</em></strong></p> \[Dice (A, B) = 2 * |A\cap B|/(|A| + |B|)\] <p>The overlap between $A$ and $B$. How well do they overlap?</p> <h2 id="machine-learning-based-segmentation">Machine Learning based segmentation</h2> <p>So, in a supervised setting, you have $X$ features and $Y$ output, like a regular regression model. And the learning algorithm searches for a mapping from $X\to Y$ in the hypothesis set $H$.</p> <p>The segmentation we would have the full image would be the $X$ image, $Y$ is the segmented image (labelled image).</p> <p>If it’s a patch-based segmentation, it would be a patch of an image and $Y$ could be the class of center of that pixel.</p> <h3 id="supervised-learning">Supervised Learning</h3> <div align="center"><img src="https://i.imgur.com/dSVUZYM.png" alt="image-20230321152153638" style="zoom: 25%;"></div> <p>The idea is that you don’t know your function that takes you from $X \to Y$, but what you do have is you have a set of examples which is called training data. And from that, you try to select from your hypothesis that which is basically there are the set of candidate functions. You try to select the optimal to the final hypothesis.</p> <h3 id="convolution-neural-network">Convolution Neural Network</h3> <div align="center"><img src="https://i.imgur.com/3gAxVpr.png" alt="image-20230321152507729" style="zoom:30%;"></div> <h3 id="fully-convolutional-networks">Fully Convolutional Networks</h3> <div align="center"><img src="https://i.imgur.com/QTZTtLI.png" alt="image-20230321152645148" style="zoom:33%;"></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"LiuYing-1/liuying-1.github.io","data-repo-id":"R_kgDOMScbNQ","data-category":"Announcements","data-category-id":"DIC_kwDOMScbNc4Cgruw","data-mapping":"pathname","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Y. Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JC70RZ57BT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JC70RZ57BT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"CV",description:"Here is a brief overview of my academic and professional background, and hope there is something interesting for you. The more comprehensive version can be accessed by clicking the PDF icon on right top corner of this page.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-line-segment-intersection",title:"Line Segment Intersection",description:"Self-learning note for CG - Line Segment Intersection.",section:"Posts",handler:()=>{window.location.href="/blog/2024/line-segment-intersection/"}},{id:"post-graduation",title:"Graduation",description:"Congratulations to my graduation from the DIKU, University of Copenhagen.",section:"Posts",handler:()=>{window.location.href="/blog/2024/graduation/"}},{id:"post-review-of-swav",title:"Review of SwAV",description:"This is the learning of the paper SwAV.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/swav.pdf"}},{id:"post-review-of-simsiam",title:"Review of SimSiam",description:"This is the learning of the paper SimSiam.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/SimSIAM.pdf"}},{id:"post-extracts-from-byol",title:"Extracts from BYOL",description:"Review of the paper BYOL and to extract the main points",section:"Posts",handler:()=>{window.location.href="/blog/2024/byol/"}},{id:"post-cookbook-reading-1",title:"Cookbook Reading - 1",description:"This is the first part of this book.",section:"Posts",handler:()=>{window.location.href="/blog/2024/cookbook-1/"}},{id:"post-advanced-image-registration",title:"Advanced Image Registration",description:"Self-learning note for advanced image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/advanced-image-registration/"}},{id:"post-image-registration-basics",title:"Image Registration Basics",description:"Learning note for medical image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/image-registration-l1/"}},{id:"post-segmentation-basics",title:"Segmentation Basics",description:"Learning note for medical image segmentation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation-basics/"}},{id:"post-inpainting",title:"Inpainting",description:"Learning note for inpainting.",section:"Posts",handler:()=>{window.location.href="/blog/2023/inpainting/"}},{id:"post-magnetic-resonance",title:"Magnetic Resonance",description:"Preview of the lecture for MRI.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-mr/"}},{id:"post-x-ray-and-ct",title:"X-ray and CT",description:"This is the first lecture of the course Medical Image Analysis at UCPH.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-l1/"}},{id:"post-dansk-gt-0307",title:"Dansk &gt; 0307",description:"This is the learning note for my first lesson at Ucplus.",section:"Posts",handler:()=>{window.location.href="/blog/2023/dansk-week-1/"}},{id:"post-dictation-gt-the-art-of-balancing-stones",title:"Dictation &gt; The Art of Balancing Stones",description:"This is a daily dictation 2 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-art-of-balancing-stones/"}},{id:"post-dictation-gt-the-egg",title:"Dictation &gt; The Egg",description:"This is a daily dictation 1 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-egg/"}},{id:"post-neural-network-ii-mlops",title:"Neural Network II | MLOps",description:"This is the lecture for Neural Network II and MLOps.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ml-ops/"}},{id:"post-neural-network",title:"Neural Network",description:"Recap of Neural Network",section:"Posts",handler:()=>{window.location.href="/blog/2023/neural-network-recap/"}},{id:"post-easter-lunch",title:"Easter Lunch",description:"This is the first time to have lunch with my Danish family at Easter.",section:"Posts",handler:()=>{window.location.href="/blog/2023/easter-lunch/"}},{id:"post-segmentation",title:"Segmentation",description:"Image Segmentation, including Hough Transform",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation/"}},{id:"post-features",title:"Features",description:"Image feature detection and matching with application.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/features.pdf"}},{id:"post-transformation",title:"Transformation",description:"Affine, Rigid, Perspective linear transformations, etc.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/geometry.pdf"}},{id:"post-deconvolution",title:"Deconvolution",description:"Image Restoration by Deconvolution",section:"Posts",handler:()=>{window.location.href="/blog/2023/deconvolution/"}},{id:"post-histogram",title:"Histogram",description:"Thresholding segmentation and histogram techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2023/histogram/"}},{id:"post-fourier",title:"Fourier",description:"Complex numbers and Fourier transformation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/fourier/"}},{id:"post-filtering",title:"Filtering",description:"Linear and non-linear filtering, derivative operators.",section:"Posts",handler:()=>{window.location.href="/blog/2023/filtering/"}},{id:"post-convolution",title:"Convolution",description:"Pixel-wise Operations, Intensity, Transformations, Image Formation, and the Convolution Integral.",section:"Posts",handler:()=>{window.location.href="/blog/2023/convolution/"}},{id:"post-app-and-term",title:"App and Term",description:"Introduction of the Signal and Image Processing course, including the basic concepts, terminology, and applications.",section:"Posts",handler:()=>{window.location.href="/blog/2023/intro/"}},{id:"post-aads-grade",title:"AADS Grade",description:"This is to record I got 12 on the course Advanced Algorithms and Data Structures.",section:"Posts",handler:()=>{window.location.href="/blog/2023/aads-grade/"}},{id:"post-polygon-triangulation",title:"Polygon Triangulation",description:"This is the learning note for Polygon Triangulation.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/polygon.pdf"}},{id:"post-approximation",title:"Approximation",description:"This is the learning note for Approximation algorithm - 1.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/approx1.pdf"}},{id:"post-exact-parameterized",title:"Exact-Parameterized",description:"This is the learning note for Exact-Parameterized algorithm.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/EE.pdf"}},{id:"post-npc",title:"NPC",description:"This is the learning note for NPC.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/NPC.pdf"}},{id:"post-van-emde-boas-tree",title:"van Emde Boas Tree",description:"This is the learning note for vEB.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/van-Emde-Boas-Trees.pdf"}},{id:"post-sad-mood",title:"Sad Mood",description:"This blog is to record my current sad mood.",section:"Posts",handler:()=>{window.location.href="/blog/2023/sad-mood/"}},{id:"post-hashing",title:"Hashing",description:"This is the learning note for Hashing.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Hashing.pdf"}},{id:"post-randomized-algorithms",title:"Randomized Algorithms",description:"This is the learning note for the Randomized Algorithms.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/RA.pdf"}},{id:"post-linear-programming",title:"Linear Programming",description:"This is the learning note for the Linear Programming.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Linear-programming.pdf"}},{id:"post-max-flow",title:"Max-flow",description:"This is the learning note for the Max-flow.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Max-flow.pdf"}},{id:"post-recovery-gt-crash",title:"Recovery &gt; Crash",description:"This is the special case of the recovery when the system crashes.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-crash/"}},{id:"post-recovery-gt-normal",title:"Recovery &gt; Normal",description:"This is the special part for recovery and normal.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-normal/"}},{id:"post-experiment-recovery",title:"Experiment | Recovery",description:"Here are the chapters of Experiments and Recovery.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recover/"}},{id:"post-concourrency-2",title:"Concourrency 2",description:"This is the chapter relating to concurrency control - Part 2.",section:"Posts",handler:()=>{window.location.href="/blog/2023/concurrency-2/"}},{id:"post-concourrency-1",title:"Concourrency 1",description:"This is the chapter relating to concurrency control.",section:"Posts",handler:()=>{window.location.href="/blog/2022/concurrency/"}},{id:"post-godt-nyt\xe5r",title:"Godt Nyt\xe5r",description:"The blog is to record my first New Year&#39;s Eve in Denmark!",section:"Posts",handler:()=>{window.location.href="/blog/2022/godt-nytar/"}},{id:"post-incorrect-url",title:"Incorrect URL",description:"Last time, my neighbor, whose world ranking in CSGO is the Global Elite, received a Christmas gift by his nisse.",section:"Posts",handler:()=>{window.location.href="/blog/2022/incorrect-url/"}},{id:"post-rpc-performance",title:"RPC | Performance",description:"RPC and Permance are the two topics we will cover today.",section:"Posts",handler:()=>{window.location.href="/blog/2022/rpc/"}},{id:"post-abstractions",title:"Abstractions",description:"Basic concepts of computer systems.",section:"Posts",handler:()=>{window.location.href="/blog/2022/abstractions/"}},{id:"post-tivoli-firework",title:"Tivoli Firework",description:"To accompany Jianxiang Yu, we went to the second oldest theme park worldwide, Tivoli in Denmark yesterday.",section:"Posts",handler:()=>{window.location.href="/blog/2022/tivoli-firework/"}},{id:"post-royal-guardian",title:"Royal Guardian",description:"The shift of the royal guardian in Marmorkirken.",section:"Posts",handler:()=>{window.location.href="/blog/2022/royal-gardian/"}},{id:"post-swan-nearby",title:"Swan Nearby",description:"A record to keep this swan in my memory.",section:"Posts",handler:()=>{window.location.href="/blog/2022/swan-nearby/"}},{id:"post-chef-first-time",title:"Chef First Time",description:"This is my first time being the chef to cook for my danish family!",section:"Posts",handler:()=>{window.location.href="/blog/2022/chief-first-time/"}},{id:"post-travel-to-malm\xf6",title:"Travel to Malm\xf6",description:"This is the final version of the image preprocessing with the limitation that cannot automatically crop the image in the center. However, it can work properly in normal situations. Also, this blog is used to record my trip to Malm\xf6, Sweden with Xuanlang.",section:"Posts",handler:()=>{window.location.href="/blog/2022/travel-to-malmo/"}},{id:"post-dev-progress",title:"Dev Progress",description:"In this version, the development of my blog website is already passed half. \u8fd9\u91cc\u662f\u4e2d\u6587\u5b57\u4f53\u6d4b\u8bd5\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2022/dev-progress/"}},{id:"post-take-metro",title:"Take Metro",description:"This blog is to record my route from DIKU back to my dorm at 00:53 am several days ago.",section:"Posts",handler:()=>{window.location.href="/blog/2022/take-metro/"}},{id:"post-start-my-blog",title:"Start My Blog",description:"Memorize the start of the implementation of my blog-web.",section:"Posts",handler:()=>{window.location.href="/blog/2022/start-my-blog/"}},{id:"news-i-have-defended-my-master-thesis-quot-exploration-of-self-supervised-learning-methods-for-longitudinal-image-analysis-quot-and-received-my-master-39-s-degree-meanwhile-i-have-been-offered-a-job-as-an-ai-engineer-in-component-ai-aps",title:"I have defended my master thesis **&quot;Exploration of Self-Supervised Learning Methods for Longitudinal Image Analysis&quot;** and received my Master&#39;s Degree. Meanwhile, I have been offered a job as an **AI Engineer** in Component-AI ApS. \ud83c\udf93\ud83c\udf89\ud83d\ude80",description:"",section:"News"},{id:"news-going-to-be-one-of-tas-for-the-course-advanced-deep-learning-in-b4-2024-at-university-of-copenhagen",title:"\ud83e\udd70 Going to be one of TAs for the course **Advanced Deep Learning** in B4-2024 at University of Copenhagen.",description:"",section:"News"},{id:"news-congrats-to-me-for-passing-the-courses-medical-image-analysis-advanced-topics-in-image-analysis-and-project-of-research-in-self-supervised-learning-methods-for-longitudinal-images-with-10-out-of-12",title:"\ud83e\udd73 Congrats to me for passing the courses **Medical Image Analysis**, **Advanced Topics in Image Analysis**, and **Project of Research in Self-Supervised Learning Methods for Longitudinal Images** with **10 out of 12**.",description:"",section:"News"},{id:"news-i-am-happy-to-share-i-got-12-out-of-12-in-the-courses-of-advanced-algorithms-and-data-structures-signal-and-image-processing-and-advanced-deep-learning",title:"\ud83c\udf7b I am happy to share I got **12 out of 12** in the courses of **Advanced Algorithms and Data Structures**, **Signal and Image Processing**, and **Advanced Deep Learning**.",description:"",section:"News"},{id:"news-learning-notes-advanced-algorithms-and-data-structures-https-liuying-1-github-io-blog-tag-aads-signal-and-image-processing-https-liuying-1-github-io-blog-tag-sip-and-advanced-computer-systems-https-liuying-1-github-io-blog-tag-acs-have-been-transferred-from-my-self-developed-website-to-this-one",title:"\ud83d\udcd4 Learning notes [**Advanced Algorithms and Data Structures**](https://liuying-1.github.io/blog/tag/aads), [**Signal and Image Processing**](https://liuying-1.github.io/blog/tag/sip), and [**Advanced Computer Systems**](https://liuying-1.github.io/blog/tag/acs) have been transferred from my self-developed website to this one.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%79%35%38%31%30%39%39@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/liu-ying-463ab925b","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/100085288350390","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>