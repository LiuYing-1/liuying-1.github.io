<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlece17f0a456a89a36.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Neural Network | Y. Liu </title> <meta name="author" content="Y. Liu"> <meta name="description" content="Recap of Neural Network"> <meta name="keywords" content="ku, ucph, copenhagen, diku, portfolio-website, liuying, dk, yingliu, ying liu, liu ying"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Playfair:ital,opsz,wght@0,5..1200,300..900;1,5..1200,300..900&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?157bbd74cef60250fd5a67a2e078966e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liuying-1.github.io/blog/2023/neural-network-recap/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Neural Network",
            "description": "Recap of Neural Network",
            "published": "April 25, 2023",
            "authors": [
              
              {
                "author": "Ying Liu",
                "authorURL": "https://di.ku.dk/Ansatte/forskere/?pure=da/persons/762476",
                "affiliations": [
                  {
                    "name": "DIKU, UCPH",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Y.</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Neural Network</h1> <p>Recap of Neural Network</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#neural-networking-learning">Neural networking learning</a> </div> <ul> <li> <a href="#simple-neuron-models">Simple neuron models</a> </li> <li> <a href="#some-common-activation-functions">Some common activation functions</a> </li> <li> <a href="#simple-neural-network-models">Simple neural network models</a> </li> <li> <a href="#notation">Notation</a> </li> <li> <a href="#sum-of-squres-error">Sum-of-squres error</a> </li> <li> <a href="#back-propagation">Back Propagation</a> </li> <li> <a href="#lecture-written-note">Lecture written-note</a> </li> </ul> <div> <a href="#reference">Reference</a> </div> </nav> </d-contents> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/nnadl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <h3 id="neural-networking-learning">Neural networking learning</h3> <p>Using data to adapt (train) the parameters (weights) of a mathematical model.</p> <h4 id="simple-neuron-models">Simple neuron models</h4> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/G8p6psX.png" alt="image-20230425074015153" style="zoom: 20%;" class="img-fluid rounded z-depth-1"> </div> </div> </div> <div class="caption"> Figure 1. Single neuron with bias (Christian Igel, 2023) </div> <p><strong>Input</strong> is the vector $\boldsymbol{x}$ or $\vec{x}\in \mathbb{R}^d$.</p> \[\boldsymbol{x} = (x_1, ..., x_d)\] <p><strong>Output</strong> of neuron $i$ be denoted by $z_i(\boldsymbol{x})$. Omit dependency on $\boldsymbol{x}$ to keep uncluttered.</p> <ul> <li> <strong>Integration</strong>: Computing weighted sum with bias parameter $w_{i0}\in \mathbb{R}$</li> </ul> \[a_i = \sum^d_{j=1}w_{ij} + w_{i0}\] <ul> <li> <strong>Firing</strong>: Applying transfer function (<strong>activation</strong> function) $h$:</li> </ul> \[z_i = h(a_i) = h\left(\sum^d_{j=1}w_{ij}x_j + w_{i0}\right)\] <p>We have an input with $d$ dimension for the neural network. For each node $i$, the first step is to calculate the integration which accepts all the incoming $x_i$ from $\boldsymbol{x}$ with corresponding weights $w_{ij}$, and then, add the bias $w_{i0}$. The second step is to activate the integration to be its output $z_{i}(\boldsymbol{x})$.</p> <h4 id="some-common-activation-functions">Some common activation functions</h4> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/sqKr3JL.png" alt="image-20230425081429722" style="zoom:25%;" class="img-fluid rounded z-depth-1"> </div> </div> </div> <div class="caption"> Figure 2. Activation functions (Christian Igel, 2023) </div> <ul> <li>Step / threshold:</li> </ul> \[h(a) = \mathbb{I}\{a&gt;0\}\] <ul> <li>Fermi / logistic:</li> </ul> \[h(a) = \frac{1}{1+e^{-a}}\] <ul> <li>Hyperbolic tangens:</li> </ul> \[h(a) = \tanh(a)\] <ul> <li>Alternative sigmoid:</li> </ul> \[h(a) = \frac{a}{1+\vert{a}\vert}\] <ul> <li>Rectified linear unit (ReLU):</li> </ul> \[h(a) = \max(0, a)\] <ul> <li>Exponential LU (ELU):</li> </ul> \[h(a) = \begin{cases} a, &amp; a \geq 0 \\ \alpha (e^a - 1), &amp; a &lt; 0 \end{cases}\] <p>and $\alpha &gt; 0$</p> <h4 id="simple-neural-network-models">Simple neural network models</h4> <p>Neural network (NN) is <strong>a set of connected neurons</strong>.</p> <p>It can be described by a <strong>weighted directed graph</strong>. 1. Neurons are the nodes/<strong>vertices</strong> and numbered by integers $V = {0, 1, 2, …}$. 2. <strong>Connections</strong> between neurons are the <strong>edges</strong> $A$. 3. Strength of connection $(j, i)\in A$ from neuron $j$ to neuron $i$ is described by weight $w_{ij}$. 4. All weights are collected in weight vector $\boldsymbol{w}$.</p> <p>To <strong>Feed-forward NNs</strong>: we <strong>do not allow cycles</strong> in the connectivity graph.</p> <p>NN represents mapping</p> \[f: \mathbb{R}^d \to \mathbb{R}^K\] <p>parameterized by $\boldsymbol{w}: f(\boldsymbol{x; w})_i = \hat{y}_i$</p> <p>In another representation with <font color="red">multiplications \*</font></p> \[f: \mathbb{R}^d \times \mathbb{R}^M \to \mathbb{R}^K\] <p>where $\mathbb{R}^M$ represents weights. <font color="red">Here, we omit the biases as they can be written together with weights?</font> $w_{i0}$</p> <p><u>In other words, $\mathbb{R}^M$ is the vector $w_{i0}, \text{all the weights}$.</u> <font color="red">\*</font></p> <h4 id="notation">Notation</h4> <p>$d$ input neurons, $K$ Output neurons, $M$ hidden neurons:</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/BlS9Upf.jpeg" alt="notation" style="zoom:25%;" class="img-fluid rounded z-depth-1"> </div> </div> </div> <div class="caption"> Figure 3. Explanation of the Notations </div> \[\begin{align} V_{\text{input}} &amp; = \{1, ..., d \} \nonumber\\ V_{\text{hidden}} &amp; = \{d+1, ..., d+M \} \nonumber\\ V_{\text{output}} &amp; = \{d+M+1, ..., d+M+K \nonumber \} \end{align}\] <p>Therefore, the whole set of vertices $V$ is,</p> \[V = \{0 \} \cup V_{\text{input}} \cup V_{\text{hidden}} \cup V_{\text{output}} = \{0, 1, 2, ..., d+M+K \}\] <p>Activation function of neuron $i$ is denoted by $h_i$,</p> \[h_i(a) = a\text{ for } i\in \{0\} \cup V_{\text{input}}\] <p>Typically $h_i \neq h_j$ for $i \in V_{\text{hidden}}$ and $j \in V_{\text{output}}$</p> <p>Neuron $i$ can get only input from neuron $j$ if $j &lt; i$, this ensures that the graph is <strong>acyclic</strong> $\leftarrow$ Feed-Forward NN</p> <p>Activation of neuron $i&gt;1$ is</p> \[a_i = \sum_{(j, i)\in A}w_{ij}z_j\] <p>Output of neuron $i$ is denoted by $z_i$</p> <p>$z_i(a_i) = h_i(a_i)$</p> <p>$z_0(\cdot) = 1$ ($w_{i0}z_0$ is the bias parameter of neuron $i$. <font color="red">Defintion?</font>)</p> <p>$z_i(\cdot) = x_i$ for $i$ for $i\in V_{\text{input}} = {1, …, d }$</p> <p>Output of the network $f(\boldsymbol{x;w})$</p> \[f(\boldsymbol{x;w}) = \pmatrix{z_{M+d+1}\\z_{M+d+2}\\...\\z_{M+d+K}} = \pmatrix{\hat{y_1}\\ \hat{y_2} \\ ... \\ \hat{y_K}}\] <h4 id="sum-of-squres-error">Sum-of-squres error</h4> <p>NN shall learn function</p> \[f: \mathbb{R}^d \to \mathbb{R}^K\] <p>$\implies d$ input neurons, $K$ output neurons</p> <p>Training data</p> \[S = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), ..., (\boldsymbol{x}_N, \boldsymbol{y}_N)\}, \boldsymbol{x}_i \in \mathbb{R}^d, \boldsymbol{y}_i\in \mathbb{R}^K, 1\leq i \leq N\] <p>$# \text{training} = N$.</p> <p>Usually , linear output neurons:</p> \[h_i(a) = a \text{ for $i \in V_{\text{output}}$}\] <p><strong>Sum-of-squares error</strong></p> \[E = \frac{1}{2}\sum^N_{n=1}\Vert f(\boldsymbol{x}_n\boldsymbol{; w}) - \boldsymbol{y}_n\Vert^2\] <h4 id="back-propagation">Back Propagation</h4> <p>Here we define $L$ represents loss, sum of the errors is below,</p> \[E(\boldsymbol{x; w}) = L\left(f(\boldsymbol{x;w}), \boldsymbol{y}\right)\] <p>To minimize $E$ by adjusting the weights $\boldsymbol{w}$, compute the $\nabla_\boldsymbol{w} E$.</p> <p>Below is an concrete example to help understand the <a href="https://zhuanlan.zhihu.com/p/40378224" rel="external nofollow noopener" target="_blank">back propagation by SGD</a></p> <p>The algorithm randomly generates $\boldsymbol{w}$, for each $w$ inside, using the following formula to update it such that coverage to the ground-truth.</p> \[w^+ = w - \eta \cdot \frac{\partial E}{\partial w}\] <p>Here, $\frac{\partial E}{\partial w}$ is the current $\nabla_w E$ which represents the range of the change for $w$ with respect to $E$, $\eta$ is the so-called learning rate (normally less than $0.1$) used to control the step.</p> <p>The reason to compute the gradient is to estimate the true $\boldsymbol{w} \implies \nabla_w E$.</p> <p><strong>Chain Rule</strong></p> <p>Assume $y = g(x), z = f(y)$, then, $z = h(x), h=f\circ g$ where $h$ is a <strong><em>compound function</em></strong>.</p> <p>We know that,</p> \[\frac{d_y}{d_x} = g'(x), \frac{d_z}{d_y}=f'(y)\] <p>By calculus,</p> \[h'(x) = \frac{d_z}{d_x} = \frac{d_z}{d_y} \cdot \frac{d_y}{d_x}\] <p>This is the case of single variable, but it suits for the multiple variables.</p> <p><strong>Example of BP</strong></p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/zOe5Wtp.png" alt="image-20230425104734735" style="zoom:33%;" class="img-fluid rounded z-depth-1"> </div> </div> </div> <div class="caption"> Figure 4. Concrete Feed-Forward NN example for back propagation (HexUp, 2018) </div> <p>Here we have two layers, there are two neurons $x_1 = 1, x_2=0.5$ in the input layer, two neurons $h_1, h_2$ in the hidden layer, and the final neuron $y$ for output layer. Each neuron gets fully connected.</p> <p>Given that the ground-truth values for $\boldsymbol{w}$,</p> \[w_1 = 1, w_2 = 2, w_3 = 3, w_4 = 4, w_5 = 0.5, w_6 = 0.6\] <p>In this setting, the true value for $t = 4$.</p> <p>To mock a process of back propagation, assume we only know $x_1 = 1, x_2 = 0.5$ and the target $t = 4$.</p> <p>First, randomly generates the weights,</p> \[w_1 = 0.5, w_2 = 1.5, w_3 = 2.3, w_5 = 1, w_6 = 1\] <p>Then, conduct the <strong>forward pass</strong>,</p> \[\begin{align} h_1 &amp; = w_1 \cdot x_1 + w_2 \cdot x_2 = 1.25\nonumber \\ h_2 &amp; = w_3 \cdot x_1 + w_4 \cdot x_2 = 3.8\nonumber \\ y &amp; = w_5 \cdot h_1 + w_6 \cdot h_2 = 5.05\nonumber \\ E &amp; = \frac{1}{2}(y-t)^2 = 0.55125\nonumber \end{align}\] <p>Sequently, do the <strong>backward pass</strong>. $y$ is the expected value from NN, and the target true is $t = 4$, we need to update $w_5$.</p> \[\frac{\partial E}{\partial w_5} = \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial w_5}\] <p>Because $E = \frac{1}{2}(t - y)^2$, therefore,</p> \[\begin{align} \frac{\partial E}{\partial y} &amp; = 2 \cdot \frac{1}{2} \cdot (t-y)\cdot (-1)\nonumber \\ &amp; = y - t\nonumber \\ &amp; = 5.05 - 4\nonumber \\ &amp; = 1.05\nonumber \end{align}\] <p>For $y = w_5 \cdot h_1 + w_6 \cdot h_2$,</p> \[\frac{\partial y}{\partial w_5} = h_1 + 0 = h_1 = 1.25\] <p>By the chain rule,</p> \[\begin{align} \frac{\partial E}{\partial w_5} &amp; = \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial w_5} = (y-t)\cdot h_1\nonumber\\ &amp; = 1.05 \cdot 1.25 = 1.3125\nonumber \end{align}\] <p>Updating $w_5$ by using previous formula, where $\eta = 0.1$,</p> \[\begin{align} w_5^+ &amp; = w_5 - \eta \cdot \frac{\partial E}{\partial y}\nonumber \\ &amp; = 1-0.1 \cdot 1.3125\nonumber \\ &amp; = 0.86875 \nonumber \end{align}\nonumber\] <p>Update the $w_6$ in the same steps $\implies w_6^+ = 0.601$</p> <p>Move to $w_1, w_2, w_3, w_4$, take the computation for $\nabla w_1 E$ as an example,</p> \[\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial h_1} \cdot \frac{\partial h_1}{\partial w_1}\] <p>$\frac{\partial E}{\partial y} = y - t$ has already been computed and for $y = w_5 \cdot h_1 + w_6 \cdot h_2$,</p> \[\frac{\partial y}{\partial h_1} = w_5 + 0 = w_5\] <p>By $h_1 = w_1 \cdot x_1 + w_2 \cdot x_2$,</p> \[\frac{\partial E}{\partial w_1} = x_1 + 0 = x_1\] <p>Therefore,</p> \[\frac{\partial E}{\partial w_1} = (y-t) \cdot w_5 \cdot x_1\] <p>Update $w_1$, and similar for the rest.</p> \[w_1^+ = 0.395, w_2^+ = 1.4475, w_3^+ = 2.195, w_4^+ =2.9475\] <p>Conduct the forward pass again,</p> \[y^+ = 3.1768, E^+ = 0.3388 &lt; E\] <p>Repeating the steps above, then that is the back propagation.</p> <p>To get connection with our lectures,</p> \[\begin{align} \nabla_w E = (D_wE)^\text{T}\nonumber \\ D_w E = D_1 L \cdot D_w f\nonumber \end{align}\] <p>Here,</p> \[D_wf = \pmatrix{\frac{\partial f^1}{\partial w^1} &amp; ... &amp; \frac{\partial f^1}{\partial w ^M}\\... &amp; ... &amp; ... \\ \frac{\partial f^K}{\partial w^1} &amp; ... &amp; \frac{\partial f^K}{\partial w^M}}, \text{ Jacobian Matrix}\] <p>We can do the following derivation. We already know that $f(\boldsymbol{x; w})$ can be written like below,</p> \[f(\boldsymbol{x; w}) = \boldsymbol{h}^N\left(\boldsymbol{w}^N \boldsymbol{h}^{N-1}(\boldsymbol{w}^{N-1}...\boldsymbol{h}^1(\boldsymbol{wx}) \right)\] <p>Then, $D_wf$ could be written as,</p> \[D_wf = D\boldsymbol{h}^ND\boldsymbol{w}^ND\boldsymbol{h}^{N-1}....D\boldsymbol{h}^1D\boldsymbol{w}^1\] <p>Each item above is a matrix. Then, compute the $\nabla_wE$.</p> \[\begin{align} \nabla_w E &amp; = (D_w E)^\text{T} = (D f)^\text{T} (D L)^\text{T}\nonumber\\ &amp; = (Dw^1)^\text{T}(Dh^1)^{\text{T}} ... (Dw^ N)^\text{T}(Dh^ N)^\text{T}(DL)^\text{T}\nonumber \end{align}\] <p>As the last weights always need to be computed than the previous, then, that is the reason for calling back propagation.</p> <h4 id="lecture-written-note">Lecture written-note</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/77Ie8QH.jpeg" alt="151711682420321_.pic" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 5. Hand-written from lecture (Stefan, 2023) </div> <h3 id="reference">Reference</h3> <p>Christian Igel. (2023). <em>Neuron Networks</em>. Deep Learning. UCPH</p> <p>HexUp. (2018). <em>Back Propagation（梯度反向传播）实例讲解</em>. 知乎专栏. https://zhuanlan.zhihu.com/p/40378224</p> <p>Stefan Sommer. (2023). <em>Recap of Neural Networks.</em> Advanced Deep Learning. UCPH</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Y. Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JC70RZ57BT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JC70RZ57BT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>