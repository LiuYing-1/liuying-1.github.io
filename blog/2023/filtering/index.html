<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlece17f0a456a89a36.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Filtering | Y. Liu </title> <meta name="author" content="Y. Liu"> <meta name="description" content="Linear and non-linear filtering, derivative operators."> <meta name="keywords" content="ku, ucph, copenhagen, diku, portfolio-website, liuying, dk, yingliu, ying liu, liu ying"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?157bbd74cef60250fd5a67a2e078966e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liuying-1.github.io/blog/2023/filtering/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Filtering",
            "description": "Linear and non-linear filtering, derivative operators.",
            "published": "February 15, 2023",
            "authors": [
              
              {
                "author": "Ying Liu",
                "authorURL": "https://di.ku.dk/Ansatte/forskere/?pure=da/persons/762476",
                "affiliations": [
                  {
                    "name": "DIKU, UCPH",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Y.</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Filtering</h1> <p>Linear and non-linear filtering, derivative operators.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#"></a> </div> </nav> </d-contents> <div class="reference" style="color: #999; font-size: 0.8em; margin-top: 1em; margin-bottom: 1em; ">Reference: Lecture from <a href="https://di.ku.dk/english/staff/vip/researchers_image/?pure=en/persons/83684" _target="blank" rel="external nofollow noopener" target="_blank">Kim Steenstrup Pedersen</a> </div> <h2 id="filtering">Filtering</h2> <p>Convolution is no longer a pixel-wise operation but I operate functions of images that require several pixels in order to compute a value for one pixel.</p> <h3 id="definitions">Definitions</h3> <p><strong>Filtering</strong> is a fundamental part of signal and image processing.</p> <p>It (a <strong>filter</strong>) takes us its input as a signal or image if we like and produce as the output another processed signal or image.</p> <p>It’s a function of an image and its output is an image.</p> <p>Filters on images are defined on <strong>image neighborhoods</strong>.</p> <p>There are different types of filtering, <strong>linear filtering</strong>, and <strong>non-linear filtering</strong>.</p> <p>The most common filters are <strong>linear and shift invariant</strong> (LSI) - i.e. the filter does not depend on position and the filter is a linear operator and is defined by convolution integral and the $h$ (PSF) function.</p> <h3 id="applications-of-filtering">Applications of filtering</h3> <ul> <li> <p><strong>Noise reduction</strong></p> <p>Noise often has high frequency, and it can be reduced by <strong>a low-pass filter</strong> effectively removing high frequencies.</p> </li> <li> <p><strong>Contrast enhancement</strong></p> <p>To improve bright and dark areas in image make it more visible.</p> </li> <li> <p><strong>Sharpening / deblurring / deconvolution</strong></p> <p>If you have a blurred image, you can perform sharpening / debarring / deconvolution. The end goal is to construct a more <strong>sharp version</strong> of the original image.</p> </li> <li> <p><strong>Detection of edges and other features</strong></p> <p>For analysis purposes, for instance, detect something called features which could be intensity <strong>edges</strong> and so on.</p> </li> <li> <p><strong>Recognition by templates</strong></p> <p>Recognize <strong>certain shapes</strong> of parts of an image and one way to do this is using <strong>templates</strong>.</p> </li> <li> <p><strong>Prediction / Recognition / Detection of materials / Objects in images</strong></p> </li> </ul> <h3 id="linear-filtering">Linear filtering</h3> <p><strong>Recap - Linear shift-invariant systems =&gt; the convolution integral</strong></p> <p>A <strong>linear system</strong> is defined by a <strong>linear operator</strong> $S$.</p> <p>An operator $S$ is <strong>linear</strong>, if $S{aX+bY} = aS{X}+bS{Y}$.</p> <p>The linear operator $S$ is defined by the linear superposition integral.</p> <p>We say that the linear system and PSF is <strong>shift-invariant</strong> if</p> \[h(x, y;x',y') = h(x-x', y-y')\] <p>This leads to the <strong>convolution integral</strong>.</p> \[g(x, y) = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty}f(x', y')h(x-x', y-y')dx'dy'\] <h4 id="linear-continuous-filter">Linear continuous filter</h4> <p>Consider an image defined by the function $f(x, y):\Omega \mapsto \R$ where $\Omega \subseteq \R^2$. For simplicity, we consider a 1 channel image, e.g., a real-valued grayscale image.</p> <div style="font-family: 'Noto Serif SC'">两个坐标指向一个像素点的位置中包含的灰度值（简单起见）。</div> <p>A linear filter is defined by an <strong>impulse response</strong> or <strong>PSF</strong> given by the function $h(x, y): {H}\mapsto \R$ where $H \subseteq \R^2$.</p> <p>Applying the linear filter to our image $f$ is done by <strong>convolution</strong></p> \[g(x, y) = \{f*h \}(x, y) = \int\int^{\infty}_{-\infty}f(x', y')h(x-x', y-y')dx'dy'\] <p>The resulting function $g$ is the filter’s response to the image $f$.</p> <p>The support of the image, $\Omega$, and the filter, $H$, does not have to be equal.</p> <table> <tbody> <tr> <td> <strong>Note:</strong> We also need to assume that both functions are square-integrable, $\int\int_{-\infty}^{\infty}</td> <td>f(x, y)</td> <td>^2dxdy &lt; \infty$.</td> </tr> </tbody> </table> <h4 id="discrete-filtering---intuition">Discrete filtering - Intuition</h4> <p>In discrete filtering, we compute a new value for a <strong>target</strong> pixel as a function of the neighboring pixels.</p> <p>An <strong>image neighborhood</strong> is defined by a local window of $N\times M$ pixels that are all connected neighbors.</p> <p>Common to use <strong>square</strong> windows, $N\times N$.</p> <p>An example of a $3\times 3$ pixel neighborhood.</p> <div align="center"><img src="https://i.imgur.com/yzup0Mu.png"></div> <p>In the <strong>neighborhood window</strong> we also need to define a filter <strong>center</strong> which <strong>coincide with the target pixel</strong>.</p> <p>If the window size is odd the center is usually at the center of the filter window. But the center could be anywhere in the window.</p> <p>For instance, if we use NW pixel here, we could consider not the actual neighbors around the target, but the pixels that are to the right and below the target because then, the target would be right here.</p> <h4 id="filtering-with-a-sliding-neighborhood-window">Filtering with a sliding neighborhood window</h4> <p>How we apply the filter to an image is that, we position our neighborhood window somewhere. We assume that our target pixel is where I put the red dot. And that <strong>the center of the mask is also in the middle of the window</strong>. =&gt; So these two things overlap and it means that we are considering all the pixels in this window that we do some function of these pixel values to compute a value for the pixel in the middle.</p> <p>So we are creating a new image where in the new image, the pixel at that location would be a function of all the pixels here in the input. And it’s normal to have this sliding window apprach where you move this window across the image.</p> <p>It’s natural to take stride of 1 pixel so that you move the window by shifting one pixel.</p> <p>But in principle, we could take larger strides. In this case, we’are ending up with the resulting image that comes out of the filter has a lower resolution than the input.</p> <p>If the stride is 2, then, we are halving the width and the height of the image.</p> <div align="center"><img src="https://i.imgur.com/CLpWR2Y.png" alt="image-20230214195900647" style="zoom:67%;"></div> <h4 id="linear-discrete-filtering">Linear discrete filtering</h4> <p>Consider a discrete image defined by the function $f(x, y):\Omega \mapsto \R$ where $\Omega \subseteq \Z^2$. For simplicity we consider a real-valued 1 channel image. $\Z$ represents the integers.</p> <p>A linear filter is defined by an <strong>impulse response</strong> given by the function $h(x, y): H\mapsto \R$ where $H \subseteq \Z^2$.</p> <p>If $h$ has finite support (finite impluse response (RIR)) with neighborhood window size $N\times M$, then applying the linear filter to our image $f$ is done by discrete convolution</p> \[g(x, y) = \{ f*h\}(x, y) = \sum^{\lfloor N/2\rfloor}_{i=-\lfloor N/2\rfloor}\sum^{\lfloor M/2\rfloor}_{j=-\lfloor M/2\rfloor}f(x-i, y-i)h(i, j)\] <p>Notice the convention of flipping $f$ instead of $h$. =&gt; It’s equivalent to either do the shifting in each. They lead to the same result. So when programming, we need to consider whether it’s easier to flip the image or the filter kernel.</p> <h4 id="discrete-filtering-for-one-pixel-in-output-image">Discrete filtering for one pixel in output image</h4> <div align="center"><img src="https://i.imgur.com/7yxuDUZ.png" alt="image-20230214203115471" style="zoom:67%;"></div> <p>We could think of it as we take the block of pixels around the target and then we flip it left to right and upside down as well. Then, we start to do the product.</p> <p>So, image in, we need to define an impulse function $h$ - FIR and represent it with some filter mask / kernel and by applying convolution, we get a new image out =&gt; $g$.</p> <div align="center"><img src="https://i.imgur.com/yMaEPaV.png" alt="image-20230214203919061" style="zoom:50%;"></div> <p>So as before what happens when you do convolution, you consider all the pixels in the image. You consider all the pixels in the image so you slide your filter kernel across the imaeg and that every location you do this weighted sum that gives you a number that you can write into the output image.</p> <h4 id="some-common-linear-filters">Some common linear filters</h4> <p><strong>Example of last week</strong> - Convolution with a continuous shifted box filter $h(x)$.</p> <div align="center"><img src="https://i.imgur.com/vxuNVSj.png" alt="image-20230214204407238" style="zoom:67%;"></div> <p>We have two things, a signal or an image $f$, and an impulse response function $h$ which is called the <strong>box filter</strong>.</p> <p><strong><em>How this box function could be defined?</em></strong></p> <p>So, in 1D, the box function could be defined as,</p> \[h_w(x) = \begin{cases} 1, |x| \leq w/2 \\ 0, otherwise \end{cases}\] <table> <tbody> <tr> <td>$</td> <td>x</td> <td>$ represents the absolute value of $x$. $w$ is the width of the box. Filter center at $x = 0$. And this says the center of this filter is given at $x$ equals $0$ which is in the middle of the box function.</td> </tr> </tbody> </table> <div style="font-family: 'Noto Serif SC'">Box filter是一种用于图像处理和计算机视觉中的平滑滤波器。它是一种线性滤波器，使用具有相同权重的正方形内核来平滑图像中的像素。Box filter的内核通常是一个归一化的矩形或正方形，其中每个元素的权重相等，因此也被称为均值滤波器。Box filter通常用于降噪或模糊图像。在计算机视觉中，它也可以用于图像缩放、边缘检测和其他一些算法中。Box filter的缺点是它的平滑效果较弱，不够柔和，在对于较小的图像细节和纹理等细节的处理上表现不佳。在实际应用中，通常会结合其他滤波器来获得更好的平滑效果。</div> <h4 id="discrete-mean-filter">Discrete Mean filter</h4> <p>Away from $x = 0$, we can also form a discrete box filter =&gt; <strong>Discrete Mean Filter</strong>.</p> <p>The mean filter is basically a normalized discrete finite support version of the continuous box filter function.</p> <p>A discrete mean filter kernel ($N \times M$ pixel neighborhood window) is a constant kernel with weights $h_{ij} = 1/NM$.</p> <p>Computes the mean of the intensities in the neighborhood window.</p> <p>Example: A $3\times 3$ mean filter kernel.</p> <div align="center"><img src="https://i.imgur.com/bS4kkve.png" alt="image-20230214211637411" style="zoom:67%;"></div> <div style="font-family:'Noto Serif SC'">Discrete mean filter，也称离散均值滤波器，是一种常见的图像平滑滤波器。它使用一个正方形的窗口（通常称为内核或卷积核）滑动遍历整个图像，在每个位置计算窗口内像素的平均值，然后将该平均值作为该位置的像素值。具体地，离散均值滤波器的内核大小是一个具有相同权重的矩形或正方形，该内核的每个元素的权重相等，通常被称为"盒子"或"箱子"（box）内核，因此也被称为"盒子滤波器"（box filter）或"均值滤波器"（mean filter）。离散均值滤波器的优点是简单易懂、易于实现、计算速度快，适用于各种类型的图像和图像噪声。但是，由于使用均值滤波器可能会导致图像模糊、损失图像细节和边缘信息，因此在一些应用场景下需要使用更高级的滤波器。</div> <p>When you do convolution with this filter, you are taking the average of the intensities of the image underneath this filter.</p> <p>You can think of the $n$ in this discrete version of the box filter as being equivalent to the parameter $w$ in the continuous box filter.</p> <h4 id="filtering-at-the-border-of-the-image">Filtering at the border of the image</h4> <p>At the image border, parts of the filter kernel will be outside the image - i.e. we have no pixels to compute the weighted sum of.</p> <p>Possible solutions:</p> <ul> <li> <strong>Pad the border</strong> with a constant value, usually zeros (means that the contribution from outside the image is zero).</li> <li> <strong>Symmetric mirroring</strong> of image pixels - extend the image by padding with pixels by mirroring the pixels at the image border.</li> <li> <strong>Periodic boundary</strong> - the image is wrapped onto a sphere. Filtering on the right border requires pixels from the left border.</li> <li>Leave <strong>border pixels unchanged</strong> (copy from input to output)</li> <li>Filter only inside the image and <strong>crop</strong> away the border (the output will be smaller than input image)</li> </ul> <p>What you should choose depends on you application.</p> <h4 id="linear-separable-filters">Linear separable filters</h4> <p>The box filter, the mean filter is a kind of special filter. It’s linear separable.</p> <p>A linear filter is <strong>linear separable</strong>, if it can be <strong>decomposed into a linear filter along the x-axis and another along the y-axis</strong>.</p> <p>Not all linear filters are separable.</p> <p>This can make filtering implementations more efficient - two $1D$ filtering operations ($2N$ Arithmetic operations) instead of a $2D$ filtering ($N^2$ arithmetic operations)</p> <p>The $2D$ box filter is separable and can be expressed by the convolution of two $1D$ box filters</p> <div align="center"><img src="https://i.imgur.com/QXOvjxZ.png" alt="image-20230214215124119" style="zoom:67%;"></div> <p><strong><em>Proof</em></strong></p> \[(\delta_{(0, 0)}*h_N) * h_N^T = h_N * h_N^T = h_{N\times N}\] <h4 id="discrete-gaussian-filter">Discrete Gaussian filter</h4> <p>The discrete Gaussian filter is a discretized finite support (FIR) version of the continuous function</p> <div align="center"><img src="https://i.imgur.com/PlLgeWd.png" alt="image-20230214215948189" style="zoom:67%;"></div> <p>The standard deviation $\sigma$ is the scale/width of the filter. $\sigma$ increased, wider filter.</p> <p>The Gaussian filter is linear separable and we have</p> \[G_\sigma (x, y) = G_\sigma(x)* G_{\sigma}(y)\] <p>Example: For $N = 3$ and $x = [-1, 0, 1]$ we have</p> \[G_{\sigma}(x) = [0.27, 0.45, 0.27]\] <p>Every time ($x = -1, 0, 1$), I get a new value that ends up being this vector or list of values that represents my finite filter.</p> <p>Usually $N$ is chosen as a function of $\sigma$, e.g., $N=k\sigma$, where $k = 1, 2, or \ 3$.</p> <p><strong>Notice the values have been scaled to fit in uint8.</strong></p> <p>Examples of 2D Gaussian filter kernels</p> <div align="center"><img src="https://i.imgur.com/05AT6WM.png" alt="image-20230214221441449" style="zoom: 67%;"></div> <p>This filter is similar to the Mean Filter, the only difference is that we actually now weigh pixels that are closer to the center more than pixels that are further away from the center of the filter mask.</p> <div align="center"><img src="https://i.imgur.com/m8HzuJw.png" alt="image-20230214221830753" style="zoom:50%;"></div> <p>Increasing $\sigma$ =&gt; wider the kernel</p> <div style="font-family: 'Noto Serif SC'">模糊粒度更细</div> <h4 id="noise-removal-by-linear-filtering">Noise removal by linear filtering</h4> <p><strong>Different noise sources.</strong></p> <div align="center"><img src="https://i.imgur.com/koTmbPh.png" alt="image-20230214224552142" style="zoom:67%;"></div> <p><strong>Salt and pepper</strong></p> <p>The mean filter is just averaging out values. That means, all white or all black pixel will be draged towards the average of a value in the neighborhood around the pixel so that would mean that it becomes more grayish.</p> <p>For Gaussian filter, we are doing the weighted average and this gives out a slightly more blurred version of the noise compared to what the mean filter does.</p> <p><strong>Gaussian</strong> shows the same.</p> <p>We started out with some noise versions of the original, we applied a filter and it comes with the cost that there’s some smearing / blurring of the structures in the image. It has some degree of noise reduction but not complete noise removal.</p> <h4 id="correlation">Correlation</h4> <p>The convolution integral</p> \[g(x, y) = \{f * h \}(x, y) = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty} f(x', y')h(x-x', y-y')dx'dy'\] <p>The correlation integral</p> \[g(x, y) = \{f \circ h \}(x, y) = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty}f(x', y')h(x+x', y+y')dx'dy'\] <p>Difference is whether we flip $h$ or not as well as some theoretical consequences.</p> <p>Correlation is often used to implement discrete convolution, and instead flip filter kernel $h$ or image $f$ before correlation.</p> \[g(x, y) = \sum^{\lfloor N/2 \rfloor}_{i=-{\lfloor N/2 \rfloor}} \sum^{\lfloor M/2 \rfloor}_{j=-{\lfloor M/2 \rfloor}}f(x+i, y+j)h(i,j)\] <font face="Noto Serif SC">我的理解是，卷积是翻卷再 correlation。</font> <p>If you have a fast implementation of correlation, you can always implement a discrete convolution simply by using correlation. The only thing you have to do is you have to take your filter kernel and <strong>flip it before you do correlation</strong>. The alternative way is that you can flip th eimage left right and up and down before you do correlation.</p> <p>The result would be identical to performing discrete convolution.</p> <p><strong>Correlation used for template matching</strong></p> <p><strong>An application of correlation</strong></p> <p>Cross-correlation between an image and a prototype and the image patches centered at the local maximum.</p> <div align="center"><img src="https://i.imgur.com/ifnbWQo.png" alt="image-20230214231635662" style="zoom:67%;"></div> <p>I apply cross-correlation, and I take out a piece of image and normalise it in an appropriate way. In this way, I can use the this to perform template matching.</p> <p>Let’s assume that we cut out a face from this image. And that face forms my filter kernel. I do a little bit of normalization of the intensities to do cross-correlation in a proper way. I take this filter and then I do discrete correlation.</p> <p>I would expect that as we move this filter across the image, then, at each location where the pixels in the filter and in the image are similar you get a high value out of this correlation. We get the left image and you can see that there’s some extra bright spots and if we take these local maxims, these local bright spots and make a cut out of the same size as the filter. Let’s say we have a bright spot and I think that is one guy’s face and we then cut out the filter and then plot that as pieces of images that we stick together.</p> <p><strong>For template matching, if you have a simple template that you want to search for copies. You can do correlation and find a lot of spots where the correlation filter peaks and then you can select those and say that’s where my template matches the most.</strong></p> <p>So in this case, we could use this to find the face of a specific person in the crowd.</p> <h3 id="non-linear-filtering">Non-linear filtering</h3> <h4 id="definitions-1">Definitions</h4> <p>The filter is a <strong>non-linear</strong> function of the input image.</p> <p>The filter is often <strong>shift invariant</strong>, hence the filter is a non-linear function of the neighborhood window that is independent of position. =&gt; We have shifting operation that we slide and neighborhood window across the image and that every location we use the function of the underlying intensities to generate a value for the specific target pixel that we’ve reached.</p> <p>It is not possible to define the <strong>filter kernel</strong> as in the linear case.</p> <p>We cannot use convolution / correlation to define such filters.</p> <h4 id="example-of-a-non-linear-filter---median--rank-filtering">Example of a non-linear filter - Median / rank filtering</h4> <p>Median filter computes the median of the neighborhood pixels.</p> <p>The median is the $50\%$ quantile or the rank order that splits the intensities into two equally likely sets.</p> <p>Rank filtering computes the order statistics of the neighborhood intensity distribution.</p> <div style="font-family:'Noto Serif SC'">中值滤波（median filter）和秩滤波（rank filtering）是两种基于排序的信号处理方法，它们在某些方面有关系，但也存在一些区别。中值滤波是一种常用的平滑滤波方法，它的基本思想是用信号中某个窗口内的中值来代替该窗口内的信号值。具体地说，对于输入信号中的每个采样点，中值滤波器将该点的数值替换为一个与该点所在窗口中的所有值按大小排序后位于中间位置的值。这个窗口的大小通常是一个奇数，例如3、5、7等。秩滤波是一种更为一般化的排序滤波方法，它不仅可以使用中值作为参考，还可以使用其他百分位数，例如1%、10%、25%、75%、90%等。秩滤波器的基本思想是对于输入信号中的每个采样点，在该点所在的窗口中按大小对所有值进行排序，然后使用指定的百分位数（即所谓的“秩”）来代替该点的数值。因此，可以将中值滤波看作是一种特殊的秩滤波，它使用中位数作为秩。另外，需要注意的是，中值滤波只能对单峰信号进行平滑，而秩滤波则可以对具有任意形状的信号进行滤波。秩滤波是更一般的表达，中值滤波是选取50%的秩滤波。</div> <p>The discrete median filter / rank filtering</p> <ul> <li>Sort / rank pixel values in neighborhood in ascending order</li> <li>Find the value in the ordered list that at the wanted rank order $q$</li> <li>Set the target pixel value to the found rank value</li> </ul> <h4 id="noise-removal-by-non-linear-filtering">Noise removal by non-linear filtering</h4> <div align="center"><img src="https://i.imgur.com/bP455gE.png" alt="image-20230214235420981" style="zoom:67%;"></div> <p><strong>Median Filter</strong></p> <p>(a) =&gt; It looks a little bit like the <strong>mean filter</strong> but it’s not computing the mean. There is some smearing of the details, but it doesn’t look much different from the mean result.</p> <p>(b) =&gt; It looks much different thatn what we had with the previous mean filter. It looks like it cna clean up the salt and pepper noise. It looks like almost the original or at least very similar to what we have in (a). The reason why we get this is that since we’re applying a median filter, even though it’s only $3\times 3$ pixels. If we consider a pixel that they contains either salt or pepper, the neighborhood of that which contains a lot of pixels that have the almost same value. =&gt; The median of the distribution of those nine pixels in that neighborhood would lead to a gray background pixel. Similar in the case of coin. =&gt; That is the reason why median filter does a better job than linear mean filter or linear Gaussian filter.</p> <p>(c) =&gt; It doesn’t look like removing the noise, it looks a little bit like the same result we saw previously, the mean and the Gaussian filter.</p> <p><strong>Rank Filter</strong></p> <p>(a) =&gt; It prefer bright values. The coin seems to be a little bit more shiny.</p> <p>(b) =&gt; It has the adverse effect of boosting the noise. It seems to select a bright value. =&gt; This filter here is not really doing anything proper.</p> <p>(c) =&gt; If you consider the background white while the background is now uniform. Even though we have the shiny coins, but if the purpose is to reduce the noise in the background, then at least the filter does the job. However, you could still debate whether this is really a good result.</p> <h4 id="median-filtering-of-different-salt-and-pepper-noise-levels-and-filter-sizes">Median filtering of different salt and pepper noise levels and filter sizes</h4> <div align="center"><img src="https://i.imgur.com/LEOj2mC.png" alt="image-20230215000854927" style="zoom:67%;"></div> <p>For the first row ($3 \times 3$) it does a little bit of blurring, but it’s not a severe effect. $7\times 7$ gets worse. =&gt; Increase the size of the median filter has the advert effect that you are blurring out some details.</p> <p>For the second row. ($3\times 3$) it seems like generally remove the noise, but one effect that the pixel got overall brighter. But for the $7 \times 7 $, we still have this blurring effect but it looks a bit like that we’re getting a slightly better job than $3\times 3$ as the average intensity is more or less the same as in the original case.</p> <p>For the third row. $3\times 3$ still have a lot salt and pepper noise. 3 by 3 not doing the job. But 7 by 7 fixed it.</p> <p>Median filtering is a good choice if you have images that are destroyed by salt and pepper noise, but we need to choose the size of the filter carefully. The larger the filter is, the more blurring / smearing of the details you get. But you would need to make a choice depending on how sever the noise is.</p> <h3 id="derivative-filters">Derivative filters</h3> <p><strong>More on linear filtering</strong></p> <h4 id="image-as-arrays-or-functions">Image as arrays or functions</h4> <p>How we can use linear filtering to compute the derivatives of the images? That is construct derivative filters.</p> <p>We had this two views of an image either as an array of pixel values, or we could do this function interpretation. And in the function interpretation, we think of the image as being a function of two variables $x$ and $y$.</p> <div align="center"><img src="https://i.imgur.com/jirdwlq.png" alt="image-20230215010124607" style="zoom:67%;"></div> <p>We get this weird landscape. What we realized that an image is just a function or by its discrete representation function.</p> <h4 id="approximating-image-derivatives">Approximating image derivatives</h4> <p>We could think about how can we compute the derivatives of such a function.</p> <p>But the porblem is of course that we don’t actually have an analytical expression of the image function. We only have a <strong>sampled</strong> version of the image function, so we need to approximate the partial derivatives.</p> <p>Let’s just consider one of them.</p> <div align="center"><img src="https://i.imgur.com/XNH2diH.png" alt="image-20230215011836329" style="zoom:67%;"></div> <p><strong>First row</strong></p> <p>When we do the approximation, because $\Delta x \to 0$ in discrete function meaning the smallest step =&gt; The next pixel along the row. =&gt; I could just move one pixel to the right =&gt; I take a step of 1. =&gt; Therefore, the approximation.</p> <p>Even though this is not the limit where we have, but we’ve just made it as small as possible with the data we have. <strong>So this is one way to approximate a partial derivative on a discrete function.</strong></p> <p>This approximation is also known as a forward approximation that is because we’re moving 1 step to the right.</p> <p>We can also have a backward approximation =&gt; Midpoint approximation.</p> <p><strong>Second row</strong></p> <p>We move a row down in the image. We need to look one pixel ahead of the target pixel. =&gt; Down</p> <p><strong>Higher dimensionality</strong></p> <p>Skip</p> <p><strong><em>We can construct linear filter kernels from these approximations (forward differencing).</em></strong></p> <p>We just need to subtract neighbor pixels from each other.</p> <p>Since correlation does not require us to flip the filter, let’s start in the case where we want to construct the filter that we can use with correlation.</p> <p>Let’s go with the derivatives with $x$ firstly. It involves a target pixel value $f(x, y)$, and the one that just to the right of that pixel. The operation actually does here is picking out the value $f(x+1, y)$ and picking out the value $f(x, y)$ and changing the sign to $-$. So the filter kernel could look like $[-1, 1]$. We are computing the derivative at $(x, y)$ and we’re going to assume that in this filter mask, the $-1$ is the center of the filter. This pixel should be a line with the target pixel $(x, y)$.</p> <p>Whatever we have at $(x, y)$, we multiply that by $-1$ and then, we take one step to the right. And we take out the value we have at $(x+1, y)$ and just multiply that by 1. At the end, correlation is a sum, so now you have</p> \[f(x+1, y) - f(x, y)\] <p>So, this implements derivative using the $[-1 \ 1]$ impulse response, and doing correlation. The difference between correlation and convolution is that flipping the filter.</p> <div align="center"><img src="https://i.imgur.com/WwqPPrL.png" alt="image-20230215015059514" style="zoom:67%;"></div> <h4 id="other-first-order-derivative-filters">Other first order derivative filters</h4> <p>We have something called <strong>Roberts</strong> field that it take the derivatives along the diagonal in this 2 by 2 mask. Roberts filter is not really used anymore.</p> <div align="center"><img src="https://i.imgur.com/KUdsfQc.png" alt="image-20230215020100219" style="zoom:67%;"></div> <p>Prewitt is doing tomething like a midpoint approximation. It involves a 3 by 3 filter, and requires intensity values in the first column one back in the image. The third is forward in the image. And the target or center of the mask is in the middle. Sobel is similar.</p> <h4 id="linear-seperable-filters">Linear seperable filters</h4> <p>Prewitt and Sobel filters are lienar separable.</p> <div align="center"><img src="https://i.imgur.com/TEtvBBf.png" alt="image-20230215020534578" style="zoom:67%;"></div> <p>The first row could be viewed as a mean filter. The second row could be viewed as Gaussian filter.</p> <p>Smearing along the y-axis and computing the derivative along the x-axis.</p> <div style="page-break-after: always;"></div> <h4 id="computing-derivatives-with-sobel-filter">Computing derivatives with Sobel filter</h4> <p>Let’s consider what the Nobel filter does.</p> <div align="center"><img src="https://i.imgur.com/bjfGE61.png" alt="image-20230215021128840" style="zoom:67%;"></div> <p>$derivatives$ are reached by applying the Sobel filter.</p> <p>If we consider this particular change or edge in the intensities (scarf), we have dark pixels that the values are close to 0 and then the brighter pixel values here.</p> <p>We are moving from dark to white, from small intensity value to larger intensity value. When we compute the sobel filter, it involves looking 1 pixel ahead (larger) and one behind (small). <strong>That means the filter response becomse positive.</strong></p> <p>Once we start to subtract pixels from each other, we can get a negative filter response. So the resulting image could have negative values.</p> <p>Dark edge is because we are moving from large value to small value. - Dark after rescaling.</p> <p><strong><em>So the derivatives says about changes in the intensity levels, the contrast changes of the image.</em></strong></p> <p>Computing derivatives is sensitive to noise - we are adding up noise from multiple pixels. =&gt; Even though sobel might have sloved it inside, but it still need to use some form of blurring first (Gaussian filter with small $\sigma$). =&gt; Then, you can get more robust derivatives that are robust to noise.</p> <p>Common to smooth image (e.g. with a Gaussian filter) before computing derivatives to be robust to noise.</p> <div style="page-break-after: always;"></div> <h4 id="differentiation-filters-amplifies-noise">Differentiation filters amplifies noise</h4> <div align="center"><img src="https://i.imgur.com/EznN3fk.png" alt="image-20230215022513358" style="zoom: 50%;"></div> <p>The problem is that for the first derivative, we need to consider two pixel values. We are subtracting together, but if the effect is that we have noise from two pixels being added up, and in the second derivative we have noise from three pixels being added up. That means we’reboosting the noise as we increase the differentiation order.</p> <p>The way to limit the effect and still get reasonable results is by applying some smoothing filter first such as the Gaussian filter.</p> <h4 id="a-snippet-of-differential-geometry">A snippet of differential geometry</h4> <p>Differential geometry is a handy mathematical formulation for building image analysis algorihms.</p> <div align="center"><img src="https://i.imgur.com/2AOdxvQ.png" alt="image-20230215023411930" style="zoom: 50%;"></div> <h4 id="edge-detection">Edge detection</h4> <p>An image intensity edge are given by locations in the image of abrupt changes in intensities (high contrast).</p> <p>Usually detected by</p> <ul> <li>Extrema of first order derivatives, e.g., maxima of gradient magnitude</li> <li>Zero crossings of second order derivatives, e.g., of the Laplacian</li> </ul> <h4 id="zero-crossings-of-laplacian-of-gaussian-filter">Zero crossings of Laplacian of Gaussian filter</h4> <div align="center"><img src="https://i.imgur.com/6YOFURT.png" alt="image-20230215024415227" style="zoom: 50%;"></div> <h3 id="filtering-color-images">Filtering color images</h3> <p>Color images and multi-channel images have vector values at each pixel - how do we apply linear / non-linear filtering to such images?</p> <p>Some possibilities are:</p> <ul> <li>Apply the same filter to each channel independently.</li> <li>Construct a multi-channel filter that computes based on all channels at the same time.</li> <li>Change color space, e.g., convert to gray scale or HSV representation and do filtering on a single channel image (e.g. the v-channel from HSV).</li> </ul> <p>What you want to do depends on the task and filter.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"LiuYing-1/liuying-1.github.io","data-repo-id":"R_kgDOMScbNQ","data-category":"Announcements","data-category-id":"DIC_kwDOMScbNc4Cgruw","data-mapping":"pathname","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Y. Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JC70RZ57BT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JC70RZ57BT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>