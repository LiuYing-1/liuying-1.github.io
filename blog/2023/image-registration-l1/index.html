<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlece17f0a456a89a36.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Image Registration Basics | Y. Liu </title> <meta name="author" content="Y. Liu"> <meta name="description" content="Learning note for medical image registration."> <meta name="keywords" content="ku, ucph, copenhagen, diku, portfolio-website, liuying, dk, yingliu, ying liu, liu ying"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?157bbd74cef60250fd5a67a2e078966e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liuying-1.github.io/blog/2023/image-registration-l1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Y.</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image Registration Basics</h1> <p class="post-meta"> Created in October 06, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/mia"> <i class="fa-solid fa-hashtag fa-sm"></i> mia</a>   ·   <a href="/blog/category/study"> <i class="fa-solid fa-tag fa-sm"></i> study</a>   <a href="/blog/category/ucph"> <i class="fa-solid fa-tag fa-sm"></i> ucph</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong><em>Disclaimer: All notes below are refered to the course “Medical Image Analysis” delivered by UCPH.</em></strong></p> <p>So today’s lecture will be on <strong><u>Image Registration</u></strong>.</p> <p>We have already talked about the topic of registration in our previous lecture. I said that there is some problems of registration. And its idea is to move from one image to another. It;s very popular in our field but I initially said we’re not talk about applications of registration so that you can think about this yourself, and also we can wait till the registration lecture like <u>which is having now</u>.</p> <h4 id="todays-learning-objectives">Todays Learning Objectives</h4> <ul> <li>Why do we need registration?</li> <li>Name and explain at least two similarity measures</li> <li>Describe the difference between rigid, affine and non-rigid registration</li> </ul> <h4 id="what-is-registration">What is Registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/i4jYCLQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 1. Example of registration </div> <ul> <li> <u>Image processing method</u> for <u>aligning two or more images with each other spatially</u> (geometrically)</li> <li>You can <u>make images from different imaging modalities, times, sections, angles</u>, etc, <u>comparable</u>.</li> </ul> <h4 id="why-do-we-need-registration">Why do we need registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/jO6tT6z.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 2. Why we need registration </div> <p><strong><em>Registration geometrically transforms one image into another.</em></strong></p> <p>So what we have is we have an image which is in the most top left. And we have a target which is in the most right bottom.</p> <p>And we slowly transform our image step by step towards the target.</p> <p>So you can see that this patient has some degenerative disease and changing in the brain, so the ventricles are growing, and you can see they slowly step by step grow when we use the image of the patient. Grow and grow until they reach this level.</p> <p>But the question is, how does that happen? What mathematical process lies behind allows us to nicely deform one image towards another one?</p> <p><strong><u>Before we go into mathematical concepts, let's look at one of the main applications of image registrations.</u></strong></p> <h5 id="atlas-based-segmentation">Atlas-based segmentation</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/BJ0T49b.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 3. Atlas registration </div> <p>It’s <strong>atlas-based segmentation</strong>.</p> <p>Before deep learning, that was specification was the main like every time when people use registration almost in 80% or something. They used it for atlas-based registration.</p> <p>So what is the idea of atlas-based registration? What we do is we have a collection of images <u>maunally segmented</u>. And we get a new image, we <strong><u>register every image from our collection towards our image</u></strong>. During this registration, we get <strong>transformation matrix</strong>.</p> <p>So basically we get some map which says which pixels went where during the registration, how we changed during the process. And the way you can use this transformation metrics or we can apply it to the segmentation from our atlas.</p> <p>我的理解是我们原本有一系列标准的人工分割好的图片，将每一张图片都以我们的新图像作为 Target 进行配准，在配准过程中，我们获得了变换矩阵，而这些矩阵可以应用到分割上从而获得新图像的 segmentation。</p> <p>We deform this image towards our target and then, we use the transformation to deform the segmentation towards our target, so that our segmentation kind of stretches and changes that tries to capture our new image.</p> <p>We can do it for all images in our atlas and then we get many versions of segmentations. And then we average them somehow (label fusion), and we just take this average result as a registered segmentation result.</p> <p>This is one of the main idea for using registration.</p> <p>What are the <strong>advantages</strong> and <strong>disadvantages</strong> of atlas-based segmentation?</p> <ul> <li> <p>Segmentation speed linearly depends on the number of training images $\implies adv$</p> <p>For any other algorithms, like deep learning algorithms, the number of training images does not affect the speed of segmentation itself. It can affect training but does not affect the segmentation. If we have million training images, the speed is the same.</p> <p>Because we registered every atlas image individually to our target, the speed directly linearly increases with the number of training images. $\implies disadv$ So the people investigate how we can use not all the training images but also some specific training images which are most similar to our target. So they apply some preprocessing and figure out which training images are most similar and then only register them to our target.</p> </li> <li> <p>Segmentation speed does not depend on the number of target structures</p> <p>There’s a couple of <strong>advantages</strong>. One advantage is that the <strong>speed of atlas-based segmentation</strong> <strong>does not depend on the number of structures we have</strong>. We can have one million different regions in the object but the speed will be the same. We just take training images, register them, take the transformation metrics and then deform the mask.</p> <p>But many alternative algorithms, maybe like random walker, their <strong>speed grows linearly with the number of objects we have</strong>.</p> </li> <li> <p>Needs way less training samples than deep learning</p> <p>Another advantage is that we need way less training images for algorithm to work.</p> <p>In contrast to now deep learning, we need thousands or hundreds images, we can use 20 segmented brains to have a reasonably good atlas-based segmentation.</p> </li> </ul> <h5 id="detecting-differences">Detecting differences</h5> <p><strong>Diffcult to see and examine differences/changes</strong> if the images are <strong>not oriented</strong> the same or <strong>on top of each other</strong>.</p> <p>There are many different reasons for wanting to register images:</p> <ul> <li> <p><u>Get different contrasts</u> from different modalities superimposed</p> <p>This refers to the ability to <u>overlay or combine</u> <u>images obtained from different imaging modalities</u>, such as MRI, CT, or PET. <u>Each modality provides unique information or contrast about the same anatomical region</u>. <u>Registering</u> these images allows you to see <u>the anatomical structures in relation to each other</u>, providing a <u>more comprehensive view</u> of the patient’s condition.</p> <p>(从不同模式的叠加中获得不同的对比)</p> <ul> <li> <p>Get an image with poor resolution or with little anatomical detail superimposed on an image with good anatomical information</p> <p>Sometimes, medical images may have <u>low resolution</u> or <u>lack fine anatomical details</u>. <u>Registering</u> such images with <u>higher-resolution</u> or <u>more detailed images</u> can help in <u>better visualization</u> and <u>understanding of the region of interest</u>. This process essentially <u>enhances the quality of the lower-quality image by aligning it with a reference image with better anatomical information</u>.</p> <p>(将分辨率低或解剖细节少的图像叠加到解剖信息丰富的图像上)</p> </li> </ul> </li> <li> <p>Inverstigate <u>changes over time</u> (e.g., before-after treatment)</p> <p>Image registration is used to <u>compare images acquired at different time points</u>, such as <u>before and after medical treatment</u> or surgery. By <u>aligning</u> these images <u>accurately</u>, you can <u>precisely quantify changes in size</u>, <u>shape</u>, or <u>intensity</u> of structures over time. This is valuable for <u>monitoring disease progression</u> or the <u>effectiveness of a treatment</u>.</p> <ul> <li> <p>See if there are differences in <u>size, shape</u> or <u>intensity</u> over time - more sensitively</p> <p>Image registration helps in <u>detecting subtle changes</u> in the size, shape, or intensity of anatomical structures over time. It <u>enhances the sensitivity of your analysis by ensuring that the images are aligned correctly</u>, allowing for <u>precise measurement of changes that might otherwise go unnoticed</u>.</p> </li> </ul> </li> <li> <p>To compare different individuals</p> <p>Image registration allows you to <u>align and compare images from different individuals</u>, enabling a quantitative and objective assessment of anatomical structures, pathology, or physiological variations <u>across a population</u>. This is especially valuable in research studies and clinical trials.</p> <ul> <li> <p>Analyze the images to see <u>differences between individuals</u></p> <p>Image registration facilitates the <u>automated analysis of images</u> to <u>identify and quantify differences between individuals</u>. This can include <u>variations in organ size, shape, location, or pathology</u>. <u>Automated analysis</u> saves time and reduces the potential for subjective bias that may occur in <u>manual comparisons</u>.</p> </li> <li> <p>Can <u>save a lot of time</u> - for example, rather than having to <u>manually draw the structures and compare them</u></p> <p>Image registration <u>significantly reduces the need for time-consuming manual tasks</u>, such as <u>manually outlining or drawing structures for comparison</u>. By automating the alignment and analysis of images, it streamlines the research and diagnostic process, making it <u>more efficient and less prone to human error</u>.</p> </li> <li> <p><u>Register to atals</u>, and <u>extract specific region</u></p> <p>Registering images to a common anatomical atlas allows for <u>precise localization and extraction of specific anatomical regions or structures</u>. This is <u>useful for conducting region-specific analyses</u> or for <u>guiding surgical procedures with high precision</u>. It also aids in standardizing data across different individuals for research purposes.</p> </li> </ul> </li> <li> <p>Important to check that the <u>registrations are correct</u> - be sure to <u>compare the same anatomical structure</u>.</p> </li> </ul> <h4 id="ingredients-of-image-registration">Ingredients of image registration</h4> <p>The registration algorithms have different aspects which they work with.</p> <p>One aspect is information type. What information they use are <strong>intrinsic</strong> or <strong>extrinsic</strong> information. Another one thing which actually depends on information type is the <strong>similarity measure</strong>. How to understand that images become more similar or less similar during the registration. And another thing is <strong>transformation</strong>. So for some applications that have <strong>rigid</strong> transformations, only translation, rotation for example. Or for some application and most often we need <strong>non-rigid</strong>. So we need to somehow change the relative position of pixels, how expansive areas fix some other areas to get the new object.</p> <ul> <li> <u>What type of information</u> do we utilise?</li> <li> <u>How do we quantify</u> similarity?</li> <li> <u>What kind of transformations</u> can we use?</li> <li> <u>What kind of interpolations</u> can we use?</li> </ul> <h5 id="sources-of-information">Sources of information</h5> <ul> <li> <p>Intrinsic information</p> <ul> <li> <p>The images are visually similar (non-necessarily by absolute intensities):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oEvhMvl.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 4. Intrinsic information </div> </li> </ul> </li> </ul> <p>The intrinsic information is used when these images are visually similar to each other. The intensities are similar most important. Fro example, here you can see two same images and there is a histogram how good they match to each other. And now this image will be rotated and this histogram is moving and it’s becoming more like a straight line.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/9eJMBdI.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 5. Illustration of Histogram </div> <p>The histogram pixel reflects how much these two pixels are similar to each other. And the higher color is the more similar. So if you take the next pixel, how similar is the first pixel to second pixel and so on. And when we go through all pixels of first row, we compare the first pixel the first image to all pixels in the second image. And we just measure how similar they are.</p> <p>So if these images are very similar to each other, what we would expect is to see somehow the high values on the diagonal when we compare the corresponding pixels, and to see a low values outside the diagnoal, and we don’t compare the corresponding pixels.</p> <p>对角线上的值越大，说明两幅图片像素值视觉上越相近。</p> <p>So this is the idea of the histogram.</p> <p><strong>Extrinsic information</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/wDl0rkq.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 6. Example of extrinsic information </div> <ul> <li>The images are too different from each other visually</li> <li>We need to <strong>help registration by providing</strong> <strong>correspondences</strong> </li> </ul> <p>The extinsic information, we usually work with images which are not similar to each other. So you can see these two images, one of them is CT and one of them is PET. And the intensities here are completely different.</p> <p>For example, we can see that the mandible is more or less visible, but in PET, the mandiable is not visible at all and the bone is invisible. But tumor is very much visible at PET, very poor visible in CT.</p> <p>So in that case, if the images have two different visually from each other. What we need to do is we need to provide some additional correspondences. And one of the most common thing to do is automated landmarks.</p> <p>We train algorithm to the specific points on both images, and we use these specific points to align the images. So we detect these points individually on PET, and detect these points on CT individually. And then we use these points to alignt he images to non-register form. So there’s a point measure to each other.</p> <h4 id="different-types-of-transformations">Different types of transformations</h4> <ul> <li>Rigid</li> <li>Affine</li> <li>Non-rigid</li> </ul> <h5 id="rigid-and-affine-transformations">Rigid and Affine transformations</h5> <p>See details in my previous blog post from <a href="https://liuying-1.github.io/assets/pdf/geometry.pdf">Signal and Image Processing - Transformations</a>. <img class="emoji" title=":point_left:" alt=":point_left:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f448.png" height="20" width="20"></p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UqtiM6p.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <h5 id="non-rigid-transformations">Non-rigid transformations</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/aHW956w.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>Now let’s define this <strong>mathematically</strong>.</p> <p>How is a 2D <strong>transformation or rotation</strong> defined mathematically?</p> <p>Can we extend this to 3D?</p> <h5 id="rigid-registration---mathematically">Rigid registration - mathematically</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/cAO4SD8.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p><u>In what kind of applications to we want to use rigid registrations?</u></p> <p>E.g., <strong>Intra-subject rigid body</strong></p> <p><strong>Intra-Subject:</strong> “Intra” means <strong>within</strong>, and “subject” typically <strong>refers to an individual or a patient</strong>. Intra-subject, in this context, means that <strong>you are dealing with multiple images of the same individual or patient</strong>. These <strong>images are acquired at different time points</strong> or <strong>using different imaging modalities</strong> while <strong>focusing on the same anatomical region</strong> within the <strong>same person</strong>.</p> <p><strong>Rigid Body</strong>: In the context of image registration, a rigid body transformation is <strong>a mathematical transformation</strong> that includes <strong>translation</strong>, <strong>rotation</strong>, and <strong>scaling</strong> but does <strong>not allow for any deformation or stretching</strong> of the image. In other words, the <strong>relative positions and orientations of structures within the image can change</strong>, but the <strong>shapes and sizes of the structures remain the same</strong>.</p> <p><strong>Intra-Subject Rigid Body Registration</strong>:</p> <ul> <li>This refers to the process of <strong>aligning</strong> and <strong>registering</strong> images of the <strong>same individual or patient</strong> that have been obtained under <strong>different conditions</strong> or at <strong>different times</strong>.</li> <li>For example, if <strong>a patient undergoes a series of medical imaging scans over time</strong> (e.g., MRI or CT scans) to monitor a condition or treatment <strong>progress</strong>, you may use <strong>intra-subject rigid body registration to align these images</strong>.</li> <li>The purpose is to ensure that <strong>corresponding anatomical structures in each image are in the same spatial position and orientation</strong>, even if the patient’s position or the imaging conditions have changed slightly between scans.</li> <li>The primary goal is typically to <strong>track changes in the patient’s anatomy</strong>, such as tumor growth, organ movement, or any other anatomical variations, with precision and accuracy.</li> </ul> <p>Applications of <strong>intra-subject rigid body registration</strong> can be found in various medical fields, including <strong>radiation therapy</strong>, where it’s crucial to <strong>ensure that the radiation beam is accurately targeted at the same area during multiple treatment sessions</strong>, or in <strong>longitudinal studies to monitor disease progression</strong> or <strong>response to treatment within the same patient</strong>.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JpzfBT9.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>In summary, <strong>intra-subject rigid body registration</strong> involves <strong>aligning and comparing images of the same individual taken at different times</strong> or <strong>under different conditions</strong>. It’s a fundamental technique in medical image analysis used for <strong>monitoring, diagnosis, and treatment planning</strong>, where precise <strong>spatial alignment of anatomical structures</strong> is essential.</p> <h5 id="affine-registration---mathematcically">Affine registration - mathematcically</h5> <p>However, basically, the scaling is considered as affine transformation.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/CRsio4B.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>In what kind of applications to we want to use affine registrations?</p> <p>E.g. <strong>inter-subject</strong></p> <p><strong>Inter-Subject</strong></p> <ul> <li>“Inter” means <strong>between or among</strong>, and “subject” typically refers to <strong>different individuals</strong> or <strong>patients</strong> in the context of medical imaging.</li> <li>Therefore, “inter-subject” means that <strong>you are dealing with images acquired from different individuals or patients</strong>.</li> </ul> <p><strong>Affine Transformation</strong>:</p> <ul> <li>An affine transformation is a type of transformation that <strong>includes translation, rotation, scaling</strong>, and <strong>shearing</strong> but does not allow for <strong>deformation or non-linear warping of the image</strong>. Essentially, it’s a more flexible transformation than rigid body transformations (which only include translation and rotation) but still <strong>preserves the basic shape of objects</strong>.</li> </ul> <p><strong>Inter-Subject Affine Transformation</strong>:</p> <ul> <li>This refers to the process of <strong>aligning and registering images acquired from different individuals or patient</strong>s.</li> <li>In medical imaging, you might have a set of images from different patients, such as MRI scans of different brains, CT scans of different chests, or X-rays of different limbs.</li> <li>Inter-subject affine transformation allows you to <strong>align and compare these images</strong>, despite variations in size, orientation, and scaling between individuals.</li> <li>The primary goal is to <strong>establish a common spatial coordinate system that enables meaningful comparisons</strong>, such as in population-based studies, group analyses, or template-based approaches.</li> </ul> <p>Applications of <strong>inter-subject affine transformation</strong> can be found in various areas of medical research and clinical practice, such as <strong>comparing anatomical structures across a population</strong>, <strong>creating population-based atlases or templates</strong>, and <strong>performing group-level statistical analyses</strong>. It’s particularly useful when you need to make comparisons or perform analyses that involve images from <strong>different individuals</strong> while accounting for variations in size, orientation, and scaling.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/ngmeDyx.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>In summary, inter-subject affine transformation involves <strong>aligning and registering images from different individuals or patients</strong>, allowing for <strong>meaningful comparisons and analyses across a population</strong>. This technique plays a crucial role in various medical image analysis applications, particularly in group studies and population-based research.</p> <h5 id="example-of-affine-registration-across-several-subjects">Example of affine registration across several subjects</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/XNxsaN5.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <h5 id="non-rigid-registration---mathematically">Non-rigid registration - mathematically</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Vw4yIRX.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>In what kind of applications to we want to use non-rigid registrations?</p> <p>E.g. <strong>intra or inter-subject non-rigid body</strong></p> <p>A 3x3 matrix that represents <strong>a non-rigid transformation</strong>. <strong>Each element in the matrix affects the deformation</strong> of the image in different ways. The exact values of these elements determine <strong>how the image is deformed locally</strong>. Here’s a brief explanation of the elements:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">m1</code>, <code class="language-plaintext highlighter-rouge">m5</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control <strong>scaling</strong> factors in the x, y, and z directions, respectively.</li> <li> <code class="language-plaintext highlighter-rouge">m2</code>, <code class="language-plaintext highlighter-rouge">m4</code>, <code class="language-plaintext highlighter-rouge">m6</code>, and <code class="language-plaintext highlighter-rouge">m8</code> control <strong>shearing</strong> or skewing effects.</li> <li> <code class="language-plaintext highlighter-rouge">m3</code>, <code class="language-plaintext highlighter-rouge">m7</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control the <strong>actual deformation or warping</strong>.</li> </ul> <p><strong>1. Why Use Non-Rigid Transformations?</strong></p> <ul> <li> <strong>Non-rigid transformations</strong> are used when you need to account for <strong>more complex changes in the shape and structure of objects or regions within the image</strong>. Rigid and affine transformations are too restrictive for scenarios where local deformations occur, such as changes in the shape of organs or tissues, as in medical image analysis.</li> </ul> <p><strong>2. Applications of Non-Rigid Registrations (Intra or Inter-Subject):</strong></p> <ul> <li> <strong>Intra-Subject Non-Rigid Registration</strong>: In this case, you are dealing with images of the <strong>same individual or patient at different time points</strong>. <strong>Non-rigid registration</strong> can be used to account for <strong>anatomical changes over time</strong>, such as organ deformation during respiration, muscle contractions, or gradual changes in tissue structures due to disease progression.</li> <li> <strong>Inter-Subject Non-Rigid Registration</strong>: When you have images from <strong>different individuals (inter-subject)</strong>, <strong>non-rigid registration</strong> becomes valuable for <strong>aligning images that exhibit significant anatomical variations</strong>. Examples include aligning MRI scans of different brains, which have <strong>varying shapes and sizes</strong>, or comparing images of different patients with congenital or acquired deformities.</li> </ul> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JPahdsm.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <p>In both <strong>intra-subject</strong> and <strong>inter-subject</strong> scenarios, <strong>non-rigid registration allows you to account for the complex and local variations in anatomy</strong>, enabling <strong>accurate comparisons, analysis, and medical interventions</strong>. It’s particularly important when dealing with deformable structures and when rigid or affine transformations are insufficient to capture the necessary deformations. Applications range from disease monitoring to surgical planning and population studies.</p> <p><strong>1. 为什么要使用非刚性变换？</strong></p> <ul> <li>当您需要考虑<strong>图像中更复杂的物体或区域的形状和结构发生变化</strong>时，就需要使用<strong>非刚性变换</strong>。对于医学图像分析等领域，刚性和仿射变换对于局部变形场景太过受限制。</li> </ul> <p><strong>2. 非刚性配准的应用（个体内或个体间）：</strong></p> <ul> <li> <strong>个体内非刚性配准：</strong> 在这种情况下，您处理的是<strong>同一人或患者在不同时间点的图像</strong>。非刚性配准可用于考虑随时间发生的解剖变化，例如呼吸过程中的器官变形、肌肉收缩或由于疾病进展引起的组织结构逐渐变化。</li> <li> <strong>个体间非刚性配准：</strong> 当您有来自不同个体的图像（个体间）时，非刚性配准变得重要，以对齐展示明显解剖差异的图像。例如，对齐具有不同形状和大小的不同大脑的MRI扫描图像，或比较不同患者具有先天性或后天性畸形的图像。</li> </ul> <p>在个体内和个体间的情况下，非刚性配准允许您考虑解剖的复杂和局部变化，从而实现准确的比较、分析和医疗干预。它在处理可变形结构和刚性或仿射变换无法捕捉必要变形的情况下尤为重要。应用范围从疾病监测到手术规划和人群研究。</p> <h5 id="example-of-non-rigid-registration-across-several-subjects">Example of non-rigid registration across several subjects</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vY3vg4N.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> <h4 id="similarity-measures">Similarity measures</h4> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6imIdry.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 7. MSE </div> </div> <p>So let’s talk about <strong>similarity measures</strong>.</p> <p>What could we use as <strong>similarity measure</strong>?</p> <ul> <li>Absolute difference</li> <li><strong>Sum of squared difference</strong></li> <li><strong>Cross-correlation</strong></li> <li>Mutual information</li> </ul> <p>There are two types of similarity measures described above? What are the two types?</p> <h5 id="mean-sum-of-squared-differences">Mean sum of squared differences</h5> \[MSE = \frac{1}{n}\frac{1}{m}\sum^{n}_{x=1}\sum^{m}_{y=1}\left(I(x, y)-J(x,y)\right)^2\] <p>We have one of the most obvious similarity measure is the mean sum of squared differences. So what we do would take the same pixel in each image, and compare the intensity difference and we square it and we sum up all squared differences and we divided by the number of pixels we have.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vaWcNyY.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 8. MSE calculation </div> </div> <p>So here is an example of these two images and the result of similar measure calculation. And the smaller the value, the most similar the images are. So we want to <strong>minimize</strong> it.</p> <h5 id="normalized-sum-of-squared-differences">Normalized sum of squared differences</h5> <p>The sum of squared differences works well if the images have the same intensity ranges. What if they have <strong>a different intensity ranges</strong>?</p> <p>One of the good idea which is used almost always when the pixel-wise comparison is used is <strong>normalization</strong>.</p> <p>What we do is we take our intensities in our image, we compute the mean intensity, we compute the standard deviation. We can test this and we normalise them. From the original images subtract the mean that the new mean will become $0$. And we divide it by the standard deviation.</p> \[I^* = \frac{I-\overline{I}}{\sigma(I)}\] <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/3w7cKY7.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 9. NMSE </div> </div> <p>By doing this, everything is the same except the <strong>intensity range</strong>. And it is more good useful in practical applications.</p> <p>So <strong>normalization</strong> is one of the things which is very commonly used and this is <strong>pixel-wise similarity measure</strong>.</p> <h5 id="normalized-cross-correlation">Normalized cross-correlation</h5> <p>Another idea which we can go further in this area, what we can do is we can do the <strong>cross-correlation</strong>.</p> \[NCC = \frac{\sum_{x, y}\left(\left(I(x, y) - \overline{I}) \cdot(J(x, y)-\overline{J}\right)\right)}{\sqrt{\sum_{x, y}(I(x, y) - \overline{I})^2\sum_{x, y}(J(x, y)-\overline{J})^2}}\] <p>So what we do is to <strong>find the normalized pixel values</strong> and then <strong>multiply each other</strong>, and <strong>find the sum of them</strong>. And then we divide it by the squared root of multiplication of squared difference. So to see how these things behaves, let’s go to the mirror board to see some examples.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/p8i5spT.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 10. Cross-correlation </div> </div> <p>So, what we would like to do is would like this expression to get good values when we compare the upper two patches, and to have bad values when compared to the other two. We are going to quickly compute them to see what happens.</p> \[NCC_{good} = \frac{3\cdot3 + 3\cdot(-1)\cdot(-1)}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{12}{12}=1\] <p>What about the bad patches?</p> \[NCC_{bad} = \frac{-3+1+1-3}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{-4}{12}=-\frac{1}{3}\] <p>What we would like the normal cross correlation to be as high as possible because we saw that for the <strong>same patch</strong>, we get $1$. And for two different patches, we get $-\frac{1}{3}$. And we actually see one of the good property of the normal cross-correlation. It seems to be that its values are actually restricted into the $-1$ and $+1$ range. So when we get to same patches, basically the best possible scenario. We get the result with $1$. So probably if we get the opposite patch, we will probably get $-1$.</p> <p>The good thing about is normalise as cross correlation is, that’s why the name comes that our values. They <strong>get normalized</strong> into the $-1$ to $+1$ range. This is very <strong>useful</strong> when we compare two patches.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/M4pqEGG.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 11. Maximization </div> </div> <p><strong><em>Will MSE and NCC work for CT-MR image registration?</em></strong></p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/SCQfK8H.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 12. CT-MR </div> </div> <p>We want to perform <strong>registration</strong> on <strong>CT-MR</strong> image on two images, which are <strong>very different intenstity patterns</strong>. The edges in these images are kind of <strong>similar</strong>.</p> <p>You can see that there is a <strong>mandible</strong> on the right chart, which is <strong>black</strong> but on the left side, it is <strong>white</strong>. You can see the edges over human skin. They are also kind of more or less visible. But what is important is that the intensities now is completely different. So the intensitity plan is completely different.</p> <p><strong><em>Now the question is if you can use MSE or NCC or something like this for this registration.</em></strong></p> <p>Actually turns out we <strong><u>cannot</u></strong> use them because what MSE and what all of these metrics for what they tried to do is try to <strong><u>match high intensity with high intensity, and low intensity with low intensity</u></strong>.</p> <p>But in our particular case, what we would like to do is we want to <strong>match somehow the patterns the edges within the pixels</strong>. For example, we want to match a bone, where a black and white intensities in different patches. <strong>It’s very difficult to just in advance define this, so we can know what exactly we want to match</strong>. =&gt; <em>But we want to match patterns not individual intensities.</em></p> <h5 id="mutual-image-information">Mutual image information</h5> <p>For this reason, there is another metric which is called <strong>mutual image information</strong>.</p> <p>The metric is most commonly used in <strong>image registration</strong>. It requires a lot of computation resources, but at the same time, it is a very <strong>versatile</strong> can be applied for many different images. I do not need to know many things about images in advance. We can just apply it and it will most likely work.</p> <p>Here is an equation how mutual information works.</p> \[MI = E(I, J)\] <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6FN99jJ.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 13. Joint entropy </div> </div> <p>So we need to compute the entropy issues and we will compute in histogram manner.</p> <p>Below is the illustration.</p> <p>Let’s say we have different images which we want to match each other. Let’s assume these are the images of some object.</p> <p>What we do is we create a 2D plot where at a lower dimension.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oosMzEx.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 14. Illustration </div> </div> <p>Then, we will do the same thing for the next pixel, and ….. it will finally come with a cloud of points. And we are interested to how <strong>compact</strong> is this cloud.</p> <p>From this, if there are two images just inversion of each other, our plots will be very compact. It means, we just need very small regions to cover all the possible pixels. But when the images were random, the pixels which would plot it on this 2D plot, they would distribute to the space in a completely manner. They could be occupied a lot of area.</p> <p>So what we can do and the idea of the mutual information is to complete a histogram of the pixels in both axises $x$ and $y$ and which is a probability histogram. And then compute probability histogram for joint probability.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/K8wN4pA.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 15. Illustration </div> </div> <p><strong><u>So the more compact the more higher will be mutual information.</u></strong></p> <p>And here is basically how they look like for this particular image.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Wuk9Ahc.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 16. Mutual information </div> </div> <p>These are with the mutual information for these two images which are just inversion of each other.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/GFL1GWk.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 17. Rotation mutual information </div> </div> <p>Here is the rotated version. And you can see that the plot now is no longer compact as it was. It occupies a lot space. And that’s why the mutual information is way lower. And our ideas is to <strong>maximize mutual information</strong>.</p> <h4 id="interpolation">Interpolation</h4> <p>Another concept which we need to know is the concept of <strong>interpolation</strong>.</p> <p>What different interpolation techniques do you know?</p> <ul> <li>Nearest neighbour</li> <li>Linear interpolation</li> <li>Barycentric interpolation</li> <li>Spline interpolation</li> </ul> <h5 id="interpolation-effects">Interpolation effects</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/1EVjwqL.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 18. Interpolation effects </div> </div> <p>Different interpolation techniques work on diff. images, the choice of interpolation technique can have varying effects or perform differently depending on the characteristics and content of the input images.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YanUjxl.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 19. Effects of different interpolation techniques </div> </div> <p>不同插值技术中提到的 RMSE（均方根误差）值可以作为一种量化指标，用于评估每种插值方法应用于特定图像时的质量或准确性。RMSE 是一种常用指标，用于量化原始图像与插值（转换）图像之间的差异。</p> <p>The <strong>interpolation</strong> in image registration plays two roles.</p> <p>One of them is for <strong>computing the values of pixels</strong>. Let’s say if the pixels has a coordinate which do not match original coordinates. For example, one pixel has a coordinate $(0, 0)$. But if we would like to move it a little bit, $(-0.1, 0.3)$. <strong>What kind of value of this pixel have?</strong> Should we just around these numbers to get $(0, 0)$ or should we do something more intellectual.</p> <p>所以就是当我们做配准时，我们移动了一点点图像，那么原先点上的像素应该是变成多少呢？使用插值法来计算。</p> <p>Actually we take account the neighboring pixels if they can play some role in the value of a new pixel. There is one way how we can compute it.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YYy2YJ4.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 20. Example of interpolation </div> </div> <p>We have four points $(0,0), (1,0), (0,1)$ and $(1,1)$. Let’s say there are pixels $1$, $2$, $3$ and $4$. And would like to find the value of a pixel which will be located here. And then the question is <strong>what will be the value of the pixel</strong>? We know the intensities of these four pixels, and also the corresponding $(x, y)$ of the target pixel.</p> <p>So the way to do it is to <strong>calculate the contribution</strong>. The contribution of this pixel to our new pixel. Let’s just imagine that this $x, y$ will be very close to $1$.</p> \[x, y\approx 0.999\] <p>So how similar should be the contribution of the pixel? Each of these four pixels or contribute something to the intensity.</p> <p>The contribution of $I(0, 0)$:</p> <ul> <li>Proportional to the opposite “rectangle”.</li> </ul> \[(1-x)\cdot(1-y)\cdot I(0, 0)\] <p>The same logical goal for other four pixels. So for pixel $(1,1)$, it’s contribution to the intensity of our new pixel through the proportional to this.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/J0lWvji.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 21. Contribution illustration </div> </div> \[\begin{align} I(\cdot) &amp;= (1-x)\cdot(1-y)\cdot I(0. 0) \\ &amp; + x\cdot (1-y)\cdot I(1, 0) \\ &amp; + (1-x)\cdot y \cdot I(0, 1) \\ &amp; + x \cdot y \cdot I(1, 1) \end{align}\] <p>So the rule is that we compute the <strong>size of these boxes</strong> and then <strong>multiplying them</strong> by the opposite pixel. Then, sum them up and get the final new value of the pixel.</p> <h4 id="image-registration-is-optimization">Image registration is optimization</h4> <p>We need to try to apply registration algorithm, try to optimise registration process.</p> <p>So what we can learn until now is we now know two pixels in the space and their neighborhood of them, how to compute how similar are they using different similarity measures based on pixel based comparison, like sum of quare differences or normalized cross correlation, or based on some advanced techniques like mutual information.</p> <p>We also know how this image moving will change if we move it 7.5 pixels up, how many intensities will change. =&gt; Interpolation</p> <p>So we know all this and we can try to formulate the registration, how the registration should be done.</p> <p>So what we need to do is we need to <strong>minimize</strong> depending on what kind of similarity measure we use. We want to minimize a distance metric.</p> \[\min\sum_{i, j}d\left(I(i, j) - J\left(x(i, j, \theta), y(i, j, \theta\right)\right)\] <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/fgLvTG6.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 22. Similarity measure </div> </div> <p>So here is our formulation of for registration optimization. What we need to find is to find $\theta$. <strong><em>The question is how to do it.</em></strong></p> <p>The common way to solve it is using an alogrithm called <strong><u>gradient descent algorithm</u></strong>. So let’s just choose the most simple similarity measures we have. So, for $d$, let’s choose the squared intensity difference. So basically, we just take a pixel and find the difference between each pixel individually, and that’s it. No mutual information or anything complex like that.</p> \[d(I(i, j), J(x, y)) = (I(i, j)-J(x, y))^2\] <p>And $\theta$ will only use one transformation - translation. So basically $F$ from $I$ to $J$ is basically a translation we would translate and the $I$ to some value that the $x$ and $J$ to some value.</p> \[f(I, J, \theta) - \text{translation using }\theta \\ f(i, j, \theta) = [i+\theta_x, j+\theta_y]\] <p>And this is one of the most simple configuration we can have, and now the question is <strong>how we get optimized solution</strong>? How we can find the optimal $\theta$? And the way to do it is to use the <strong>gradient descent algorithm</strong>.</p> <p>Then, the formulation of the problem is, <strong><u>we want to optimize</u></strong>:</p> \[E(\theta) = \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta))^2\] <h5 id="gradient-decent-algorithm">Gradient decent algorithm</h5> <p>By using <strong>gradient decent algorithm</strong>:</p> \[\nabla E(\theta) = \frac{\partial E(\theta)}{\partial \theta}\] <p>And move our solution move like take some $\theta$, change our $\theta$ according to the value of the gradient.</p> \[\theta_{t+1} = \theta_{t} + \eta\nabla E(\theta)\] <p>Usually weighting factor is something very small, like $0.01$. In deep learning, it is called, learning rate. So the question is how to find the gradient of the expression?</p> <p>In our case,</p> \[\nabla E(\theta) = \nabla \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))^2 = \\ -\sum_{i, j} 2(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))\cdot\frac{\partial J}{\partial \theta}\] <p>Now, the question is how to differentiate the $J$ against $\theta$.</p> \[\frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial X} \cdot \frac{\partial X}{\partial \theta}\] <p>And this is again a complex function, we need to first differentiate it against its coordinates (transformed coordinates), basically aginst $x$ and $y$.</p> <p>So if you want to differeniate an image against its coordinates, what it means is actually very simple thing.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Lx6vlHx.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> </div> \[\frac{\partial J}{\partial x} = \frac{1}{2}(I(x+1, y) - I(x-1, y))\] <p>Find two neighboring values for every pixel, subtract them from each other, and then multiply by 0.5. And then we do the same thing for the vertical direction.</p> \[\frac{\partial J}{\partial y} = \frac{1}{2}(I(x, y+1) - I(x, y-1))\] <p>Below is the illustration.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YQxD2o5.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 23. Understanding </div> </div> <p>This part is easy, but the question is how to find the last part on the abovementioned differentiation, the gradient of our coordinates against our transformation.</p> \[T(x, y) = \begin{bmatrix}1 \quad 0 \quad \theta_x\\0 \quad 1 \quad \theta_y\\ 0 \quad 0\quad 1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix} = [x+\theta_x, y+\theta_y]\\ \frac{\partial X}{\partial \theta} = \begin{bmatrix} \frac{\partial X}{\partial \theta_x}; \frac{\partial X}{\partial \theta_y}\end{bmatrix} = [1, 1]\] <p>Now, we have everything.</p> \[\frac{\partial J}{\partial \theta} = [\frac{1}{2}\left(I(x+1, y\right) - I(x-1, y), \frac{1}{2}(I(x, y+1) - I(x, y-1))]\] <p>This is just differentiaion of the image because our transformation is just translation. Then, we can compute the whole graient of our system.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/RkzH4GA.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 24. Whole gradient </div> </div> <p>Iteration and computation until it stops.</p> <h5 id="problem-with-optimization">Problem with optimization</h5> <p>This is a very expensive procedure to do as these similiarty measures $d(I, J)$ can be complex and expensive to compute (mutual information - regions around pixels).</p> <p>Another issue is that $\theta$ can contain many variables. In reality, $\theta$ can have thousands of variables, so each individual pixel can move individually. So each pixel will have transformations. Unlike in our case, only two variables.</p> <ul> <li> <p>9 for similarity transformations</p> \[\begin{bmatrix} \theta_1 \quad \theta _2 \quad \theta_3\\ \theta_4 \quad \theta_5 \quad \theta_6 \\ \theta_7 \quad \theta_8 \quad \theta_9 \end{bmatrix}\] </li> <li> <p>thousands for non-rigid transformations</p> </li> </ul> <p><strong>How to simplify this?</strong></p> <p>What people usually do is they use so-called <strong>multi-resolution pyramid registration</strong>.</p> <h5 id="multi-resolution-pyramid-registration">Multi-resolution pyramid registration</h5> <ul> <li>Downscale (and smooth) reference and moving images x16 (types)</li> <li>Register them and memorize transformation</li> <li>Downscale (and smooth) reference and moving images x8, apply memorized transformation</li> <li>Register images and combine transformations from both steps</li> <li>Continue</li> </ul> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/PiCMkeP.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 25. Multi-resolution pyramid registration </div> </div> <p>“多分辨率金字塔配准” 是一种在图像配准中使用的技术，特别是用于将两个具有不同分辨率或尺度的图像对准。它采用分层方法逐步对齐和优化两幅图像之间的配准。让我解释一下您提供的要点，以说明这个技术的工作原理：</p> <ol> <li> <strong>将参考图像和移动图像降采样（并平滑）16倍（类型）</strong>： <ul> <li>首先，参考图像和移动图像都会降低分辨率，降低16倍，并可选择平滑以减少噪音。</li> <li>这一步创建了图像的低分辨率版本。</li> </ul> </li> <li> <strong>将它们进行配准并记忆变换</strong>： <ul> <li>对降采样的参考图像和移动图像进行配准，即调整它们的空间对齐以找到最佳匹配。</li> <li>记忆或保存调整这些低分辨率图像的变换。</li> </ul> </li> <li> <strong>将参考图像和移动图像降采样（并平滑）8倍，应用记忆的变换</strong>： <ul> <li>参考图像和移动图像再次降低分辨率，这次降低8倍。</li> <li>先前记忆的变换（来自步骤2）被应用于这些低分辨率图像。</li> <li>这一步创建了更高分辨率但仍然减小比例的对齐图像。</li> </ul> </li> <li> <strong>对图像进行配准并结合来自两个步骤的变换</strong>： <ul> <li>来自步骤3的更高分辨率参考图像和移动图像进一步配准，进一步精细调整它们的对齐。</li> <li>从这个配准步骤获得的变换与步骤2中的记忆的变换组合在一起。</li> <li>这些变换的组合确保图像在更高分辨率下准确对齐。</li> </ul> </li> <li> <strong>继续</strong>： <ul> <li>这个过程以迭代方式继续，图像逐渐降低分辨率，逐步对齐和变换。</li> <li>在每个级别上，变换得到进一步的精细调整和组合。</li> <li>这一过程持续进行，直到达到最高所需分辨率或达到收敛条件。</li> </ul> </li> </ol> <p><strong>关键点</strong>：</p> <ul> <li>多分辨率金字塔配准是一种处理具有不同细节水平和分辨率的图像的常见策略。</li> <li>它从低分辨率开始进行粗略对齐，然后随着考虑到更高分辨率逐步优化对齐。</li> <li>这种方法可以显着加快配准过程，提高对齐的准确性，特别是在处理大型或复杂图像时。</li> </ul> <p>总的来说，多分辨率金字塔配准是一种强大的技术，用于稳健地对齐具有不同尺度或细节级别的图像，尤其在医学影像、计算机视觉和遥感等领域，其中图像可能具有不同的分辨率或包含细节信息。</p> <p>When you even do pyramid approach, there is still a problem in computation.</p> <p>In the pyramid approach, people don’t do it for every pixel, people do it for a grid. So you just define a grid of points. For example, the size of the image is $100 \times 100$, but the number of grid points is $10 \times 10$. So we just place great place grid points at $10 \times 10$, skip every pixels and base with grids. And what we do is we compute our similarity measure only for this points. And we compute the optimization only for these points. And then we somehow inerpolate the transformation for the internal points.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DV1yHKh.png" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 26. Grid-based registration </div> </div> <ul> <li>Do not perform registration per pixel but use a sparse grid</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/line-segment-intersection/">Line Segment Intersection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/graduation/">Graduation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/swav/">Review of SwAV</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/simsiam/">Review of SimSiam</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/byol/">Extracts from BYOL</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"LiuYing-1/liuying-1.github.io","data-repo-id":"R_kgDOMScbNQ","data-category":"Announcements","data-category-id":"DIC_kwDOMScbNc4Cgruw","data-mapping":"pathname","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Y. Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JC70RZ57BT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JC70RZ57BT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"CV",description:"Here is a brief overview of my academic and professional background, and hope there is something interesting for you. The more comprehensive version can be accessed by clicking the PDF icon on right top corner of this page.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-line-segment-intersection",title:"Line Segment Intersection",description:"Self-learning note for CG - Line Segment Intersection.",section:"Posts",handler:()=>{window.location.href="/blog/2024/line-segment-intersection/"}},{id:"post-graduation",title:"Graduation",description:"Congratulations to my graduation from the DIKU, University of Copenhagen.",section:"Posts",handler:()=>{window.location.href="/blog/2024/graduation/"}},{id:"post-review-of-swav",title:"Review of SwAV",description:"This is the learning of the paper SwAV.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/swav.pdf"}},{id:"post-review-of-simsiam",title:"Review of SimSiam",description:"This is the learning of the paper SimSiam.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/SimSIAM.pdf"}},{id:"post-extracts-from-byol",title:"Extracts from BYOL",description:"Review of the paper BYOL and to extract the main points",section:"Posts",handler:()=>{window.location.href="/blog/2024/byol/"}},{id:"post-cookbook-reading-1",title:"Cookbook Reading - 1",description:"This is the first part of this book.",section:"Posts",handler:()=>{window.location.href="/blog/2024/cookbook-1/"}},{id:"post-advanced-image-registration",title:"Advanced Image Registration",description:"Self-learning note for advanced image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/advanced-image-registration/"}},{id:"post-image-registration-basics",title:"Image Registration Basics",description:"Learning note for medical image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/image-registration-l1/"}},{id:"post-segmentation-basics",title:"Segmentation Basics",description:"Learning note for medical image segmentation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation-basics/"}},{id:"post-inpainting",title:"Inpainting",description:"Learning note for inpainting.",section:"Posts",handler:()=>{window.location.href="/blog/2023/inpainting/"}},{id:"post-magnetic-resonance",title:"Magnetic Resonance",description:"Preview of the lecture for MRI.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-mr/"}},{id:"post-x-ray-and-ct",title:"X-ray and CT",description:"This is the first lecture of the course Medical Image Analysis at UCPH.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-l1/"}},{id:"post-dansk-gt-0307",title:"Dansk &gt; 0307",description:"This is the learning note for my first lesson at Ucplus.",section:"Posts",handler:()=>{window.location.href="/blog/2023/dansk-week-1/"}},{id:"post-dictation-gt-the-art-of-balancing-stones",title:"Dictation &gt; The Art of Balancing Stones",description:"This is a daily dictation 2 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-art-of-balancing-stones/"}},{id:"post-dictation-gt-the-egg",title:"Dictation &gt; The Egg",description:"This is a daily dictation 1 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-egg/"}},{id:"post-neural-network-ii-mlops",title:"Neural Network II | MLOps",description:"This is the lecture for Neural Network II and MLOps.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ml-ops/"}},{id:"post-neural-network",title:"Neural Network",description:"Recap of Neural Network",section:"Posts",handler:()=>{window.location.href="/blog/2023/neural-network-recap/"}},{id:"post-easter-lunch",title:"Easter Lunch",description:"This is the first time to have lunch with my Danish family at Easter.",section:"Posts",handler:()=>{window.location.href="/blog/2023/easter-lunch/"}},{id:"post-segmentation",title:"Segmentation",description:"Image Segmentation, including Hough Transform",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation/"}},{id:"post-features",title:"Features",description:"Image feature detection and matching with application.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/features.pdf"}},{id:"post-transformation",title:"Transformation",description:"Affine, Rigid, Perspective linear transformations, etc.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/geometry.pdf"}},{id:"post-deconvolution",title:"Deconvolution",description:"Image Restoration by Deconvolution",section:"Posts",handler:()=>{window.location.href="/blog/2023/deconvolution/"}},{id:"post-histogram",title:"Histogram",description:"Thresholding segmentation and histogram techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2023/histogram/"}},{id:"post-fourier",title:"Fourier",description:"Complex numbers and Fourier transformation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/fourier/"}},{id:"post-filtering",title:"Filtering",description:"Linear and non-linear filtering, derivative operators.",section:"Posts",handler:()=>{window.location.href="/blog/2023/filtering/"}},{id:"post-convolution",title:"Convolution",description:"Pixel-wise Operations, Intensity, Transformations, Image Formation, and the Convolution Integral.",section:"Posts",handler:()=>{window.location.href="/blog/2023/convolution/"}},{id:"post-app-and-term",title:"App and Term",description:"Introduction of the Signal and Image Processing course, including the basic concepts, terminology, and applications.",section:"Posts",handler:()=>{window.location.href="/blog/2023/intro/"}},{id:"post-aads-grade",title:"AADS Grade",description:"This is to record I got 12 on the course Advanced Algorithms and Data Structures.",section:"Posts",handler:()=>{window.location.href="/blog/2023/aads-grade/"}},{id:"post-polygon-triangulation",title:"Polygon Triangulation",description:"This is the learning note for Polygon Triangulation.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/polygon.pdf"}},{id:"post-approximation",title:"Approximation",description:"This is the learning note for Approximation algorithm - 1.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/approx1.pdf"}},{id:"post-exact-parameterized",title:"Exact-Parameterized",description:"This is the learning note for Exact-Parameterized algorithm.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/EE.pdf"}},{id:"post-npc",title:"NPC",description:"This is the learning note for NPC.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/NPC.pdf"}},{id:"post-van-emde-boas-tree",title:"van Emde Boas Tree",description:"This is the learning note for vEB.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/van-Emde-Boas-Trees.pdf"}},{id:"post-sad-mood",title:"Sad Mood",description:"This blog is to record my current sad mood.",section:"Posts",handler:()=>{window.location.href="/blog/2023/sad-mood/"}},{id:"post-hashing",title:"Hashing",description:"This is the learning note for Hashing.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Hashing.pdf"}},{id:"post-randomized-algorithms",title:"Randomized Algorithms",description:"This is the learning note for the Randomized Algorithms.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/RA.pdf"}},{id:"post-linear-programming",title:"Linear Programming",description:"This is the learning note for the Linear Programming.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Linear-programming.pdf"}},{id:"post-max-flow",title:"Max-flow",description:"This is the learning note for the Max-flow.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Max-flow.pdf"}},{id:"post-recovery-gt-crash",title:"Recovery &gt; Crash",description:"This is the special case of the recovery when the system crashes.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-crash/"}},{id:"post-recovery-gt-normal",title:"Recovery &gt; Normal",description:"This is the special part for recovery and normal.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-normal/"}},{id:"post-experiment-recovery",title:"Experiment | Recovery",description:"Here are the chapters of Experiments and Recovery.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recover/"}},{id:"post-concourrency-2",title:"Concourrency 2",description:"This is the chapter relating to concurrency control - Part 2.",section:"Posts",handler:()=>{window.location.href="/blog/2023/concurrency-2/"}},{id:"post-concourrency-1",title:"Concourrency 1",description:"This is the chapter relating to concurrency control.",section:"Posts",handler:()=>{window.location.href="/blog/2022/concurrency/"}},{id:"post-godt-nyt\xe5r",title:"Godt Nyt\xe5r",description:"The blog is to record my first New Year&#39;s Eve in Denmark!",section:"Posts",handler:()=>{window.location.href="/blog/2022/godt-nytar/"}},{id:"post-incorrect-url",title:"Incorrect URL",description:"Last time, my neighbor, whose world ranking in CSGO is the Global Elite, received a Christmas gift by his nisse.",section:"Posts",handler:()=>{window.location.href="/blog/2022/incorrect-url/"}},{id:"post-rpc-performance",title:"RPC | Performance",description:"RPC and Permance are the two topics we will cover today.",section:"Posts",handler:()=>{window.location.href="/blog/2022/rpc/"}},{id:"post-abstractions",title:"Abstractions",description:"Basic concepts of computer systems.",section:"Posts",handler:()=>{window.location.href="/blog/2022/abstractions/"}},{id:"post-tivoli-firework",title:"Tivoli Firework",description:"To accompany Jianxiang Yu, we went to the second oldest theme park worldwide, Tivoli in Denmark yesterday.",section:"Posts",handler:()=>{window.location.href="/blog/2022/tivoli-firework/"}},{id:"post-royal-guardian",title:"Royal Guardian",description:"The shift of the royal guardian in Marmorkirken.",section:"Posts",handler:()=>{window.location.href="/blog/2022/royal-gardian/"}},{id:"post-swan-nearby",title:"Swan Nearby",description:"A record to keep this swan in my memory.",section:"Posts",handler:()=>{window.location.href="/blog/2022/swan-nearby/"}},{id:"post-chef-first-time",title:"Chef First Time",description:"This is my first time being the chef to cook for my danish family!",section:"Posts",handler:()=>{window.location.href="/blog/2022/chief-first-time/"}},{id:"post-travel-to-malm\xf6",title:"Travel to Malm\xf6",description:"This is the final version of the image preprocessing with the limitation that cannot automatically crop the image in the center. However, it can work properly in normal situations. Also, this blog is used to record my trip to Malm\xf6, Sweden with Xuanlang.",section:"Posts",handler:()=>{window.location.href="/blog/2022/travel-to-malmo/"}},{id:"post-dev-progress",title:"Dev Progress",description:"In this version, the development of my blog website is already passed half. \u8fd9\u91cc\u662f\u4e2d\u6587\u5b57\u4f53\u6d4b\u8bd5\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2022/dev-progress/"}},{id:"post-take-metro",title:"Take Metro",description:"This blog is to record my route from DIKU back to my dorm at 00:53 am several days ago.",section:"Posts",handler:()=>{window.location.href="/blog/2022/take-metro/"}},{id:"post-start-my-blog",title:"Start My Blog",description:"Memorize the start of the implementation of my blog-web.",section:"Posts",handler:()=>{window.location.href="/blog/2022/start-my-blog/"}},{id:"news-i-have-defended-my-master-thesis-quot-exploration-of-self-supervised-learning-methods-for-longitudinal-image-analysis-quot-and-received-my-master-39-s-degree-meanwhile-i-have-been-offered-a-job-as-an-ai-engineer-in-a-company",title:"I have defended my master thesis **&quot;Exploration of Self-Supervised Learning Methods for Longitudinal Image Analysis&quot;** and received my Master&#39;s Degree. Meanwhile, I have been offered a job as an **AI Engineer** in a company. \ud83c\udf93\ud83c\udf89\ud83d\ude80",description:"",section:"News"},{id:"news-going-to-be-one-of-tas-for-the-course-advanced-deep-learning-in-b4-2024-at-university-of-copenhagen",title:"\ud83e\udd70 Going to be one of TAs for the course **Advanced Deep Learning** in B4-2024 at University of Copenhagen.",description:"",section:"News"},{id:"news-congrats-to-me-for-passing-the-courses-medical-image-analysis-advanced-topics-in-image-analysis-and-project-of-research-in-self-supervised-learning-methods-for-longitudinal-images-with-10-out-of-12",title:"\ud83e\udd73 Congrats to me for passing the courses **Medical Image Analysis**, **Advanced Topics in Image Analysis**, and **Project of Research in Self-Supervised Learning Methods for Longitudinal Images** with **10 out of 12**.",description:"",section:"News"},{id:"news-i-am-happy-to-share-i-got-12-out-of-12-in-the-courses-of-advanced-algorithms-and-data-structures-signal-and-image-processing-and-advanced-deep-learning",title:"\ud83c\udf7b I am happy to share I got **12 out of 12** in the courses of **Advanced Algorithms and Data Structures**, **Signal and Image Processing**, and **Advanced Deep Learning**.",description:"",section:"News"},{id:"news-learning-notes-advanced-algorithms-and-data-structures-https-liuying-1-github-io-blog-tag-aads-signal-and-image-processing-https-liuying-1-github-io-blog-tag-sip-and-advanced-computer-systems-https-liuying-1-github-io-blog-tag-acs-have-been-transferred-from-my-self-developed-website-to-this-one",title:"\ud83d\udcd4 Learning notes [**Advanced Algorithms and Data Structures**](https://liuying-1.github.io/blog/tag/aads), [**Signal and Image Processing**](https://liuying-1.github.io/blog/tag/sip), and [**Advanced Computer Systems**](https://liuying-1.github.io/blog/tag/acs) have been transferred from my self-developed website to this one.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%79%35%38%31%30%39%39@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/liu-ying-463ab925b","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/100085288350390","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>