<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="googlece17f0a456a89a36.html"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Inpainting | Y. Liu </title> <meta name="author" content="Y. Liu"> <meta name="description" content="Learning note for inpainting."> <meta name="keywords" content="ku, ucph, copenhagen, diku, portfolio-website, liuying, dk, yingliu, ying liu, liu ying"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?157bbd74cef60250fd5a67a2e078966e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liuying-1.github.io/blog/2023/inpainting/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Y.</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Inpainting</h1> <p class="post-meta"> Created in September 19, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/ssl"> <i class="fa-solid fa-hashtag fa-sm"></i> ssl</a>   ·   <a href="/blog/category/study"> <i class="fa-solid fa-tag fa-sm"></i> study</a>   <a href="/blog/category/ucph"> <i class="fa-solid fa-tag fa-sm"></i> ucph</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Inpainting, as known as <strong>predict missing pixels</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/KLK2ZIN.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 1. Context Encoders: Feature Learning by Inpainting (Pathak et al., 2016) </div> <p><strong><em>Disclaimer: Below are the extraction from the original paper.</em></strong> It will be reposted once I finished read all of them when I am available.</p> <p><strong><em>Reference: Context Encoders: Feature Learning by Inpainting</em></strong></p> <hr> <h4 id="abstraction">Abstraction</h4> <p>We present <strong><u>an unsupervised visual feature learning algorithm</u></strong> driven by context-based pixel prediction.</p> <p>Context Encoders - a <strong>convolutional neural network</strong> trained to <strong>generate the contents of an arbitrary image region conditioned on its surroundings</strong>.</p> <p>To succeed at this task, context encoders need to both <strong>understand the content of the entire image,</strong> as well as <strong>produce a plausible hypothesis for the missing parts</strong>.</p> <p>When training context encoders, we have experimented with both <strong>a standard pixel-wise reconstruction loss</strong>, as well as <strong>a reconstruction plus an adversarial loss</strong>.</p> <p>The <strong>latter produces much sharper results</strong> because it can <strong>better handle multiple modes</strong> in the output.</p> <p>A context encoder learns <strong>a representation that captures not just appearance</strong> but also the <strong>semantics of visual structures</strong>. ==&gt; <em>Question</em></p> <p>Context encoders can be used for <strong>semantic inpainting tasks</strong>, either <strong>stand-alone</strong> or <strong>as initialization for non-parametric methods</strong>.</p> <h4 id="introduction">Introduction</h4> <p>Our visual world is very diverse, yet <strong>highly structured</strong>, and humans have an uncanny ability to make sense of this structure.</p> <p>This work is to explore <strong>whether state-of-the-art computer vision algorithms can do the sam</strong>e.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/nFaEBX3.png" alt="image-20230919205428308" class="img-fluid rounded z-depth-1" data-zoomable=""> </div> </div> <div class="caption"> Figure 2. Demonstration </div> <p>Although the center part of the image is missing, most of us can easily imaging its content from the surrounding pixels, without having ever seen that exact scene. Some of us can even draw it.</p> <p>This ability comes from the fact that <strong>natural images, despite their diversity, are high structured</strong>. In this case, the regular pattern of windows on the facade. We <strong>humans</strong> are able to <strong>understand this structure</strong> and <strong>make visual predictions</strong> even when seeing <strong>only parts of the scene</strong>.</p> <p>In this paper, we show that <strong>it is possible to learn and predict this structure</strong> using <strong>convolutional neural networks</strong> (CNNs), a class of models that have recently shown success across a variety of <strong>image understanding tasks</strong>.</p> <p>Given <strong>an image with a missing region</strong>, we train a convolutional neural network to <strong>regress to the missing pixel values</strong>. <em>Question</em></p> <p>We call our model <strong>context encoder</strong>, as it consists of <strong>an encoder capturing the context of an image into a compact latent feature representation</strong> and <strong>a decoder which uses that representation to produce the missing image content</strong>.</p> <p>The context encoder is closely related to auto encoders, as it shares a <strong>similar encoder-decoder architecture</strong>.</p> <p><strong>Autoencoders</strong> <strong>take an input image</strong> and try to <strong>reconstruct it</strong> after it passes through a low-dimensional “bottleneck” layer, with the aim of <strong>obtaining a compact feature representation</strong> is likely to <strong>just compresses the image content without learning a semantically meaningful representation</strong>. ==&gt; Encoder has done the abovementioned, the decoder is to map back to the original datashape.</p> <hr> <p>ChatGPT gives the following explaination to autoencoder.</p> <ol> <li>“<strong>Autoencoders</strong>”: Autoencoders are a class of neural network architectures used for unsupervised learning. They consist of an encoder and a decoder, and their <u>primary purpose is to learn a compressed representation of the input data</u>.</li> <li>“<strong>Take an input image</strong>”: Autoencoders typically operate on input data, which in this case is an image. The image is passed through the network.</li> <li>“<strong>Reconstruct it</strong>”: The <u>core task of an autoencoder is to take the input image</u> and attempt to <strong><u>generate an output (reconstruction) that is as close as possible to the original input</u></strong>. <strong><u>In essence, it tries to replicate the input</u></strong>.</li> <li>“<strong>After it passes through a low-dimensional ‘bottleneck’ layer</strong>”: Autoencoders have an intermediate layer called the bottleneck or latent space, which has a lower dimension than the input data. This layer forces the network to <strong><u>learn a compressed representation of the input</u></strong> data.</li> <li>“<strong>With the aim of obtaining a compact feature representation</strong>”: The primary objective of this compression process is to <strong><u>obtain a compact feature representation of the input</u></strong> data. This means that the autoencoder is trying to <u>capture the most important and salient features of the input</u> data in a condensed form.</li> <li>“<strong>Is likely to just compresses the image content without learning a semantically meaningful representation</strong>”: This part of the statement suggests <u>a potential limitation of traditional autoencoders</u>. In some cases, when using basic autoencoder architectures and simple loss functions like mean squared error, the learned compressed representation may indeed <strong><u>focus more on compressing the raw image content</u></strong> (e.g., pixel values) rather than <strong><u>capturing higher-level, semantically meaningful features</u></strong> (e.g., object shapes or meaningful abstractions).</li> </ol> <p>In other words, <strong>traditional autoencoders</strong> may prioritize achieving a low reconstruction error (<strong><u>faithfully reproducing the input</u></strong>) over learning abstract, semantic information. However, this limitation can be addressed or mitigated by modifying the architecture, loss functions, or training strategies, as mentioned in the previous response. <strong><u>More advanced autoencoder</u></strong> variants, like variational autoencoders (VAEs) or denoising autoencoders, <strong><u>are designed to encourage the learning of semantically meaningful representations</u></strong>.</p> <hr> <p><strong>Denoising autoencoders</strong> address this issue by <strong>corrupting the input image</strong> and requirng the network to <strong>undo</strong> the damage. However, this corruption process is typically very <strong>localized and low-level</strong>, and does <strong>not require much semantic information</strong> to undo.</p> <hr> <p>ChatGPT explains what are localized and low-level. <img class="emoji" title=":point_down:" alt=":point_down:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png" height="20" width="20"></p> <ol> <li>“<strong>Localized</strong>”: When it is said that the corruption process is “localized,” it means that the noise or distortion introduced to the input data <strong>tends to affect only a small portion of the data</strong>. In the context of images, for example, localized corruption might involve adding noise to individual pixels or small regions of the image. It does not involve large-scale changes or alterations to the entire image.</li> <li>“<strong>Low-level</strong>”: The term “low-level” refers to the nature of the corruption. Low-level corruptions are simple and typically involve basic operations on the data, such as adding random noise, blurring, or altering individual data points (e.g., pixel values in an image). These corruptions do not carry complex semantic meanings or involve high-level abstractions. They are often applied at the level of raw data elements.</li> </ol> <p>In the context of denoising autoencoders, the idea is that <strong>the noise or distortions introduced to the input</strong> data are <strong>relatively simple</strong> and <strong>affect only small parts</strong> of the data. Because these <strong>corruptions are not complex or semantically rich</strong>, the network may <strong>not need to learn deep</strong>, <strong>high-level semantics to undo</strong> the effects of the corruption. Instead, it can focus on relatively shallow and low-level operations to recover the clean data. This is why the statement suggests that denoising autoencoders may not necessarily learn highly meaningful semantic representations since the corruption process itself is not very demanding in terms of semantic understanding.</p> <hr> <p>In contrast, our context encoder needs to solve <strong><u>a much harder task</u></strong>: <strong><u>to fill in large missing areas of the image, where it can't get "hints" from nearby pixels.</u></strong></p> <p>This requires <strong>a much deeper semantic understanding of the scene</strong>, and the <strong>ability to synthesize high-level features over large spatial extents</strong>.</p> <p>For example, an entire window needs to be <strong>conjured up “out of thin air”</strong> in Figure 2 above.</p> <p>This is similar in spirit to wrd2vec which <strong><u>learns word representation from natural language sentences by predicting a word given its context</u></strong>.</p> <p>Like autoencoders, <strong><u>context encoders</u></strong> are <strong><u>trained in a completely unsupervised manner</u></strong>.</p> <p>Our results demonstrate that in order to succeed at this task, a model needs to <strong><u>both understand the content of an image</u></strong>, as well as <strong><u>produce a plausible hypothesis for the missing parts</u></strong>.</p> <p>This task, however, is inherently <strong>multi-modal</strong> as there are <strong>multiple ways</strong> to <strong>fill the missing region while also maintaining coherence with the given context</strong>.</p> <p>We <strong>decouple this burden</strong> in our loss function by <strong>jointly training our context encoders</strong> to <strong>minimize both a reconstruction loss and adversarial loss</strong>.</p> <p>The reconstruction loss (L2) <strong>captures the overall structure</strong> of the missing region in relation to the context, while the adversarial loss has the effect of <strong>picking a particular mode</strong> from the distribution.</p> <ul> <li>“<strong>Reconstruction loss (L2)</strong>”: This loss function measures how well the generated output (the completed or inpainted region) matches the original data. It’s often calculated as the mean squared difference between the generated output and the ground truth (original data). In other words, it captures how well the generated content resembles the actual missing region in terms of overall structure.</li> <li>“<strong>Adversarial loss</strong>”: This type of loss function is typically associated with GANs. It involves a discriminator network that tries to distinguish between real (ground truth) data and generated data. The adversarial loss <strong>encourages the generated data to be more realistic</strong> and closer to the distribution of real data. In this context, it is mentioned that the adversarial loss has the effect of selecting a particular mode from the distribution, which means <strong>it encourages the generated content to be more similar to specific instances of real data</strong> (modes) rather than just any possible output.</li> </ul> <p>So, the overall strategy described here is to <strong>simultaneously train context encoders</strong> to <strong>generate missing regions</strong> in a way that <strong>not only captures the overall structure</strong> (reconstruction loss) but also <strong>encourages the generated content to resemble specific instances of real data</strong> (adversarial loss). This approach aims to <strong>strike a balance between maintaining coherence with the context and producing realistic</strong>, data-driven details in the inpainted or completed regions.</p> <p>Figure 2 shows that using only the <strong>reconstruction loss produces blurry results</strong>, whereas <strong>adding the adversarial loss results in much sharper predictions</strong>.</p> <p><strong><u>Evaluate the encoder and the decoder independently.</u></strong></p> <p>On the encoder, we show that <strong>encoding</strong> just the <strong>context of an image patch</strong> and using the <strong>resulting feature</strong> to <strong>retrieve nearest neighbor contexts from a dataset</strong> produces patches which are semantically similar to the original (unseen) patch.</p> <ul> <li>After encoding the context of an image patch, the authors take the <strong>resulting feature representation</strong> and <strong>use it to find other image patches in a dataset that have similar feature representations</strong>. In other words, they <strong>look for other patches that share similar contextual characteristics</strong> based on the extracted features.</li> <li>The key result or observation here is that when they retrieve image patches from the dataset based on the feature representation of the context, the <strong>retrieved patches are found to be “semantically similar” to the original, unseen patch</strong>. This means that <strong>the retrieved patches share meaningful visual or structural characteristics with the original patch</strong>, even though the encoder processed only the context of the patch.</li> </ul> <p><strong>By encoding only the context of an image patch</strong> and using the <strong>resulting features to find similar contexts in a dataset</strong>, they can <strong>retrieve other patches that semantically similar to the original patch</strong>. This suggests that the context encoder is <strong>effective at capturing meaningful contextual information from images</strong>, even without considering the patch itself.</p> <p>We further validate the quality of the learned feature representation by fine-tuning the encoder for a variety of image understanding tasks, including classfication, object detection, and semantic segmentation.</p> <p>We are competitive with the state-of-the-art unsupervised/self-supervised methods on those tasks.</p> <p>On the decoder side, we <strong>show that our method is often able to fill in realistic image content</strong>.</p> <p>Indeed, to the best of our knowledge, ours is the <strong>first parametric inpainting algorithm</strong> that is able to <strong>give resonable results for semantic hole-filling</strong> (large missing regions).</p> <p>The context encoder can also be useful as a better visual feature for computing nearest neighbors in non-parametric inpainting methods.</p> <h4 id="related-work">Related work</h4> <p>Computer vision has made tremendous progress on <strong>semantic image understanding tasks</strong> such as classification, object detection, and segmentation in the past decade.</p> <p>Recently, <strong>Convolutional Neural Networks (CNNs)</strong> have greatly advanced the performance in these tasks. The success of such models on image classification paved the way to tackle harder problems, including unsupervised understanding and generation of natural images.</p> <p>We briefly review the related work in each of the sub-fields pertaining to this paper.</p> <h5 id="unsupervised-learning"><strong>Unsupervised learning</strong></h5> <p><strong>CNNs trained for ImageNet classification</strong> with over a million <strong>labeled examples</strong> learn features which generalize very well across tasks. However, <strong>whether such semantically informative and generalizable features can be learned from raw images alone, without any labels</strong>, remains an open question. Some of the earliest work in deep unsupervised learning are autoencoders. Along similar lines, denoising autoencoders reconstruct the image from local corruptions, to make encoding robust to such corruptions. While context encoders <strong>could be thought of as a variant of denoising autoencoders</strong>, the corruption applied to the model’s input <strong>is spatially much larger</strong>, requiring <strong>more semantic information</strong> to undo.</p> <h5 id="weakly-supervised-and-self-supervised-learning">Weakly-supervised and self-supervised learning</h5> <p>Very recently, there has been significant interesting in learning meaningful representations using weakly-supervised and self-supervised learning. One useful source of supervision is to <strong>use the temporal information</strong> contained in videos. <strong>Consistency across temporal frames</strong> has been used as supervision to learn embeddings which perform well on a number of tasks. Another way to use consistency is to track patches in frames of video containing task-relevant attributes and use the coherence of tracked patches to guide the training.</p> <p>Most closely related to the present paper are efforts at <strong>exploiting spatial context as a source of free and plentiful supervisory signal</strong>. Recently, Doersch et al. used the task of <strong>predicting the relative positions of neighboring patches within an image</strong> as a way to train an unsupervised deep feature representations.</p> <p>Our context encoder solves <strong>a pure prediction problem</strong> (what pixel intensities should go in the hole?).</p> <p>In contrast, <strong>context encoders can be applied to any unlabeled image database</strong> and <strong>learn to generate images based on the surrounding context</strong>.</p> <p>The later part in the article is not that related to my interests. I will repost this blog as soon as possible to form my own interpretation focus on the technique, <strong>inpainting</strong>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/line-segment-intersection/">Line Segment Intersection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/graduation/">Graduation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/swav/">Review of SwAV</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/simsiam/">Review of SimSiam</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/byol/">Extracts from BYOL</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Y. Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JC70RZ57BT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JC70RZ57BT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"CV",description:"Here is a brief overview of my academic and professional background, and hope there is something interesting for you. The more comprehensive version can be accessed by clicking the PDF icon on right top corner of this page.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-line-segment-intersection",title:"Line Segment Intersection",description:"Self-learning note for CG - Line Segment Intersection.",section:"Posts",handler:()=>{window.location.href="/blog/2024/line-segment-intersection/"}},{id:"post-graduation",title:"Graduation",description:"Congratulations to my graduation from the DIKU, University of Copenhagen.",section:"Posts",handler:()=>{window.location.href="/blog/2024/graduation/"}},{id:"post-review-of-swav",title:"Review of SwAV",description:"This is the learning of the paper SwAV.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/swav.pdf"}},{id:"post-review-of-simsiam",title:"Review of SimSiam",description:"This is the learning of the paper SimSiam.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/SimSIAM.pdf"}},{id:"post-extracts-from-byol",title:"Extracts from BYOL",description:"Review of the paper BYOL and to extract the main points",section:"Posts",handler:()=>{window.location.href="/blog/2024/byol/"}},{id:"post-cookbook-reading-1",title:"Cookbook Reading - 1",description:"This is the first part of this book.",section:"Posts",handler:()=>{window.location.href="/blog/2024/cookbook-1/"}},{id:"post-advanced-image-registration",title:"Advanced Image Registration",description:"Self-learning note for advanced image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/advanced-image-registration/"}},{id:"post-image-registration-basics",title:"Image Registration Basics",description:"Learning note for medical image registration.",section:"Posts",handler:()=>{window.location.href="/blog/2023/image-registration-l1/"}},{id:"post-segmentation-basics",title:"Segmentation Basics",description:"Learning note for medical image segmentation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation-basics/"}},{id:"post-inpainting",title:"Inpainting",description:"Learning note for inpainting.",section:"Posts",handler:()=>{window.location.href="/blog/2023/inpainting/"}},{id:"post-magnetic-resonance",title:"Magnetic Resonance",description:"Preview of the lecture for MRI.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-mr/"}},{id:"post-x-ray-and-ct",title:"X-ray and CT",description:"This is the first lecture of the course Medical Image Analysis at UCPH.",section:"Posts",handler:()=>{window.location.href="/blog/2023/mia-l1/"}},{id:"post-dansk-gt-0307",title:"Dansk &gt; 0307",description:"This is the learning note for my first lesson at Ucplus.",section:"Posts",handler:()=>{window.location.href="/blog/2023/dansk-week-1/"}},{id:"post-dictation-gt-the-art-of-balancing-stones",title:"Dictation &gt; The Art of Balancing Stones",description:"This is a daily dictation 2 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-art-of-balancing-stones/"}},{id:"post-dictation-gt-the-egg",title:"Dictation &gt; The Egg",description:"This is a daily dictation 1 for English improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-egg/"}},{id:"post-neural-network-ii-mlops",title:"Neural Network II | MLOps",description:"This is the lecture for Neural Network II and MLOps.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ml-ops/"}},{id:"post-neural-network",title:"Neural Network",description:"Recap of Neural Network",section:"Posts",handler:()=>{window.location.href="/blog/2023/neural-network-recap/"}},{id:"post-easter-lunch",title:"Easter Lunch",description:"This is the first time to have lunch with my Danish family at Easter.",section:"Posts",handler:()=>{window.location.href="/blog/2023/easter-lunch/"}},{id:"post-segmentation",title:"Segmentation",description:"Image Segmentation, including Hough Transform",section:"Posts",handler:()=>{window.location.href="/blog/2023/segmentation/"}},{id:"post-features",title:"Features",description:"Image feature detection and matching with application.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/features.pdf"}},{id:"post-transformation",title:"Transformation",description:"Affine, Rigid, Perspective linear transformations, etc.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/geometry.pdf"}},{id:"post-deconvolution",title:"Deconvolution",description:"Image Restoration by Deconvolution",section:"Posts",handler:()=>{window.location.href="/blog/2023/deconvolution/"}},{id:"post-histogram",title:"Histogram",description:"Thresholding segmentation and histogram techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2023/histogram/"}},{id:"post-fourier",title:"Fourier",description:"Complex numbers and Fourier transformation.",section:"Posts",handler:()=>{window.location.href="/blog/2023/fourier/"}},{id:"post-filtering",title:"Filtering",description:"Linear and non-linear filtering, derivative operators.",section:"Posts",handler:()=>{window.location.href="/blog/2023/filtering/"}},{id:"post-convolution",title:"Convolution",description:"Pixel-wise Operations, Intensity, Transformations, Image Formation, and the Convolution Integral.",section:"Posts",handler:()=>{window.location.href="/blog/2023/convolution/"}},{id:"post-app-and-term",title:"App and Term",description:"Introduction of the Signal and Image Processing course, including the basic concepts, terminology, and applications.",section:"Posts",handler:()=>{window.location.href="/blog/2023/intro/"}},{id:"post-aads-grade",title:"AADS Grade",description:"This is to record I got 12 on the course Advanced Algorithms and Data Structures.",section:"Posts",handler:()=>{window.location.href="/blog/2023/aads-grade/"}},{id:"post-polygon-triangulation",title:"Polygon Triangulation",description:"This is the learning note for Polygon Triangulation.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/polygon.pdf"}},{id:"post-approximation",title:"Approximation",description:"This is the learning note for Approximation algorithm - 1.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/approx1.pdf"}},{id:"post-exact-parameterized",title:"Exact-Parameterized",description:"This is the learning note for Exact-Parameterized algorithm.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/EE.pdf"}},{id:"post-npc",title:"NPC",description:"This is the learning note for NPC.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/NPC.pdf"}},{id:"post-van-emde-boas-tree",title:"van Emde Boas Tree",description:"This is the learning note for vEB.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/van-Emde-Boas-Trees.pdf"}},{id:"post-sad-mood",title:"Sad Mood",description:"This blog is to record my current sad mood.",section:"Posts",handler:()=>{window.location.href="/blog/2023/sad-mood/"}},{id:"post-hashing",title:"Hashing",description:"This is the learning note for Hashing.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Hashing.pdf"}},{id:"post-randomized-algorithms",title:"Randomized Algorithms",description:"This is the learning note for the Randomized Algorithms.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/RA.pdf"}},{id:"post-linear-programming",title:"Linear Programming",description:"This is the learning note for the Linear Programming.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Linear-programming.pdf"}},{id:"post-max-flow",title:"Max-flow",description:"This is the learning note for the Max-flow.",section:"Posts",handler:()=>{window.location.href="/assets/pdf/Max-flow.pdf"}},{id:"post-recovery-gt-crash",title:"Recovery &gt; Crash",description:"This is the special case of the recovery when the system crashes.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-crash/"}},{id:"post-recovery-gt-normal",title:"Recovery &gt; Normal",description:"This is the special part for recovery and normal.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recovery-normal/"}},{id:"post-experiment-recovery",title:"Experiment | Recovery",description:"Here are the chapters of Experiments and Recovery.",section:"Posts",handler:()=>{window.location.href="/blog/2023/recover/"}},{id:"post-concourrency-2",title:"Concourrency 2",description:"This is the chapter relating to concurrency control - Part 2.",section:"Posts",handler:()=>{window.location.href="/blog/2023/concurrency-2/"}},{id:"post-concourrency-1",title:"Concourrency 1",description:"This is the chapter relating to concurrency control.",section:"Posts",handler:()=>{window.location.href="/blog/2022/concurrency/"}},{id:"post-godt-nyt\xe5r",title:"Godt Nyt\xe5r",description:"The blog is to record my first New Year&#39;s Eve in Denmark!",section:"Posts",handler:()=>{window.location.href="/blog/2022/godt-nytar/"}},{id:"post-incorrect-url",title:"Incorrect URL",description:"Last time, my neighbor, whose world ranking in CSGO is the Global Elite, received a Christmas gift by his nisse.",section:"Posts",handler:()=>{window.location.href="/blog/2022/incorrect-url/"}},{id:"post-rpc-performance",title:"RPC | Performance",description:"RPC and Permance are the two topics we will cover today.",section:"Posts",handler:()=>{window.location.href="/blog/2022/rpc/"}},{id:"post-abstractions",title:"Abstractions",description:"Basic concepts of computer systems.",section:"Posts",handler:()=>{window.location.href="/blog/2022/abstractions/"}},{id:"post-tivoli-firework",title:"Tivoli Firework",description:"To accompany Jianxiang Yu, we went to the second oldest theme park worldwide, Tivoli in Denmark yesterday.",section:"Posts",handler:()=>{window.location.href="/blog/2022/tivoli-firework/"}},{id:"post-royal-guardian",title:"Royal Guardian",description:"The shift of the royal guardian in Marmorkirken.",section:"Posts",handler:()=>{window.location.href="/blog/2022/royal-gardian/"}},{id:"post-swan-nearby",title:"Swan Nearby",description:"A record to keep this swan in my memory.",section:"Posts",handler:()=>{window.location.href="/blog/2022/swan-nearby/"}},{id:"post-chef-first-time",title:"Chef First Time",description:"This is my first time being the chef to cook for my danish family!",section:"Posts",handler:()=>{window.location.href="/blog/2022/chief-first-time/"}},{id:"post-travel-to-malm\xf6",title:"Travel to Malm\xf6",description:"This is the final version of the image preprocessing with the limitation that cannot automatically crop the image in the center. However, it can work properly in normal situations. Also, this blog is used to record my trip to Malm\xf6, Sweden with Xuanlang.",section:"Posts",handler:()=>{window.location.href="/blog/2022/travel-to-malmo/"}},{id:"post-dev-progress",title:"Dev Progress",description:"In this version, the development of my blog website is already passed half. \u8fd9\u91cc\u662f\u4e2d\u6587\u5b57\u4f53\u6d4b\u8bd5\u3002",section:"Posts",handler:()=>{window.location.href="/blog/2022/dev-progress/"}},{id:"post-take-metro",title:"Take Metro",description:"This blog is to record my route from DIKU back to my dorm at 00:53 am several days ago.",section:"Posts",handler:()=>{window.location.href="/blog/2022/take-metro/"}},{id:"post-start-my-blog",title:"Start My Blog",description:"Memorize the start of the implementation of my blog-web.",section:"Posts",handler:()=>{window.location.href="/blog/2022/start-my-blog/"}},{id:"news-i-have-defended-my-master-thesis-quot-exploration-of-self-supervised-learning-methods-for-longitudinal-image-analysis-quot-and-received-my-master-39-s-degree-meanwhile-i-have-been-offered-a-job-as-an-ai-engineer-in-a-company",title:"I have defended my master thesis **&quot;Exploration of Self-Supervised Learning Methods for Longitudinal Image Analysis&quot;** and received my Master&#39;s Degree. Meanwhile, I have been offered a job as an **AI Engineer** in a company. \ud83c\udf93\ud83c\udf89\ud83d\ude80",description:"",section:"News"},{id:"news-going-to-be-one-of-tas-for-the-course-advanced-deep-learning-in-b4-2024-at-university-of-copenhagen",title:"\ud83e\udd70 Going to be one of TAs for the course **Advanced Deep Learning** in B4-2024 at University of Copenhagen.",description:"",section:"News"},{id:"news-congrats-to-me-for-passing-the-courses-medical-image-analysis-advanced-topics-in-image-analysis-and-project-of-research-in-self-supervised-learning-methods-for-longitudinal-images-with-10-out-of-12",title:"\ud83e\udd73 Congrats to me for passing the courses **Medical Image Analysis**, **Advanced Topics in Image Analysis**, and **Project of Research in Self-Supervised Learning Methods for Longitudinal Images** with **10 out of 12**.",description:"",section:"News"},{id:"news-i-am-happy-to-share-i-got-12-out-of-12-in-the-courses-of-advanced-algorithms-and-data-structures-signal-and-image-processing-and-advanced-deep-learning",title:"\ud83c\udf7b I am happy to share I got **12 out of 12** in the courses of **Advanced Algorithms and Data Structures**, **Signal and Image Processing**, and **Advanced Deep Learning**.",description:"",section:"News"},{id:"news-learning-notes-advanced-algorithms-and-data-structures-https-liuying-1-github-io-blog-tag-aads-signal-and-image-processing-https-liuying-1-github-io-blog-tag-sip-and-advanced-computer-systems-https-liuying-1-github-io-blog-tag-acs-have-been-transferred-from-my-self-developed-website-to-this-one",title:"\ud83d\udcd4 Learning notes [**Advanced Algorithms and Data Structures**](https://liuying-1.github.io/blog/tag/aads), [**Signal and Image Processing**](https://liuying-1.github.io/blog/tag/sip), and [**Advanced Computer Systems**](https://liuying-1.github.io/blog/tag/acs) have been transferred from my self-developed website to this one.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%79%35%38%31%30%39%39@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/liu-ying-463ab925b","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/100085288350390","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>