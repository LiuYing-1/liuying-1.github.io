<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://liuying-1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://liuying-1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T06:29:37+00:00</updated><id>https://liuying-1.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Line Segment Intersection</title><link href="https://liuying-1.github.io/blog/2024/line-segment-intersection/" rel="alternate" type="text/html" title="Line Segment Intersection"/><published>2024-08-20T08:00:00+00:00</published><updated>2024-08-20T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/line-segment-intersection</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/line-segment-intersection/"><![CDATA[<h4 id="线段求交-专题图叠合">线段求交-专题图叠合</h4> <p>This post is the collection of the extracts from the book Computational Geometry \(3^{rd}\) - Berg et. al., and it is collected in Chinese so that I can recall it quickly when I need it later on.</p> <p><strong>Why</strong> 地图不能帮助游客知道确切想要的信息。即使知道某个小镇的大致方位，也很难在地图上确定他的位置。</p> <p><strong>So</strong> 为了增加可读性，地图由很多个不同类型的 thematic map 堆叠组成。比如某一层存放公路，某一层存放河流。</p> <p>每层 <strong>map 数据类型可能都不一样</strong>。比如道路是一组线段或曲线，而城市那层可能是一组点。</p> <p>为了发挥地图的最大效用，比如用户可以先通过查看城市 map 从而知道大致方向，然后通过<strong>叠合</strong>只关注道路等 map 来获取到这个城市的路线，从而不被别的 map 所干扰。</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/G8g8lCu.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. 加拿大西部的城市、河流、铁道线，以及叠合后的效果 </div> <p>在叠合这个过程中，不同 map 相交的位置就显得很有用。比如气候学家可以通过叠合 松林覆盖 map 和降雨量 map 筛选出符合这两个条件的子区域（可以理解为交集）。</p> <h5 id="线段求交">线段求交</h5> <p>我们提取出其中的数学表达形式。</p> <p><strong>给定由线段组成的两个集合，计算出其中来自一个集合的所有线段与来自于另一个集合线段之间的交点。</strong></p> <p>可是怎么才算<strong>两条线段相交</strong>呢？回归初始问题，任何一个专题图中的单元，比如道路图中的道路，河流图中的河流，都可以表示为一条（由依次相联的）<strong>线段（组成的）链</strong>，因此，<strong>“道路与河流的交汇点”即为一条链的内部和另一条链的内部的交点</strong>。此外，交点也有可能发生在链中线段的任何一个端点处，因此，<strong>某条线段的端点正好落在另一条 线段上也是一个交点</strong>。</p> <p><strong>为了进一步简化，将两组线段的集合合到一起变成一个集合，然后考虑其中所有线段之间的交点。此外，尤为关注于来自同一个集合的各线段之间的交点。</strong></p> <p>于是，问题就演变成了：</p> <p>给定由平面上 \(n\) 条闭线段构成的一个集合 \(S\)，报告出 \(S\)​ 中各线段之间的所有交点。</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/rgc3BGf.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. 最坏情况下，所有线段都两两相交。 </div> <p>\(O\) 代表 \(\leq\) 表示在最坏情况下，算法的运行时间至多为，而 \(\Omega\) 代表 \(\geq\) 表示在最坏情况下，算法的运行时间至少为。</p> <p>对于一般情况，可以依次检查每一对线段，看看是否相交：如果的确相交，就将其交点报告出来。可是将会需要 \(O(n^2)\)​ 时间。结合后续提到的扫描线算法。针对如图 2 的这种情况，不管什么算法，都至少需要 \(\Omega(n^2)\)​ 时间。</p> <p><strong>在实际应用中，大多数线段要么根本不与其他线段相交，要么只与少数的线段相交。因此交点的数量远达不到平方量级。</strong></p> <p>有没有其他更快的算法呢？比如，<strong>“输出敏感”</strong>的算法（<strong>Output-sensitive algorithm</strong>）：其运行时间不仅与输入中线段的数目有关，还与实际的交点数目有关。这种算法的运行时间对实际输出的大小很敏感。这个算法也可以成为是<strong>“交点敏感的”</strong>（<strong>Intersection-sensitive</strong>），因为输出的大小就是由<strong>交点的数目</strong>决定的。</p> <p><strong>在找出所有交点的过程中，如何避免对所有的线段进行测试呢？</strong></p> <p><u>几何特性：只有相互靠近的线段才可能会相交；而相距甚远的线段则不可能相交。</u></p> <p>利用这一特性，可以得出一个解决<strong>线段求交问题（line segment intersection problem）</strong>的输出敏感的算法。</p> <p>:star2::star2::star2: 设定，给定一个集合 \(S:=\{s_1, s_2, ..., s_n\}\)。避免对相距很远的线段进行求交。</p> <p>首先排除一个简单的情况，如下图。</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Zx3HEL8.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. 通过投影，排除不可能相交的线段对 </div> <p>将一条线段在 \(y\) 轴上进行正交投影，定义它的 \(y\) 区间 （\(y-interval\)）。任何两条线段，只要其 \(y-interval\) 没有重叠部分，它们在 \(y\) 轴方向上相距很远，换句话说，它们一定不会相交。\(\implies\) 只要对那些 \(y-interval\) 上有重合的那些线段进行比对即可。</p> <p>为了找出这样的线段对，用一条直线 \(l\) ，<strong>从一个高于所有线段的位置，从上而下的扫过整个平面</strong>。在这条假想的直线扫过平面的过程中，跟踪记录所有与之相交的线段，以找出所需的所有线段对。</p> <p>这类算法被称为<strong>平面扫描算法（plane sweep algorithm）</strong>，其中使用到的直线 \(l\)​ 被称为<strong>扫描线（sweep line）</strong>。与<strong>当前扫描线相交的所有线段的集合</strong>是这条<strong>扫描线的状态（status）</strong>。随着扫描线的向下推进，它的状态不断变化，不过并不是连续的。只有在某些特定位置，才需要对扫描线的状态进行更新。我们称这些位置为平面扫描算法的时间点（event point）。在这个算法中，这里的事件点（event point）就是各线段的端点，如下图。</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/4JGt9R6.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. 阐述 </div>]]></content><author><name></name></author><category term="study"/><category term="c-ai"/><category term="cg"/><summary type="html"><![CDATA[Self-learning note for CG - Line Segment Intersection.]]></summary></entry><entry><title type="html">Graduation</title><link href="https://liuying-1.github.io/blog/2024/graduation/" rel="alternate" type="text/html" title="Graduation"/><published>2024-06-28T10:15:00+00:00</published><updated>2024-06-28T10:15:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/graduation</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/graduation/"><![CDATA[<p>I am thrilled to announce that I have successfully defended my master thesis and received my Master’s Degree in Computer Science from the Department of Computer Science (DIKU), University of Copenhagen.</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/akAyC97.jpeg" alt="graduation-photo" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/7Ee4Raf.jpeg" alt="graduation-photo-2" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <p>The title of my thesis is “Exploration of Self-Supervised Learning Methods for Longitudinal Image Analysis”, specifically focusing on the application of self-supervised learning methods, which are BYOL and SimSIAM, in medical image analysis for longitudinal studies (i.e., tumor progression) on lung cancer. The 3D ResNet model is first pre-trained on the LIDC-IDRI dataset, and then the model is tasked with predicting the tumor volume at the future timepoint of the same subject, on the Longitudinal 4D Lung dataset. The results show that the self-supervised learning methods cannot be used to train 3D ResNet to learn meaningful representations for longitudinal image analysis, and the learned representations are not significantly correlated with the actual tumor volume, verified by the linear probing task. This failure is attributed to the following reasons: 1. the volume featured by the LIDC-IDRI dataset (nodules) is with substatial differences from the Longitudinal 4D Lung dataset (tumors), 2. the current pre-processing pipeline does not isolate the lung region perfectly, 3. the model is not fine-tuned on the longitudinal downstream task, and 4. the invovled augmentations impair the model’s ability to learn precise tumor volume representations.</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/16tSUKF.png" alt="2831691531627_.pic" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <p>I am grateful for the support and guidance from my supervisor, friends, and family, along the way. Thank you to everyone who has been a part of this journey with me. 🎓🎉</p> <p>Additionally, I am very excited to share the news that I am going to work as an AI engineer in a company, to embark on the next chapter of my life and look forward to new opportunities and challenges. 🚀</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="fun"/><summary type="html"><![CDATA[Congratulations to my graduation from the DIKU, University of Copenhagen.]]></summary></entry><entry><title type="html">Review of SwAV</title><link href="https://liuying-1.github.io/blog/2024/swav/" rel="alternate" type="text/html" title="Review of SwAV"/><published>2024-02-12T10:31:00+00:00</published><updated>2024-02-12T10:31:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/swav</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/swav/"><![CDATA[]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the learning of the paper SwAV.]]></summary></entry><entry><title type="html">Review of SimSiam</title><link href="https://liuying-1.github.io/blog/2024/simsiam/" rel="alternate" type="text/html" title="Review of SimSiam"/><published>2024-02-12T09:31:00+00:00</published><updated>2024-02-12T09:31:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/simsiam</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/simsiam/"><![CDATA[]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the learning of the paper SimSiam.]]></summary></entry><entry><title type="html">Extracts from BYOL</title><link href="https://liuying-1.github.io/blog/2024/byol/" rel="alternate" type="text/html" title="Extracts from BYOL"/><published>2024-02-11T08:00:00+00:00</published><updated>2024-02-11T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/byol</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/byol/"><![CDATA[<p>BYOL is the abbr. of <u>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</u>.</p> <h3 id="cookbook-for-byol">Cookbook for BYOL</h3> <p>First of all, BYOL method probably the first one removes the clustering step, introduces a predictor and projector network, fefines the continuous targets as the output of a momentum network, renormalize each sample representation by its \(l_2\)-norm and leverage positive pairs. The predictor acts as a whitening operator preventing collapse, and momentum network can be applied only to the projector.</p> <p>It first introduced self-distillation as a means to avoid collapse. It usees two networks along with a predictor to map the outputs of one network to the other. The network predicting the output is called the online or student network while the network producing the target is called the target or teacher network. Each network receives a different view of the same image formed by image transformations including random resizing, cropping, color jittering, and brightness alterations. The student network is updated throughout training using gradient descent. The teacher network is updated with an exponential moving average (EMA) updates of the weights of the online network. The slow updates induced by exponential moving average creates an asymmetry that is crucial to BYOL’s success.</p> <h3 id="original-paper">Original Paper</h3> <p>Please check the original paper here <a href="https://arxiv.org/abs/2006.07733">BYOL</a>.</p> <h4 id="abstract">Abstract</h4> <ul> <li><strong>BYOL</strong>, a <strong>new</strong> apporach to <strong>self-supervised learning</strong>.</li> <li>BYOL relies on two neural networks, referred to as the <strong>online</strong> network and the <strong>target</strong> network, that <strong>interact</strong> and <strong>learn from each other</strong>.</li> <li>From <strong>an augmented view of an image</strong>, we train the <strong>online network to predict the target network representation of the same image under a different augmented view</strong>.</li> <li>At the same time, we <strong>update the target network with a slow-moving average of the online network</strong>.</li> <li>(Back then), while state-of-the-art methods <strong>rely on negative pairs</strong>, BYOL achieves a new state of the art <strong>without them</strong>.</li> </ul> <hr/> <p>Online first updates, updates of target come from online by EMA.</p> <p>Without negative pairs can be competative as well.</p> <p>Train the online to predict the target network representation.</p> <hr/> <h4 id="introduction">Introduction</h4> <ul> <li>(Back then,) state-of-the-art <strong>contrastive methods</strong> are trained by <strong>reducing the distance between representations of different augmented views of the same image (‘positive pairs’)</strong>, and <strong>increasing the distance between representations of augmented views from different images (‘negative pairs’)</strong>.</li> <li>These methods need <strong>careful treatment of negative pairs</strong> by either <strong>relying on large batch sizes</strong>, <strong>memory banks</strong> or <strong>customized mining strategies</strong> to retrieve the negative pairs.</li> <li>In addition, their performance <strong>critically</strong> depends on the <strong>choice of image augmentations</strong>.</li> </ul> <hr/> <p>BYOL can overcome the previous shortages from the previous methods, which are: <strong>no need negative pairs</strong>, <strong>careful treatment of negative pairs</strong>, <strong>choice of hyper-parameters</strong>, and <strong>choice of augmentations</strong>.</p> <hr/> <ul> <li>BYOL achieves higher performance than state-of-the-art contrastive methods <strong>without using negative pairs</strong>.</li> <li>It <strong>iteratively</strong> <strong>bootstraps the outputs of a network to serve as targets for an enhanced representation</strong>.</li> <li>BYOL is <strong>more robust</strong> to the <strong>choice of image augmentations</strong> than contrastive methods; we suspect that <strong>not relying on negative pairs</strong> is one of the leading reasons for its <strong>improved robustness</strong>.</li> <li>We propose to <strong>directly bootstrap the representations</strong>. ==&gt; Different from previous methods.</li> <li>In particular, <strong>BYOL uses two neural networks, referred to as online and target networks, that interatct and learn from each other.</strong></li> <li>Starting from <strong>an augmented view of an image</strong>, BYOL <strong>trains its online network</strong> to <strong>predict the target network’s representation</strong> of <strong>another augmented view</strong> of the <strong>same image</strong>.</li> <li>While this objective <strong>admits collapsed solutions</strong>, e.g., <strong>outputting the same vector for all images</strong>, we empirically show that <strong>BYOL does not converage to such solutions</strong>.</li> <li>We hypothesize that the <strong>combination</strong> of (i) the <strong>addition of a predictor to the online network</strong> and (ii) the <strong>use of a slow-moving average of the online parameters as the target network</strong> encourages <strong>encoding more and more information within the online projection</strong> and <strong>avoids collapsed solutions</strong>.</li> </ul> <hr/> <ul> <li>BYOL achieves state-of-the-art results under the <strong>linear evaluation protocol on ImageNet without using negative pairs</strong>.</li> <li>Our learned representation outperforms the state of the art on semi-supervised and transfer benchmarks.</li> <li>BYOL is <strong>more resilient</strong> to <strong>changes in the batch size</strong> and <strong>in the set of image augmentations</strong> compared to its contrastive counterparts.</li> <li>In particular, BYOL sufferes a much smaller performance drop than SimCLR, a strong contrastive basline, when only using random crops as image augmentations.</li> </ul> <h4 id="related-work">Related work</h4> <ul> <li>Most <strong>unsupervised methods for representation learning</strong> can be categorized as either <strong>generative</strong> or <strong>discriminative</strong>.</li> <li><strong>Generative approaches to representation learning</strong> build <strong>a distribution over data and latent embedding</strong> and <strong>use the learned embeddings as image representations</strong>.</li> <li>Many of these apporaches <strong>rely either on auto-encoding of images</strong> or on <strong>adversarial learning</strong>, jointly modelling data and representation.</li> <li>Generative methods typically operate <strong>directly in pixel space</strong>.</li> <li>This however is <strong>computationally expensive</strong>, and the <strong>high level of detail required for image generation</strong> may <strong>not be necessary</strong> for representation learning.</li> </ul> <hr/> <p>The <strong>intro</strong> and <strong>drawbacks</strong> of generative methods.</p> <hr/> <ul> <li>Among <strong>discriminative</strong> methods, <strong>contrastive methods</strong> currently achieves state-of-the-art performance in self-supervised learning.</li> <li><strong>Contrastive methods often</strong> require comparing each example with many other examples to work well, <strong>prompting the question of whether using negative pairs is necessary</strong>.</li> </ul> <hr/> <p><strong>Contrastive methods</strong> belongs to <strong>discriminative methods</strong>, and are best, while <strong>prompting the question of negative pairs</strong>.</p> <hr/> <ul> <li><strong>DeepCluster partially answers this question</strong>.</li> <li>While <strong>avoiding the use of negative pairs</strong>, this requires <strong>a costly clustering phase and specific precautions to avoid collapsing to trivial solutions</strong>.</li> </ul> <hr/> <p>Might <strong>not necessary</strong> for the use of <strong>negative pairs</strong> if under suitable settings.</p> <hr/> <ul> <li>Some self-supervised methods are <strong>not contrastive but rely on using auxiliary handcrafted prediction tasks</strong> to learn their representation.</li> <li>Yet, even with suitable architectures, these methods are being <strong>outperformed by contrastive methods</strong>.</li> </ul> <hr/> <p>Some other methods might <strong>better</strong> than contrastive methods.</p> <hr/> <ul> <li>Our approach <strong>has some similarities</strong> with <em>Predictions of Bootstrapped Latents</em> (PBL).</li> <li>PBL <strong>jointly trains the agent’s history representation</strong> and <strong>an encoding of future observations</strong>.</li> <li><strong>Unlike PBL</strong>, <strong>BYOL uses a slow-moving average of its representation to provide its targets, and does not require a second network.</strong></li> </ul> <hr/> <p>BYOL stems from PBL but unlike PBL.</p> <hr/> <ul> <li>The <strong>idea of using a slow-moving average target network to produce stable targets</strong> for the online network was inspired by deep RL.</li> <li> <p>Target networks stabilize the bootstrapping updates provided by the Bellman equation, <strong>making them appealing to stabilise the bootstrap mechanism in BYOL</strong>.</p> </li> <li>While <strong>most RL methods use fixed target networks</strong>, BYOL uses <strong>a weighted moving average of previous networks in order to provide smoother changes in the target representation</strong>.</li> </ul> <hr/> <p>The origin of the moving weights.</p> <hr/> <ul> <li>In the semi-supervised …. Among these methods, mean teacher also uses a slow-moving average network, called teacher, to produce targets for an online network, called student.</li> <li><strong>In contrast, BYOL introduces an additional predictor on top of the online network, which prevents collapse.</strong></li> </ul> <hr/> <p>The use of predictor.</p> <hr/> <ul> <li>Finally, in self-supervised learning, MoCo uses a slow-moving average network (momentum encoder) to maintain consistent representations of negative pairs drawn from a memory bank.</li> <li>Instead, <strong>BYOL uses a moving average network to produce prediction targets</strong> as a means of stabilizing the bootstrap step.</li> </ul> <h4 id="method">Method</h4> <ul> <li>Many such approaches <strong>cast the prediction problem directly in representation space</strong>: the <strong>representation of an augmented view of an image</strong> should be <strong>predictive of the representaion of another augmented view of the same image</strong>.</li> <li>However, <strong>predicting directly in represntation space</strong> can lead to <strong>collapsed representations</strong>.</li> <li>Contrastive methods circumvent this problem by reformulating the prediction problem into one of discrimination. … and the <strong>representations of augmented views of different images</strong>.</li> <li>In this work, we thus <strong>tasked ourselves to find out whether these negative examples are indispensable to prevent collapsing while preserving high performance.</strong></li> </ul> <hr/> <p>cross-view 能：图像的增强视图的表征能预测同一图的另一个增强的表征，但是代价是总能预测自己来导致 trivial solutions（坍塌）。所以我们的方法来验证能否保持高性能的同时，负样本的存在是否必须。</p> <hr/> <ul> <li>To <strong>prevent collapse, a straightforward solution is to use a fixed randomly initialized network</strong> to poduce the <strong>targets</strong> for our predictions.</li> <li>While avoidng collapse, it <strong>empircally does not result in very good representations</strong>.</li> <li>It is interesting to note that the <strong>representation obtained using this procedure</strong> can already be <strong>much better than the initial fixed represntation</strong>.</li> <li>This experimental finding is the <strong>core motivation</strong> for BYOL: <strong>from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation.</strong></li> <li>We can expect to <strong>build a sequence of representations of increasing quality by iterating this procedure</strong>, <strong>using subsequent online networks as new target networks for further training.</strong></li> <li>In practice, BYOL <strong>generalizes this bootstrapping procedure</strong> by <strong>iteratively refining its representation</strong>, but <strong>using a slowly moving exponential average of the online network</strong> as the target network <strong>instead of fixed checkpoints</strong>.</li> </ul> <hr/> <p>经验觉得 fixed 不行，但是已经效果不错了，所以我们基于此设计了新的。从一个给定的表征（target）训练一个新的加强的表征（online）来预测 target 的表征。</p> <hr/> <h5 id="description-of-byol">Description of BYOL</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/7aUmfsC.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. BYOL's architectures </div> <ul> <li> <p>BYOL’s <strong>goal</strong> is to <strong>learn a representation</strong> \(y_\theta\) <strong>which can then be used for downstream tasks</strong>. ==&gt; <strong>representation</strong>.</p> </li> <li> <p>BYOL uses <strong>two neural networks</strong> to learn: the <strong>online</strong> and <strong>target networks</strong>.</p> </li> <li> <p>The <strong>online network is defined by a set of weights</strong> \(\theta\) and is comprised of <strong>three stages</strong>: an <em>encoder</em> \(f_\theta\), a projector \(g_\theta\) and a predictor \(q_\theta\)​.</p> </li> <li> <p>The <strong>target network has the same architecture as the online network</strong>, but <strong>uses a different set of weights</strong> \(\xi\).</p> </li> <li> <p>The <strong>target</strong> network provides the <strong>regression targets to train the online network</strong>, and its parameters \(\xi\) are an <strong>exponential moving average</strong> of the <strong>online</strong> parameters \(\theta\).</p> </li> <li> <p>Given a <strong>target decay rate</strong> \(\tau\in [0, 1]\)​, after each training step we perform the following update,</p> \[\xi \leftarrow \tau\xi + (1-\tau)\theta.\] </li> </ul> <hr/> <p>Online: encoder \(encoder(f_\theta) + prejector(g_\theta) + predictor(q_\theta)\), target has the same architecture but with different parameters comes from the above mentioned equation based on online.</p> <hr/> <ul> <li> <p>Given <strong>a set of images</strong> \(D\), an <strong>image</strong> \(x\sim D\) sampled uniformly from \(D\), and <strong>two distributions of image augmentations</strong> \(\mathcal{T}\) and \(\mathcal{T'}\), BYOL <strong>produces two augmented views</strong> \(v \triangleq t(x)\) and \(v'\triangleq t'(x)\) from \(x\) by <strong>applying respectively image augmentations</strong> \(t \sim \mathcal{T}\) and \(t' \sim \mathcal{T'}\). ==&gt; 从两个相同流程的增强策略上随机选两套，因为参数大概率不同，所以两套流程的参数不同，所以是两个不同的 view。</p> </li> <li> <p>From the <strong>first augmented view</strong> \(v\), the <strong>online network outputs</strong> a <em>representation</em> \(y_\theta \triangleq f_\theta(v)\) and a <strong>projection</strong> \(z_\theta\triangleq g_\theta(y)\).</p> </li> <li> <p>The <strong>target</strong> network outputs \(y'_\xi \triangleq f_\xi(v')\) and the <em>target projection</em> \(z'_\xi \triangleq g_\xi(y')\) from the <strong>second augmented view</strong> \(v'\)​.</p> </li> <li> <p>We then output a <strong>prediction</strong> \(q_\theta(z_\theta)\) of \(z'_\xi\) and \(l_2\)-normalize both \(q_\theta(z_\theta)\) and \(z'_\xi\)​ to</p> \[\overline{q_\theta}(z_\theta) \triangleq q_\theta(z_\theta)/\vert\vert q_\theta(z_\theta)\vert\vert_2\] <p>and</p> \[\overline{z'_\xi}/\vert\vert z'_\xi\vert\vert_2^2.\] </li> <li> <p>Note that this <strong>predictor is only applied to the online branch</strong>, making the <strong>architecture asymmetric between the online and target pipeline</strong>.</p> </li> <li> <p>Finally we dine the following mean squared error between the normalized predictions and target projections,</p> \[\mathcal{L}_{\theta, \xi} \triangleq \vert\vert{\overline{q_\theta}(z_\theta) - \overline{z'}_\xi}\vert\vert_2^2 = 2 - 2\cdot \frac{&lt;q_\theta(z_\theta), z'_\xi&gt;}{\vert\vert q_\theta(z_\theta)\vert\vert_2 \cdot \vert\vert z'_\xi\vert\vert_2}.\] </li> <li> <p>We <strong>symmetrize the loss above</strong> by separaetely feeding \(v'\) to the online network and \(v\) to the target network to compute the other one. ==&gt; 对称的换一下两个网络的 view，然后将两个相加形成最终的 loss。</p> </li> <li> <p>At <strong>each training step</strong>, we perform a stochastic optimization step to minimize</p> \[\mathcal{L}_{\theta, \xi}^{\text{BYOL}} = \mathcal{L}_{\theta, \xi} + \widetilde{\mathcal{L}}_{\theta, \xi}\] <p>with respect to \(\theta\) only, but not \(\xi\).</p> \[\theta \leftarrow \text{optimizer}(\theta, \nabla_\theta\mathcal{L}_{\theta, \xi}^{\text{BYOL}}, \eta),\\ \xi \leftarrow \tau\xi + (1-\tau)\theta\] <p>where optimizer is an optimzer and \(\eta\)​ is a learning rate.</p> </li> <li> <p>At the end of training, we <strong>only keep the encoder</strong> $f_{\theta}$.</p> </li> </ul> <h5 id="implementation-details">Implementation details</h5> <ul> <li>Image augmentations: <ul> <li>BYOL uses the same set of image augmentations as in SimCLR.</li> <li>First a <strong>random patch of the image</strong> is selected and <strong>resized</strong> to \(224\times 224\)​ with a random <strong>horizontal</strong> flip, followed by a <strong>color distrotion</strong>, consisting of a <strong>random sequence of brightness, contrast, saturation, hue adjustments, and an optional grayscale conversion</strong>. Finally, <strong>Gaussian blur</strong> and <strong>solarization</strong> are applied to the <strong>patches</strong>.</li> </ul> </li> <li><strong>Architecture</strong> <ul> <li><strong>ResNet</strong>-50 as our base parametric <strong>encoders</strong> \(f_\theta\) and \(f_\xi\)</li> <li>The <strong>representation</strong> $y$ <strong>corresopnds to the output of the final average pooling layer</strong>, which has a feature dimension of \(2048\).</li> <li>Contrary to SimCLR, the <strong>output</strong> of this MLP is <strong>not batch normalized</strong>. The <strong>predictor</strong> \(q_\theta\) uses the <strong>same</strong> <strong>architecture</strong> as \(g_\theta\)​.</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li><strong>LARS</strong> <strong>optimizer</strong> with a <strong>cosine decay learning rate schedule</strong>, <strong>without restarts</strong>, over 1000 epochs, with a <strong>warm-up period of 10 epochs</strong>.</li> <li>the <strong>base learning rate</strong> to \(0.2\), scaled linearly with the batch size (Learning rate = \(0.2 \times \text{BatchSize}/256\))</li> <li>A global weight decay parameter of \(1.5\cdot 10^{-6}\) while excluding the biases and batch normalization parameters from both LARS adaptation and weight decay.</li> <li>A <strong>batch size of 4096</strong></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[Review of the paper BYOL and to extract the main points]]></summary></entry><entry><title type="html">Cookbook Reading - 1</title><link href="https://liuying-1.github.io/blog/2024/cookbook-1/" rel="alternate" type="text/html" title="Cookbook Reading - 1"/><published>2024-01-30T08:00:00+00:00</published><updated>2024-01-30T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/cookbook-1</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/cookbook-1/"><![CDATA[<p>All the following text are stemmed from the source book, please refer to the original copy - <em><a href="https://arxiv.org/pdf/2304.12210.pdf">A Cookbook of Self-Supervised Learning</a></em>. Additionally, there will be a large amount of Chinese translations on it since it will be used for me to recall something, sorry in advance.</p> <h4 id="what-is-self-supervised-learning-and-why-bother">What is Self-Supervised Learning and Why Bother?</h4> <p>自监督学习，也被称为 the dark matter of intelligence。<u>它是一个提升机器学习的康庄大道。</u>与监督学习不一样，监督学习受制于有标签数据的数量，而<u>自监督学习方法能够从大量没有标签的数据里面进行学习</u>。自监督学习是深度学习在自然语言处理领域取得成功的基础，它推动了从自动机器翻译到在无标记文本的网络规模语料库上训练的大型语言模型的发展。在计算机视觉里面，SSL通过在10亿张图像上训练的SEER等模型，将<u>数据规模推向了新的极限</u>。一些针对计算机数据的SSL方法，即使在一些竞争相对激烈的Benchmarks上（e.g., ImageNet）<u>已经能够和一些在有数据标签的训练的监督学习上得到的模型相媲美甚至更好</u>。SSL也已经被成功运用在<u>各个模态数据</u>上，比如视频，音频，和一些时间序列。</p> <p>自监督学习根据位标签的输入<u>定义了一个辅助任务来产出descriptive and intelligible的特征</u>。在自然语言中，一个常见的SSL的目标就是去遮住（mask）一个文本中的单词，然后预测周围的单词。这种预测一个词周围的上下文的目标鼓励了模型不依赖任何标签也能去捕捉文本中词汇中的关系。同样的经过SSL模型学到的表征也能被用作一系列下游任务上，比如不同语言间的文本翻译，总结，甚至产生文本，当然也还有一些别的。在CV中，<strong>MAE</strong>或<strong>BYOL</strong>这些方法也存在相似的目标，通过学习来预测一个图像被遮住的patch或者其表征。也有别的SSL目标，通过添加颜色或者裁剪图片来形成同一个图片的两个不同视角，然后鼓励模型能学到映射到相似表征的能力。</p> <p>有了能力来在大量的未标签数据上进行训练，带来了很多好处。传统的监督学习方法通常是根据现有的有标签数据对已知的特定任务进行训练（先验的），而 <u>SSL 则是学习对许多任务都有用的通用表征</u>。<u>SSL 在医学等领域尤其有用</u>，因为在这些领域中，标签的成本很高，或者无法事先知道具体的任务。还有证据表明，SSL 模型可以学习到对对抗性示例（<u>adversarial examples</u>）、标签损坏（<u>label corruption</u>）和输入扰动（<u>input perturbations</u>）更稳健（<u>robust</u>）的表征，而且与有监督的模型相比更公平。因此，SSL是一个越来越受关注的领域。然而，和做饭很像，训练SSL方法是一个门槛很高的微妙艺术。</p> <h5 id="why-a-cookbook-for-self-supervised-learning">Why a Cookbook for Self-Supervised Learning?</h5> <p>虽然 SSL 的许多组成部分都为研究人员所熟悉，但要成功训练 SSL 方法，却需要从<u>借口任务到训练超参数</u>等一系列令人眼花缭乱的选择。SSL研究有一个高的门槛，一是因为<u>计算成本</u>，二是<u>没有完全透明的论文</u>详细讲述其中能够充分发挥SSL的潜能的复杂的实现方法，三是<u>缺乏SSL统一的词汇和理论观点</u>。因为SSL从传统的reconstruction-based无监督学习方法（e.g.，denoising variational autoencoders）建立了一套独特的范式，我们在统一视角下理解 SSL 的词汇是有限的。事实上，在一个单一视角下统一SSL方法在2021也仅仅才开始出现。如果没有一个共同的基础来描述 SSL 方法的不同组成部分，研究人员在开始研究 SSL 方法时就会面临更大的挑战。同时，SSL研究也急需新的研究人员，因为SSL现在已经遍布我们的生活了。然而，关于 SSL 的<u>通用性保证、公平性属性以及对对抗性攻击</u>甚至<u>自然发生的变异</u>的鲁棒性，仍有许多未解决的研究问题。这类问题对于SSL方法的依赖程度也很关键。</p> <p>此外，SSL 是由<u>经验驱动</u>的，它有许多活动部件（主要是<u>超参数</u>），这些部件可能会影响最终表征的关键属性，而且在已发表的作品中<u>不一定有详细说明</u>。即，为了开始研究SSL方法，我们必须<u>首先对这些方法进行详尽的实证研究</u>，才能充分掌握<u>所有这些活动部件的影响和行为</u>。这种经验盲点需要大量的计算资源和已有的实践经验，因此具有很强的局限性。总之，SOTA 的性能来自看似不同但又相互重叠的方法，现有的理论研究很少，而实际应用却很广泛，因此需要一本统一技术及其“配方”的指南，这对降低 SSL 的研究门槛至关重要。</p> <p>我们的目标是为了降低从事SSL研究的门槛，通过铺设基础和一些相对较新的SSL方法，以一种cookbook的风格来呈现。为了“烹饪”成功，你必须首先学习基本技术：切菜，炒菜等。在第 2 节中，我们首先<u>使用通用词汇介绍自监督学习的基本技术</u>。特别是，我们将从统一的视角来<u>描述这些方法系列以及将它们的目标联系起来的理论主线</u>。我们在概念框中突出了损失项或训练目标等关键概念。然后，一个厨师必须学会运用技术去做成一道美味的菜，这也就需要学习现有的菜谱，组合食材，还有品味菜品。在第三节中，我们<u>介绍了实际成功实现SSL方法中，需要考虑的点。我们讨论了通常的训练菜谱，包括超参数的选择，如何组合不同组件（比如结构和优化器），还有如何评价SSL方法。我们还将分享顶尖研究人员就常见培训配置和陷阱提出的实用建议</u>。我们希望这本手册能成为成功训练和探索自监督学习的实用基础。</p> <h4 id="the-families-and-origins-of-ssl">The families and origins of SSL</h4> <p>由于<u>大规模的特别大的数据集和高内存的GPU的可获得性</u>，自2020年以来，SSL方法就开始复兴了。但是SSL的起源可以<u>追溯到深度学习年代的最开始</u>。</p> <h5 id="origins-of-ssl">Origins of SSL</h5> <p>当今的方法都建立于我们从<u>早期实验</u>中获得到的知识。在这一章节中，我们给了一个在2020年以前关于SSL点子的简述。虽然许多具体方法已经不再被主流使用，因为它们不再能在基准问题上提供最先进的性能，我们也不会对它们进行详细讨论，但这些论文中的观点构成了许多现代方法的基础。比如，restoring missing or distorted parts of an input or contrasting two views of the same image的核心目标形成了现代SSL方法的基础。SSL <u>早期的进展主要集中在以下几类</u>（有时相互重叠）方法的开发上：</p> <ol> <li> <p><strong>Information restoration</strong>: 大量的方法被开发来<u>遮住或者移除一个图片中的一些东西</u>，然后<u>训练一个神经网络来恢复缺失部分的信息</u>。基于上色的SSL方法（<u>Colorization-based</u>）SSL methods把一个图片<u>转换成灰度图</u>，然后训练一个神经网络来<u>预测原本的RGB值</u>。由于着色需要了解对象的语义和边界，因此<u>着色被证明是早期的对象分割 SSL 方法</u>。这种信息恢复最直观简单的应用是遮住即<u>挪去图像的一部分，然后训练一个神经网络来给缺失的像素着色</u>。这个点子进化成了Masked auto-encoding methods，其中，<u>掩蔽区域是图像patches的结合体（union），可以使用transfomer进行预测</u>。</p> </li> <li> <p><strong>Using temporal relationships in video</strong>: 尽管这个回顾的焦点是在图像（而非视频）处理上，一系列专门方法已经被开发出来，通过对<u>视频进行预训练来学习单图像表征（single-image representations）</u>。需要注意的是，<u>信息还原（information restoration）方法对视频特别有用</u>，因为视频包含多种可能被屏蔽的信息模式。Wang and Gupta 使用<u>三重损失（triplet loss）对模型进行预训练</u>，以<u>提高两个不同帧中同一个物体表征的相似性</u>。最终的结果模型在物体检测上表现不错。Pathak et al. 训练了一个模型来<u>预测物体在单一帧中的运动</u>，调整由此产生的features来解决单一帧（single-frame）检测问题。Agrawal et al. <u>预测多帧图像中摄像机的自我运动</u>（ego-motion）轨迹。Owens et al. 提出<u>从视频中去掉音轨，然后预测缺失的声音</u>。对于深度映射（depth mapping）等专业应用，有人提出了自监督方法，从<u>未标明的图像对中学习单目深度</u>（monocular depth）模型，然后<u>从单摄像头视频（single-camera video）中学习帧</u>。此类方法仍是一个活跃的研究领域。</p> </li> <li> <p><strong>Learning spatial context</strong>: 这个种类的方法是<u>训练一个模型来理解物体在一个场景的相对位置和方向</u>。RotNet 通过随机旋转来<u>掩盖重力方向，然后要求模型预测旋转</u>。Doersch et al. 是最早的 SSL 方法之一，它可以<u>简单地预测图像中两个随机采样的patch的相对位置</u>。这个方法后来<u>被Jigsaw方法取代</u>了。Jigsaw方法就是把<u>一个图片分成一列disjoint的patches然后预测相互的相对位置</u>。一个不同的空间任务是学习数数：模型是在自监督方式被训练来<u>输出一个图像中物体的数量</u>。</p> </li> <li> <p><strong>Grouping similar images together</strong>: 通过将<u>语义相似的图像分组</u>，可以学习到<u>丰富的特征</u>。K-means聚类就是在传统的机器学习中最广泛运用的方法之一。许多研究都对 k-means 进行了调整，以便利用神经模型做 SSL。<u>深度聚类技术通过在特征空间（feature space）中执行 k-means 来交替为图像分配标签</u>，并<u>更新模型以respect这些分配的类别标签</u>。这个方法最近的处理方式是使用<u>mean-shift更新来把features推到他们的聚类中心</u>，而且已经被证明可以用来<u>补充BYOL</u>方法。BYOL是一种基于两个网络的方法，其目标是预测<u>每个样本的伪标签</u>。对深度聚类的别的提升包括在特征空间用optimal transport methods来创建更有信息的聚类。</p> </li> <li> <p><strong>Generative models</strong>: 一种早期的有影响力的SSL方法是贪婪层级（greedy layer-wise）预训练，其中，深度网络的各层使用autoencoder loss进行逐层训练。当时的一种类似方法是使用受限玻尔兹曼机 (RBMs)，这种机器可以进行分层训练和堆叠，以创建深度信念网。虽然这些方法已被放弃，转而采用更简单的初始化策略和更长的训练时间，但它们在历史上对 SSL 的使用产生了深远影响，因为它们促成了第一批 “深度 “网络的训练。后来在auto-encoders的表征学习的能力上的提升有包括denoising autoencoders，cross-channel prediction，和deep canoncically correlated autoencoders。但是最终发现，当要求auto-encoder恢复其输入的缺失部分时，表征转移性会更好，这就形成了 SSL 方法中的 “信息恢复 “类别。</p> <p>生成式对抗网络由一个图像生成器和一个判别器组成，判别器区分真图像和生成的假图像。这对模型的两个组成部分都可以在没有监督的情况下进行训练，而且两者都可能包含对迁移学习有用的知识。早期的GANs的论文使用 GAN 组件进行下游图像分类实验。此外，还开发了专门的特征学习程序来修改判别器、添加生成器或学习从图像到潜空间的额外映射，以改进迁移学习。</p> </li> <li> <p><strong>Multi-view invariance</strong>: 许多现代 SSL 方法，尤其是我们在本文中重点讨论的方法，都使用对比学习（contrastive learning）来创建<u>不受简单变换影响的特征表征</u>。对比学习的理念是鼓励模型<u>以相似的方式表示输入的两个增强版本</u>。在对比学习被广泛采用之前，有许多方法以各种方式强制保持不变性，从而引领了这一方向。</p> <p>从无标签数据中学习的最流行框架之一，是使用<u>弱训练网络为图像添加伪标签</u>（pseudolabels），然后以标准<u>监督方式使用这些标签进行训练</u>。这种方法后来得到了改进，<u>加强了对变换的不变性</u>（invariance to transformations）。虚拟对抗训练（virtual adversarial training）<u>利用图像的伪标签对网络进行训练</u>，此外还进行<u>对抗训练</u>，使学习到的特征几乎不受输入图像微小扰动（small perturbations）的影响。后来的工作重点是<u>保持数据增强（augmentation）变换的不变性</u>（maintain invariance to data augmentation transforms）。这类早期的重要方法包括 MixMatch，该方法通过<u>平均</u>（average）网络在<u>多个不同随机增强</u>（augmentation）<u>训练图像上的输出</u>来选择<u>伪标签</u>，从而得到<u>增强不变的标签</u>。大约在同一时间，人们发现，通过<u>训练网络使不同视角下的图像表征之间的互信息最大化</u>，可以实现<u>良好的 SSL 性能</u>。这些以增强为基础（augmentation-based）的方法在上述旧方法和本文重点讨论的现代方法之间架起了一座桥梁。</p> </li> </ol> <p>有了这些渊源，我们现在将 SSL 分成四大系列：深度度量学习（Deep Metric Learning）系列、自馏分（Self-Distillation）系列、典型相关分析（Canonical Correlation Analysis）系列和屏蔽图像建模（Masked Image Modeling）系列。</p> <p>在接下来的文章中，将会以原著的方式呈现摘录来形成笔记。</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the first part of this book.]]></summary></entry><entry><title type="html">Advanced Image Registration</title><link href="https://liuying-1.github.io/blog/2023/advanced-image-registration/" rel="alternate" type="text/html" title="Advanced Image Registration"/><published>2023-10-10T08:00:00+00:00</published><updated>2023-10-10T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/advanced-image-registration</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/advanced-image-registration/"><![CDATA[<h4 id="todays-learning-objectives">Todays Learning Objectives</h4> <ul> <li> <p>What does <strong><u>regularization</u></strong> do in the context of <strong><u>registration</u></strong>?</p> <p>In the context of image registration, regularization refers to a technique used to <strong>improve</strong> the <strong>accuracy</strong> and <strong>robustness</strong> of the registration process. Here’s <strong>what regularization does</strong>:</p> <ul> <li><strong>Control Deformation</strong>: Image registration aims to <strong><u>align two images by finding a transformation that maps one image onto the other</u></strong>. <strong><u>Regularization</u></strong> helps control the <strong><u>degree of deformation or warping allowed during this alignment</u></strong>. It <strong>prevents</strong> overly <strong><u>complex and unrealistic deformations</u></strong> that may lead to poor results.</li> <li><strong>Penalize Irregularities</strong>: Regularization introduces <strong><u>a penalty term</u></strong> in the registration objective function. This penalty term <strong><u>discourages irregular, non-smooth deformations</u></strong>. By doing so, it <strong><u>promotes smooth and plausible transformations</u></strong> between the images.</li> <li><strong>Balancing Trade-Off</strong>: Regularization <strong><u>strikes a balance</u></strong> between <strong><u>fitting the images together accurately</u></strong> and ensuring that the <strong><u>deformation remains physically plausible</u></strong>. Without regularization, registration can result in <strong><u>overly complex or noisy transformations</u></strong> that may not align images effectively.</li> <li><strong>Controlling Sensitivity</strong>: It also helps control the <strong><u>sensitivity</u></strong> of the registration process <strong><u>to noise or outliers</u></strong> in the data. Regularization can make the registration process <strong><u>more stable and less prone to small variations</u></strong> in the images.</li> </ul> <p>In summary, regularization in image registration ensures that the <strong><u>alignment process is not overly complex</u></strong> and <strong><u>produces physically meaningful transformations</u></strong> that are both accurate and stable.</p> </li> <li> <p><u>Name</u> and <u>explain</u> at least two <u>clinical applications of registration</u></p> <ol> <li><strong>Image-Guided Surgery</strong>: Image registration is widely used in surgery. It involves <strong><u>aligning preoperative medical images</u></strong> (such as CT scans or MRI scans) with the <strong><u>patient's actual anatomy</u></strong> during surgery. This allows surgeons to <strong><u>visualize the patient's internal structures</u></strong>, <strong><u>plan surgical procedures more accurately</u></strong>, and navigate in real-time during surgery. Image-guided surgery improves <strong><u>precision</u></strong> and <u>**reduces the invasiveness of procedures**</u>.</li> <li><strong>Radiation Therapy</strong>: In radiation therapy for <strong><u>cancer treatment</u></strong>, image registration is crucial. It involves <strong><u>registering a patient's diagnostic images with images taken during the treatment</u></strong> (such as cone-beam CT scans). This ensures that <strong><u>radiation beams precisely target the tumor while sparing healthy tissues</u></strong>. Accurate registration is essential to <strong><u>maximize treatment effectiveness</u></strong> and <strong><u>minimize side effects</u></strong>.</li> </ol> </li> <li> <p>Handle <u>open data sets</u></p> <p>Handling open datasets involves <strong><u>working with publicly available datasets</u></strong> for research or educational purposes. Here’s how you can handle open datasets:</p> <ol> <li><strong>Data Access</strong>: <strong><u>Find open datasets relevant to your research or learning objectives</u></strong>. These datasets are often available through websites, data repositories, or research organizations. Ensure that you have the necessary permissions to access and use the data.</li> <li><strong>Data Download</strong>: <strong><u>Download the datasets</u></strong> following the provided guidelines and terms of use. Some datasets may require registration or citation, so make sure to adhere to any specific requirements.</li> <li><strong>Data Preprocessing</strong>: Depending on the dataset, you may need to <strong><u>preprocess the data, clean it, and format it for your specific analysis or research goals</u></strong>. This step might involve data cleaning, normalization, and transformation.</li> <li><strong>Data Analysis</strong>: <strong><u>Perform the desired data analysis</u></strong>, which could include image registration, image analysis, or other tasks related to your research or learning objectives. Ensure you follow good data analysis practices and documentation.</li> <li><strong>Ethical Considerations</strong>: <strong><u>Always handle open datasets with ethical considerations</u></strong>. Respect privacy, follow legal regulati</li> </ol> </li> </ul> <h4 id="regularization">Regularization</h4> <p><strong>Terms</strong></p> <p><strong>Fixed Image</strong>:</p> <ul> <li>The “fixed” image is the <strong>reference image</strong> or the image that <strong>serves as the target</strong> for the registration process.</li> <li>It’s the image <strong>you want the “moving” image to be aligned</strong> with or transformed to match.</li> <li>In medical image analysis, the fixed image is often an anatomical image, such as a CT scan or an MRI scan, which provides the reference structure for alignment.</li> </ul> <p><strong>Moving Image</strong>:</p> <ul> <li>The “moving” image is the image that <strong>you want to align with the fixed image</strong>.</li> <li>It’s the image that <strong>undergoes a transformation</strong> to bring it into spatial alignment with the fixed image.</li> <li>In medical image analysis, the moving image could be another image of the same patient acquired at a different time or with a different modality.</li> </ul> <p>The goal of image registration is to <strong>find a transformation</strong> that can <strong>map or align</strong> the <strong>moving image to match the fixed image</strong> as closely as possible. This process is commonly used in fields like medical image analysis to compare images taken at different times, with different modalities, or for other clinical or research purposes. The fixed and moving images help <strong>establish a spatial relationship between different images, enabling comparisons, analyses, and clinical decision-making</strong>.</p> <p><strong>Deformation</strong> refers to the process of <strong>changing the <u>shape</u>, <u>size</u>, or <u>spatial arrangement</u> of an object or a structure</strong>. In the context of image registration, deformation typically involves <strong><u>altering the spatial configuration of an image or a part of an image</u></strong> to bring it into <strong><u>alignment with another image</u></strong>. Deformation can be applied to individual pixels or voxels within an image to achieve this alignment. It’s a fundamental concept in image registration because <strong><u>it describes how one image is modified to match the other</u></strong>.</p> <p><strong>Transformation</strong>, on the other hand, is a broader term that <strong><u>encompasses any mathematical operation</u></strong> applied to an image to change its position, orientation, scale, or shape. A transformation can be rigid (only involving translation and rotation), affine (involving translation, rotation, scaling, and shearing), or non-rigid (involving deformation). Transformation is a <strong><u>more general term</u></strong> that includes deformation as a specific case.</p> <p><strong>Deformed Moving</strong> in the context of image registration means that the <strong>“moving” image, which was originally not aligned with the “fixed” image</strong>, has undergone a deformation or transformation to achieve spatial alignment with the fixed image. This term indicates that <strong>the moving image has been modified or adjusted to fit or match the spatial characteristics of the fixed image</strong>. Deforming the moving image is a key step in the registration process, where the transformation is applied to bring the two images into spatial concordance.</p> <p>Here we have an example,</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/0MY6HKQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 1. Fixed (reference) and moving (ongoing) images </div> <p>Here is the <strong>deformed moving</strong> image.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/QQqviCR.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 2. Deformed moving image </div> <p><strong><em>Is this a good transformation field?</em></strong></p> <p><strong>Definitely not</strong>. There are a lot of <strong>strange internal distortions</strong>. The transformation field might not be ideal or that the registration process did not achieve the desired result.</p> <p><strong><em>Solution</em></strong></p> <p><strong>Regularization</strong></p> <ul> <li>We make the algorithm <strong>pay the price</strong> for <strong>too much deformations</strong></li> <li><strong>Price</strong> should <strong>grow nonlinearly</strong> (better <strong>move many a little bit</strong> than <strong>one a lot</strong>)</li> </ul> <p>This phrase is explaining the <strong>rationale behind the nonlinear growth of the cost</strong>. It suggests that it’s <strong>more desirable to apply small deformations to multiple regions</strong> of an image rather than <strong>applying a very significant deformation to just one region</strong>. This is often because <strong>small deformations are more likely to preserve the overall structure and quality</strong> of the image.</p> <p>To put it simply, in image registration with regularization, the <strong>cost associated with deformation should encourage small, gradual changes across the entire image</strong> rather than allowing or favoring large, abrupt changes in just one area. The idea is to <strong>ensure that the transformation or deformation is smooth and physically meaningful</strong>, which can lead to <strong>better registration results</strong> and <strong>more realistic alignments</strong>. This approach helps <strong>prevent unrealistic distortions or artifacts</strong> in the transformed image.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/VQqGWET.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 3. Definition of points </div> <p>The idea behind is that <strong>we would like to preserve distances between</strong>:</p> <p>· Point $i, j$</p> <p>· Point from $\Omega_{i, j}$</p> <p>And we would like to <strong><u>preserve distances between points</u></strong> in original and deformed grid</p> <p>The goal of “<strong>preserving distances between points</strong>” is to ensure that, <strong>after the deformation</strong>, the <strong><u>distances or spatial relationships between points on the deformed grid remain as close as possible to the distances between the corresponding points on the original grid</u></strong>. In other words:</p> <ul> <li>If <strong><u>point A is originally a certain distance away from point B on the original grid</u></strong>, we want to ensure that <strong><u>after deformation</u></strong>, <strong><u>point A remains a similar distance away from point B on the deformed grid</u></strong>.</li> <li>This preservation of distances helps <strong><u>maintain the local structure and spatial relationships in the image</u></strong>. It ensures that important <strong><u>anatomical or structural features</u></strong> in the image <strong><u>do not get distorted or altered</u></strong> significantly during the registration process.</li> </ul> <p>Overall, this concept is fundamental in regularization during image registration. It contributes to the smoothness and physical plausibility of the deformation, preventing unrealistic warping and ensuring that the registered image accurately reflects the spatial characteristics of the original image.</p> <p><strong>Problem formulation</strong></p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/P3XM5Gr.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 4. Formulation of problem </div> <p><strong>Original optimization formulation:</strong></p> \[F(I, J, \theta) = \min\sum_{i, j}d\left(I(i, j) - J(x(i,j,\theta), y(i, j, \theta))\right)\] <p><strong>Formulation with regularization:</strong></p> \[F(I, J, \theta) = \min\left(\sum_{i, j}d(I(i, j) - J(x(i, j, \theta), y(i, j, \theta))\right) \\ + \lambda\sum_{i,j}\sum_{k, l\in\Omega_{i, j}}\left(((i-k)\ - (x(i, j, \theta) - x(k, l, \theta))\right)^2\] <p>We add a regularization term to <strong>give a penalty</strong>. And what will this regularization do?</p> \[\sum_{i, j} \sum_{k.l\in\Omega_{i, j}}(\left(1-sign\left((i-k)\cdot(x(i, j, \theta) - x(k, l, \theta))\right)\right) + \left(1-sign\left((j-l)\cdot(y(i, j, \theta)-y(k, l, \theta))\right)\right))\] <p>This regularization term is used to <strong><u>encourage the preservation of distances or relationships</u></strong> between neighboring points in the image grid during the registration process. It <strong><u>penalizes situations</u></strong> where the <strong><u>distances between points change significantly</u></strong> as a result of the deformation caused by the transformation parameter $\theta$.</p> <p>The term 1−sign(…) essentially <strong><u>imposes a penalty</u></strong> when the transformation causes <strong><u>significant changes in the spatial relationships between points</u></strong>. This encourages the registration process to <strong><u>produce a deformation that maintains the local structure and spatial characteristics of the image</u></strong>.</p> <p>The “folding” you mentioned likely refers to the effect of this regularization term in <strong><u>preventing excessive deformations that could distort or fold the image</u></strong>. The regularization term <strong><u>discourages such deformations</u></strong>, promoting smoother and more physically plausible transformations.</p> <p>In summary, this regularization term helps <strong><u>maintain the spatial relationships between neighboring points in the image grid</u></strong>, preventing excessive distortion during image registration.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/LbWQENU.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 5. Regularization of this term </div> <p>And here, by using regularization, we get a better deformed result.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/tiWJyvY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 6. Regularized result </div> <h4 id="landmarks">Landmarks</h4> <p>Can we help registration of two squares?</p> <ul> <li> <p>Automatically detect square corners</p> <p>This step involves the automatic identification of corners or key points on the square objects in the images. These corners are distinctive features that can be used as landmarks for registration.</p> </li> <li> <p>Force registration to align corners</p> <ul> <li>This means using the detected square corners as constraints during the registration process. The registration algorithm will be instructed to ensure that the corners of the squares align precisely or as closely as possible.</li> <li>By “forcing” the registration to align corners, you are making it a priority to match these specific, distinctive points. This can be especially useful when you have prior knowledge of where these corners should be located in the registered image.</li> </ul> </li> </ul> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/fb1LYL8.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 7. Landmarks </div> <p>Then we have new optimization function.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UzjYqzQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 8. New optimization function </div> <p>The use of landmarks is a common technique in image registration, and it’s particularly valuable when you have objects with distinctive and easily detectable features, such as square corners. By aligning these landmarks, you can improve the registration accuracy and ensure that specific points of interest are correctly positioned in the registered image.</p> <p><strong>How to ensure the landmarks match?</strong> Validate the results by checking how closely the landmarks in the registered images match. You can calculate and evaluate the distances between corresponding landmarks to assess the quality of registration.</p> <h4 id="summary-algorithm">Summary Algorithm</h4> <ul> <li><strong>Define algorithm settings</strong> <ul> <li>Acceptable transformations (Rigid, Affine, Non-rigid)</li> <li>Similarity measure (Mean Squares, Correlation, Mutual Information)</li> <li>Multi-resolution pyramid, grid schedule</li> <li>Regularization</li> </ul> </li> <li> <p><strong>Prepare input images</strong></p> <ul> <li>Normalization of intensities</li> <li>Rescaling</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li>Compute similarity measure over <strong>grids on both images</strong></li> <li>Compute <strong>gradients on grid</strong> on moving image</li> <li>Move <strong>grid according to the gradients</strong></li> <li><strong>Deform</strong> moving image according to the <strong>grid</strong></li> <li>Use splines to <strong>interpolate</strong> pixels outside the grid</li> </ul> </li> <li>Repeat until convergence</li> </ul> <h4 id="applications">Applications</h4> <h5 id="atlas-based-segmentation">Atlas-based segmentation</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/haDNA69.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 9. Atlas-based segmentation </div> <ul> <li> <p>Advantages:</p> <ul> <li> <p>Anatomically correct segmentation</p> <p>这意味着使用基于参考图谱的分割方法可以获得解剖学上正确的分割结果。具体来说，该方法参考了已知的解剖结构信息，因此分割结果更有可能准确地匹配到具体的解剖结构，如器官、组织或区域。这有助于提高分割的准确性，尤其是在医学图像分析中，这对于诊断和研究非常重要。</p> </li> <li> <p>Light training image requirements</p> <p>与某些其他分割方法相比，基于参考图谱的分割通常需要较少的训练图像。这意味着你不必拥有大量的标记数据来训练分割模型。相反，你可以仅使用一个或几个高质量的参考图谱，然后将其应用于其他图像，从而降低了训练数据的需求和数据标记的工作量。</p> </li> <li> <p>Invariant to the number of target objects</p> <p>这意味着这种方法的性能不受需要分割的目标对象数量的影响。你可以使用相同的参考图谱和算法来分割不同数量的目标对象，而不需要根据目标数量进行调整或重新训练。这种不变性对于处理不同图像或场景中的多个目标对象非常有用，因为它减轻了定制和调整的工作。</p> </li> </ul> </li> <li> <p>Disadvantages:</p> <ul> <li> <p>Slow</p> <p><strong>计算复杂性</strong>：基于参考图谱的分割方法通常涉及计算图像之间的配准或对准。这包括计算变换场（deformation field）以将参考图谱与目标图像对齐。这些计算在像素级别进行，特别是在非刚性（non-rigid）变换的情况下，需要大量计算。这会导致算法的复杂性和计算时间的增加。</p> <p><strong>多分辨率金字塔</strong>：虽然多分辨率金字塔可以提高配准的鲁棒性，但也会增加计算的复杂性。多分辨率金字塔需要在不同分辨率级别上执行配准，每个级别都需要计算变换场。这意味着多分辨率金字塔方法可能需要更多的计算时间。</p> <p><strong>正则化</strong>：正则化用于平滑变换场以确保物理上合理的形变，但它也会引入额外的计算。正则化通常需要迭代优化过程，这会增加计算时间。</p> <p><strong>硬件限制</strong>：图像分割通常需要大量计算资源，特别是在高分辨率或大规模数据集的情况下。如果计算资源受限，速度可能会受到限制。</p> </li> </ul> </li> </ul> <p>See details about <a href="https://liuying-1.github.io/blog/2023/image-registration-l1/">Atlas-based Segmentation</a> :point_left:</p> <h5 id="multi-modal-image-registration">Multi-modal image registration</h5> <p>Improves <strong>visibility of structures</strong> (bones – CT, soft tissues – MR, tumors – PET and MR)</p> <p>“Multi-modal image registration” 指的是在医学图像分析中，同时注册或配准不同模态（多模态）的医学图像。<strong><u>每种模态</u></strong>的医学图像（如CT扫描、MRI扫描、PET扫描等）<strong><u>对人体结构和病变有不同的灰度值、对比度和信息</u></strong>。”Improves visibility of structures” 的意思是<strong><u>多模态图像配准可以改善不同模态下的结构可视性</u></strong>。</p> <p>具体来说，这些不同模态的图像可能有以下特点：</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/SBq4Pxv.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 10. Soft-tissues </div> <ol> <li> <p><strong>CT图像</strong>：在CT扫描中，<strong><u>骨骼结构</u></strong>通常显示得很清晰，因为它们有高对比度。然而，对软组织的可视性可能较差。</p> </li> <li> <p><strong>MR图像</strong>：MRI图像对于显示<strong><u>软组织结构</u></strong>非常出色，但对于骨骼结构的可视性可能不如CT。</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/HKTrqHQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 11. Bones </div> </li> <li> <p><strong>PET图像</strong>：PET扫描用于检测代谢活跃性，通常用于<strong><u>肿瘤诊断</u></strong>。PET图像可以显示<strong><u>肿瘤</u></strong>和其他异常的生物活动，但对解剖结构的细节可视性相对较低。</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/VM8awXw.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 12. Tumors </div> </li> </ol> <p>“Multi-modal image registration” 允许将这些不同模态的图像进行配准，以便在同一坐标系下对比它们。这样做有以下好处：</p> <ul> <li><strong>改善可视性</strong>：通过将不同模态的图像进行配准，你可以在一个图像中同时显示不同模态下的结构。例如，你可以在同一图像中清晰地看到骨骼（来自CT图像）、软组织（来自MR图像）和肿瘤活性（来自PET图像），从而获得更全面的信息。</li> <li><strong>指导诊断和治疗</strong>：多模态配准可帮助医生更准确地诊断病症、规划手术和监测治疗进展。医生可以在一个图像上同时查看多个信息来源，有助于更全面地了解患者的情况。</li> </ul> <p>总之，多模态图像配准有助于提高医学图像的可视性，增加信息的融合，从而更好地支持医疗诊断和治疗过程。</p> <p>多模态图像配准是将来自不同图像模态的图像进行对齐，以使它们在同一坐标系下对应到相同的解剖结构或区域。这种配准的目的是允许医生或研究人员同时查看不同模态下的图像信息，从而更全面地理解患者的情况。以下是多模态图像配准的一般流程：</p> <ol> <li><strong>图像获取</strong>：首先，不同模态的图像（例如CT、MRI、PET等）必须分别获取。这些图像通常在不同的设备上获得，具有不同的对比度和信息。</li> <li><strong>图像预处理</strong>：在进行配准之前，通常需要对图像进行预处理，包括去噪、增强、图像校正等操作，以确保它们处于最佳状态。</li> <li><strong>特征提取</strong>：从每种模态的图像中<strong><u>提取特征</u></strong>，这些特征可以是解剖结构、标记点或其他在不同模态下容易识别的图像特征。特征提取是关键的一步，因为<strong><u>它确定了配准过程中用于对齐的关键点</u></strong>。</li> <li><strong>特征匹配</strong>：在不同模态图像中提取的特征进行匹配。这可以通过各种计算机视觉算法实现，通常是在配准算法中的一个步骤。特征匹配的目标是找到相应的特征点，以便进行配准。</li> <li><strong>配准变换</strong>：<strong><u>基于找到的特征匹配，计算需要应用于其中一个图像以使其与另一个图像对齐的变换</u></strong>。这可以是刚性、仿射或非刚性变换，具体取决于图像的类型和所需的对齐程度。</li> <li><strong>配准优化</strong>：进行迭代优化，以确保变换能够最好地对齐图像。这包括优化变换参数以最小化特征点之间的距离或匹配误差。</li> <li><strong>生成配准后的图像</strong>：应用配准变换，将不同模态的图像对齐到同一坐标系下，从而生成配准后的图像。这些图像可以同时查看，以获得更多信息。</li> </ol> <p>需要注意的是，多模态图像配准是一项复杂的任务，其复杂性取决于图像的特性和所使用的配准算法。它通常需要计算机视觉和图像处理领域的专业知识，以确保高质量的配准结果。这种配准方法在医学影像学、医学诊断和研究中具有广泛的应用。</p> <h5 id="atlas-based-abnormality-detection">Atlas-based abnormality detection</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/wFbSCgB.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 13. Abnormality detection </div> <ul> <li>Register <strong><u>healthy</u></strong> image to <strong><u>potentially abnormal</u></strong></li> <li>Study deformation field to <strong><u>find abnormalities</u></strong></li> </ul> <p>“Atlas-based abnormality detection” 是一种用于检测医学图像中异常或病变的方法，它基于已知的正常解剖结构的图谱（atlas）来识别图像中的异常情况。</p> <ol> <li><strong>图谱（Atlas）</strong>：图谱是一组已知正常解剖结构的参考图像。这些图像通常来自大量的<strong><u>健康患者</u></strong>，用于表示<strong><u>正常的生理结构</u></strong>和器官。图谱可以包括各种模态的图像，如CT、MRI等。</li> <li><strong>异常检测</strong>：在医学图像中，异常通常是指与正常解剖结构不同的区域，可能是疾病、损伤或其他异常情况的迹象。异常可以表现为异常的形状、密度、亮度或其他特征。</li> <li><strong>配准与对比</strong>：在进行异常检测时，首先将<strong><u>待检测图像（可能是患者的图像）与正常图谱</u></strong>进行配准。这意味着将待检测图像中的结构与图谱中的对应结构对齐。</li> <li><strong>特征提取</strong>：一旦完成配准，就可以从待检测图像中提取特征，这些特征通常是与异常相关的特征，如区域的形状、密度、纹理等。</li> <li><strong>异常检测算法</strong>：使用特征和已知的正常图谱作为参考，异常检测算法可以<strong><u>识别待检测图像中与正常图谱不匹配</u></strong>的区域。这些不匹配的区域可能表示异常或病变。</li> <li><strong>可视化和报告</strong>：检测到的异常区域可以用于生成可视化结果，供医生或研究人员查看。异常检测的结果通常以可视化图像或报告的形式呈现。</li> </ol> <p>“Atlas-based abnormality detection” 的优势在于它依赖于已知的正常图谱作为参考，因此可以帮助<strong><u>快速识别潜在的异常区域</u></strong>。这对于医学诊断、疾病筛查和研究非常有用，因为它可以帮助医生或研究人员更快速地识别异常，而不需要手动检查整个图像。</p> <p>在 “Atlas-based abnormality detection” 中，通常采取的方法包括：</p> <ol> <li><strong>将健康图像配准到潜在异常图像</strong>：首先，将一个或多个健康图像（正常图谱）与潜在异常图像进行配准，以使它们在相同的坐标系下对齐。这意味着<strong><u>将健康图像中的正常结构与潜在异常图像对应的结构对齐</u></strong>。</li> <li><strong>研究变换场以发现异常</strong>：<strong><u>一旦进行了配准，就可以研究变换场</u></strong>（deformation field）。变换场表示<strong><u>从健康图像到潜在异常图像的变换</u></strong>，它描述了<strong><u>正常结构在潜在异常图像中的形变</u></strong>。通过<strong><u>分析变换场</u></strong>，可以<strong><u>识别出在潜在异常图像中与正常结构不匹配</u></strong>的区域。</li> </ol> <p>这个方法的思想是，通过将健康图像与潜在异常图像对齐，可以将正常结构的位置和形状信息传递到潜在异常图像中。然后，通过比较正常结构的期望位置和实际位置（通过变换场表示），可以检测到异常或病变区域，因为这些区域可能会产生异常的形变。</p> <p>这种方法在医学图像中用于异常检测，特别是在需要自动识别潜在异常或病变的情况下，具有重要的应用潜力。</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/itVoHD4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 14. Small abnormalities </div> <ul> <li>Problem <ul> <li>Difficult to detect small abnormalities</li> </ul> </li> </ul> <p>“Difficult to detect small abnormalities” 意味着在某些情况下，使用基于图谱的方法来检测小尺寸异常或病变可能面临挑战。这可能是因为以下原因：</p> <ol> <li><strong>空间分辨率限制</strong>：医学图像的空间分辨率是有限的，特别是对于某些成本昂贵的成像技术。小尺寸的异常可能在图像中占据的像素数量很少，因此可能难以在低分辨率图像中准确检测。</li> <li><strong>噪声和伪影</strong>：医学图像可能受到噪声和伪影的影响，这些因素可以干扰小异常的检测。噪声可以导致虚假的异常检测，而伪影可能使异常区域难以分辨。</li> <li><strong>特征提取的挑战</strong>：检测小异常通常需要更高级别的特征提取和分析，以便捕捉细微的变化。这可能需要更复杂的算法和更多的计算资源。</li> </ol> <p>解决这个问题的方法可能包括：</p> <ul> <li><strong>提高图像分辨率</strong>：使用更高分辨率的成像技术可以帮助更准确地检测小尺寸的异常。</li> <li><strong>降低噪声</strong>：采取噪声抑制技术，以减少噪声对异常检测的干扰。</li> <li><strong>使用多模态信息</strong>：结合不同模态的图像信息，以提高对小异常的检测能力。</li> <li><strong>使用深度学习方法</strong>：深度学习技术在医学图像分析中表现出色，可以帮助检测小异常，因为它们可以自动学习图像中的特征。</li> </ul> <p>综合来说，检测小异常是医学图像分析中的一个挑战，但可以通过改进图像质量、采用高级分析方法以及使用多种信息源来应对这一挑战。</p> <h5 id="super-resolution-images">Super-resolution images</h5> <p>We have three orthogonal 3D MR images of high in-plane resolution but high slice thickness. How to create a super-resolution 3D MR?</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/AfM6SVk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 15. Super-resolution </div> <p>创建超分辨率 3D MR 图像通常涉及将现有的低分辨率图像增强到高分辨率图像的过程。在这种情况下，你拥有三个不同方向（x、y、z）的 3D MR 图像，这些图像具有高平面分辨率，但在切片（z轴方向）上的分辨率较低。下面是一种可能的方法来创建超分辨率 3D MR 图像：</p> <ol> <li><strong>数据预处理</strong>：首先，对你的三个 3D MR 图像进行数据预处理。这包括去噪、伪影去除、校正以及确保它们已经在相同的坐标系下对齐。</li> <li><strong>切片插值</strong>：在 3D MR 图像的切片方向上，由于分辨率较低，你可以使用插值方法来增加切片数量，从而提高 z 轴分辨率。这可以采用一维插值方法，例如线性插值或样条插值。这将生成一系列新的切片，以增加在 z 轴方向上的分辨率。</li> <li><strong>3D 重建</strong>：一旦你获得了高分辨率切片，可以将它们组合成 3D 数据卷。这可以通过采用体绘图（Volume Rendering）或通过堆叠这些切片来实现。这将生成一个具有更高分辨率的 3D MR 图像。</li> <li><strong>超分辨率技术</strong>：如果你希望进一步增加图像的分辨率，可以使用超分辨率技术。这些技术基于统计方法、深度学习或其他算法，可以通过分析图像的细节来生成更高分辨率的版本。这需要具体的超分辨率算法，并可能需要使用额外的训练数据。</li> <li><strong>评估和验证</strong>：最后，一定要评估和验证生成的超分辨率图像的质量。这可以通过与高分辨率图像进行比较，或者使用图像质量指标来进行评估。</li> </ol> <p>需要注意的是，超分辨率图像的质量取决于数据的质量、使用的插值和重建方法，以及是否使用了超分辨率算法。这是一个复杂的过程，通常需要在医学影像领域具有专业知识的人员来执行。</p> <ul> <li>We convert orthogonal images to full volume but adding empty space</li> <li>This empty space should not be taken into account during registration</li> <li>Idea of masked registration</li> </ul> <p>在创建超分辨率 3D MR 图像时，将正交图像转换为完整的体积并添加空白空间是一个常见的方法。这种空白空间不应该在图像配准过程中考虑在内，因为它仅是为了在 z 轴方向上增加分辨率而添加的。</p> <p>以下是关于如何执行这一过程的一般步骤：</p> <ol> <li><strong>创建完整的 3D MR 体积</strong>：将正交图像（高分辨率但切片分辨率较低）转换为完整的 3D MR 体积。这可以通过在切片方向上重复图像来实现，以填充空白空间。这将生成一个完整的 3D 数据卷，包括原始图像和添加的空白切片。</li> <li><strong>生成掩模（Mask）</strong>：为了确保在图像配准期间不考虑空白切片，你可以生成一个掩模，其中空白切片被标记为不感兴趣区域。掩模通常是一个与图像体积具有相同尺寸的 3D 数据，其中正值表示感兴趣区域，负值或零表示不感兴趣区域（即空白切片）。</li> <li><strong>掩模配准</strong>：在进行图像配准时，你可以使用掩模来指定感兴趣区域。这意味着只有非空白部分的图像将被用于配准，而空白部分将被忽略。这有助于提高配准的准确性，因为配准算法将集中在包含有用信息的部分。</li> <li><strong>生成高分辨率 3D 图像</strong>：一旦完成配准，可以生成具有更高 z 轴分辨率的 3D MR 图像。这可以通过将正交图像的 z 轴方向上的高分辨率信息与掩模应用于已配准的体积来实现。</li> <li><strong>评估和验证</strong>：最后，一定要评估和验证生成的超分辨率 3D 图像的质量，以确保它满足你的需求。</li> </ol> <p>这种方法允许你在 z 轴方向上增加分辨率，同时保留了 x 和 y 轴方向上的高分辨率信息。使用掩模来指定感兴趣区域是一个有用的技巧，以确保只有有用的信息用于图像配准和超分辨率生成。</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/GoKFaDW.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 16. Super-resolution - 2 </div> <p>I didn’t get the points here, and there are some other applications afterwards, but I am not going to explore them in detail, for instance, <strong>Adaptive radiotherapy planning</strong>, <strong>Landmark-based registration</strong>, and <strong>2D-3D registration</strong>.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Self-learning note for advanced image registration.]]></summary></entry><entry><title type="html">Image Registration Basics</title><link href="https://liuying-1.github.io/blog/2023/image-registration-l1/" rel="alternate" type="text/html" title="Image Registration Basics"/><published>2023-10-06T08:00:00+00:00</published><updated>2023-10-06T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/image-registration-l1</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/image-registration-l1/"><![CDATA[<p><strong><em>Disclaimer: All notes below are refered to the course “Medical Image Analysis” delivered by UCPH.</em></strong></p> <p>So today’s lecture will be on <strong><u>Image Registration</u></strong>.</p> <p>We have already talked about the topic of registration in our previous lecture. I said that there is some problems of registration. And its idea is to move from one image to another. It;s very popular in our field but I initially said we’re not talk about applications of registration so that you can think about this yourself, and also we can wait till the registration lecture like <u>which is having now</u>.</p> <h4 id="todays-learning-objectives">Todays Learning Objectives</h4> <ul> <li>Why do we need registration?</li> <li>Name and explain at least two similarity measures</li> <li>Describe the difference between rigid, affine and non-rigid registration</li> </ul> <h4 id="what-is-registration">What is Registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/i4jYCLQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Example of registration </div> <ul> <li><u>Image processing method</u> for <u>aligning two or more images with each other spatially</u> (geometrically)</li> <li>You can <u>make images from different imaging modalities, times, sections, angles</u>, etc, <u>comparable</u>.</li> </ul> <h4 id="why-do-we-need-registration">Why do we need registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/jO6tT6z.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Why we need registration </div> <p><strong><em>Registration geometrically transforms one image into another.</em></strong></p> <p>So what we have is we have an image which is in the most top left. And we have a target which is in the most right bottom.</p> <p>And we slowly transform our image step by step towards the target.</p> <p>So you can see that this patient has some degenerative disease and changing in the brain, so the ventricles are growing, and you can see they slowly step by step grow when we use the image of the patient. Grow and grow until they reach this level.</p> <p>But the question is, how does that happen? What mathematical process lies behind allows us to nicely deform one image towards another one?</p> <p><strong><u>Before we go into mathematical concepts, let's look at one of the main applications of image registrations.</u></strong></p> <h5 id="atlas-based-segmentation">Atlas-based segmentation</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/BJ0T49b.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. Atlas registration </div> <p>It’s <strong>atlas-based segmentation</strong>.</p> <p>Before deep learning, that was specification was the main like every time when people use registration almost in 80% or something. They used it for atlas-based registration.</p> <p>So what is the idea of atlas-based registration? What we do is we have a collection of images <u>maunally segmented</u>. And we get a new image, we <strong><u>register every image from our collection towards our image</u></strong>. During this registration, we get <strong>transformation matrix</strong>.</p> <p>So basically we get some map which says which pixels went where during the registration, how we changed during the process. And the way you can use this transformation metrics or we can apply it to the segmentation from our atlas.</p> <p>我的理解是我们原本有一系列标准的人工分割好的图片，将每一张图片都以我们的新图像作为 Target 进行配准，在配准过程中，我们获得了变换矩阵，而这些矩阵可以应用到分割上从而获得新图像的 segmentation。</p> <p>We deform this image towards our target and then, we use the transformation to deform the segmentation towards our target, so that our segmentation kind of stretches and changes that tries to capture our new image.</p> <p>We can do it for all images in our atlas and then we get many versions of segmentations. And then we average them somehow (label fusion), and we just take this average result as a registered segmentation result.</p> <p>This is one of the main idea for using registration.</p> <p>What are the <strong>advantages</strong> and <strong>disadvantages</strong> of atlas-based segmentation?</p> <ul> <li> <p>Segmentation speed linearly depends on the number of training images $\implies adv$</p> <p>For any other algorithms, like deep learning algorithms, the number of training images does not affect the speed of segmentation itself. It can affect training but does not affect the segmentation. If we have million training images, the speed is the same.</p> <p>Because we registered every atlas image individually to our target, the speed directly linearly increases with the number of training images. $\implies disadv$ So the people investigate how we can use not all the training images but also some specific training images which are most similar to our target. So they apply some preprocessing and figure out which training images are most similar and then only register them to our target.</p> </li> <li> <p>Segmentation speed does not depend on the number of target structures</p> <p>There’s a couple of <strong>advantages</strong>. One advantage is that the <strong>speed of atlas-based segmentation</strong> <strong>does not depend on the number of structures we have</strong>. We can have one million different regions in the object but the speed will be the same. We just take training images, register them, take the transformation metrics and then deform the mask.</p> <p>But many alternative algorithms, maybe like random walker, their <strong>speed grows linearly with the number of objects we have</strong>.</p> </li> <li> <p>Needs way less training samples than deep learning</p> <p>Another advantage is that we need way less training images for algorithm to work.</p> <p>In contrast to now deep learning, we need thousands or hundreds images, we can use 20 segmented brains to have a reasonably good atlas-based segmentation.</p> </li> </ul> <h5 id="detecting-differences">Detecting differences</h5> <p><strong>Diffcult to see and examine differences/changes</strong> if the images are <strong>not oriented</strong> the same or <strong>on top of each other</strong>.</p> <p>There are many different reasons for wanting to register images:</p> <ul> <li> <p><u>Get different contrasts</u> from different modalities superimposed</p> <p>This refers to the ability to <u>overlay or combine</u> <u>images obtained from different imaging modalities</u>, such as MRI, CT, or PET. <u>Each modality provides unique information or contrast about the same anatomical region</u>. <u>Registering</u> these images allows you to see <u>the anatomical structures in relation to each other</u>, providing a <u>more comprehensive view</u> of the patient’s condition.</p> <p>(从不同模式的叠加中获得不同的对比)</p> <ul> <li> <p>Get an image with poor resolution or with little anatomical detail superimposed on an image with good anatomical information</p> <p>Sometimes, medical images may have <u>low resolution</u> or <u>lack fine anatomical details</u>. <u>Registering</u> such images with <u>higher-resolution</u> or <u>more detailed images</u> can help in <u>better visualization</u> and <u>understanding of the region of interest</u>. This process essentially <u>enhances the quality of the lower-quality image by aligning it with a reference image with better anatomical information</u>.</p> <p>(将分辨率低或解剖细节少的图像叠加到解剖信息丰富的图像上)</p> </li> </ul> </li> <li> <p>Inverstigate <u>changes over time</u> (e.g., before-after treatment)</p> <p>Image registration is used to <u>compare images acquired at different time points</u>, such as <u>before and after medical treatment</u> or surgery. By <u>aligning</u> these images <u>accurately</u>, you can <u>precisely quantify changes in size</u>, <u>shape</u>, or <u>intensity</u> of structures over time. This is valuable for <u>monitoring disease progression</u> or the <u>effectiveness of a treatment</u>.</p> <ul> <li> <p>See if there are differences in <u>size, shape</u> or <u>intensity</u> over time - more sensitively</p> <p>Image registration helps in <u>detecting subtle changes</u> in the size, shape, or intensity of anatomical structures over time. It <u>enhances the sensitivity of your analysis by ensuring that the images are aligned correctly</u>, allowing for <u>precise measurement of changes that might otherwise go unnoticed</u>.</p> </li> </ul> </li> <li> <p>To compare different individuals</p> <p>Image registration allows you to <u>align and compare images from different individuals</u>, enabling a quantitative and objective assessment of anatomical structures, pathology, or physiological variations <u>across a population</u>. This is especially valuable in research studies and clinical trials.</p> <ul> <li> <p>Analyze the images to see <u>differences between individuals</u></p> <p>Image registration facilitates the <u>automated analysis of images</u> to <u>identify and quantify differences between individuals</u>. This can include <u>variations in organ size, shape, location, or pathology</u>. <u>Automated analysis</u> saves time and reduces the potential for subjective bias that may occur in <u>manual comparisons</u>.</p> </li> <li> <p>Can <u>save a lot of time</u> - for example, rather than having to <u>manually draw the structures and compare them</u></p> <p>Image registration <u>significantly reduces the need for time-consuming manual tasks</u>, such as <u>manually outlining or drawing structures for comparison</u>. By automating the alignment and analysis of images, it streamlines the research and diagnostic process, making it <u>more efficient and less prone to human error</u>.</p> </li> <li> <p><u>Register to atals</u>, and <u>extract specific region</u></p> <p>Registering images to a common anatomical atlas allows for <u>precise localization and extraction of specific anatomical regions or structures</u>. This is <u>useful for conducting region-specific analyses</u> or for <u>guiding surgical procedures with high precision</u>. It also aids in standardizing data across different individuals for research purposes.</p> </li> </ul> </li> <li> <p>Important to check that the <u>registrations are correct</u> - be sure to <u>compare the same anatomical structure</u>.</p> </li> </ul> <h4 id="ingredients-of-image-registration">Ingredients of image registration</h4> <p>The registration algorithms have different aspects which they work with.</p> <p>One aspect is information type. What information they use are <strong>intrinsic</strong> or <strong>extrinsic</strong> information. Another one thing which actually depends on information type is the <strong>similarity measure</strong>. How to understand that images become more similar or less similar during the registration. And another thing is <strong>transformation</strong>. So for some applications that have <strong>rigid</strong> transformations, only translation, rotation for example. Or for some application and most often we need <strong>non-rigid</strong>. So we need to somehow change the relative position of pixels, how expansive areas fix some other areas to get the new object.</p> <ul> <li><u>What type of information</u> do we utilise?</li> <li><u>How do we quantify</u> similarity?</li> <li><u>What kind of transformations</u> can we use?</li> <li><u>What kind of interpolations</u> can we use?</li> </ul> <h5 id="sources-of-information">Sources of information</h5> <ul> <li> <p>Intrinsic information</p> <ul> <li> <p>The images are visually similar (non-necessarily by absolute intensities):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oEvhMvl.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Intrinsic information </div> </li> </ul> </li> </ul> <p>The intrinsic information is used when these images are visually similar to each other. The intensities are similar most important. Fro example, here you can see two same images and there is a histogram how good they match to each other. And now this image will be rotated and this histogram is moving and it’s becoming more like a straight line.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/9eJMBdI.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 5. Illustration of Histogram </div> <p>The histogram pixel reflects how much these two pixels are similar to each other. And the higher color is the more similar. So if you take the next pixel, how similar is the first pixel to second pixel and so on. And when we go through all pixels of first row, we compare the first pixel the first image to all pixels in the second image. And we just measure how similar they are.</p> <p>So if these images are very similar to each other, what we would expect is to see somehow the high values on the diagonal when we compare the corresponding pixels, and to see a low values outside the diagnoal, and we don’t compare the corresponding pixels.</p> <p>对角线上的值越大，说明两幅图片像素值视觉上越相近。</p> <p>So this is the idea of the histogram.</p> <p><strong>Extrinsic information</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/wDl0rkq.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 6. Example of extrinsic information </div> <ul> <li>The images are too different from each other visually</li> <li>We need to <strong>help registration by providing</strong> <strong>correspondences</strong></li> </ul> <p>The extinsic information, we usually work with images which are not similar to each other. So you can see these two images, one of them is CT and one of them is PET. And the intensities here are completely different.</p> <p>For example, we can see that the mandible is more or less visible, but in PET, the mandiable is not visible at all and the bone is invisible. But tumor is very much visible at PET, very poor visible in CT.</p> <p>So in that case, if the images have two different visually from each other. What we need to do is we need to provide some additional correspondences. And one of the most common thing to do is automated landmarks.</p> <p>We train algorithm to the specific points on both images, and we use these specific points to align the images. So we detect these points individually on PET, and detect these points on CT individually. And then we use these points to alignt he images to non-register form. So there’s a point measure to each other.</p> <h4 id="different-types-of-transformations">Different types of transformations</h4> <ul> <li>Rigid</li> <li>Affine</li> <li>Non-rigid</li> </ul> <h5 id="rigid-and-affine-transformations">Rigid and Affine transformations</h5> <p>See details in my previous blog post from <a href="https://liuying-1.github.io/assets/pdf/geometry.pdf">Signal and Image Processing - Transformations</a>. :point_left:</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UqtiM6p.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h5 id="non-rigid-transformations">Non-rigid transformations</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/aHW956w.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>Now let’s define this <strong>mathematically</strong>.</p> <p>How is a 2D <strong>transformation or rotation</strong> defined mathematically?</p> <p>Can we extend this to 3D?</p> <h5 id="rigid-registration---mathematically">Rigid registration - mathematically</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/cAO4SD8.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p><u>In what kind of applications to we want to use rigid registrations?</u></p> <p>E.g., <strong>Intra-subject rigid body</strong></p> <p><strong>Intra-Subject:</strong> “Intra” means <strong>within</strong>, and “subject” typically <strong>refers to an individual or a patient</strong>. Intra-subject, in this context, means that <strong>you are dealing with multiple images of the same individual or patient</strong>. These <strong>images are acquired at different time points</strong> or <strong>using different imaging modalities</strong> while <strong>focusing on the same anatomical region</strong> within the <strong>same person</strong>.</p> <p><strong>Rigid Body</strong>: In the context of image registration, a rigid body transformation is <strong>a mathematical transformation</strong> that includes <strong>translation</strong>, <strong>rotation</strong>, and <strong>scaling</strong> but does <strong>not allow for any deformation or stretching</strong> of the image. In other words, the <strong>relative positions and orientations of structures within the image can change</strong>, but the <strong>shapes and sizes of the structures remain the same</strong>.</p> <p><strong>Intra-Subject Rigid Body Registration</strong>:</p> <ul> <li>This refers to the process of <strong>aligning</strong> and <strong>registering</strong> images of the <strong>same individual or patient</strong> that have been obtained under <strong>different conditions</strong> or at <strong>different times</strong>.</li> <li>For example, if <strong>a patient undergoes a series of medical imaging scans over time</strong> (e.g., MRI or CT scans) to monitor a condition or treatment <strong>progress</strong>, you may use <strong>intra-subject rigid body registration to align these images</strong>.</li> <li>The purpose is to ensure that <strong>corresponding anatomical structures in each image are in the same spatial position and orientation</strong>, even if the patient’s position or the imaging conditions have changed slightly between scans.</li> <li>The primary goal is typically to <strong>track changes in the patient’s anatomy</strong>, such as tumor growth, organ movement, or any other anatomical variations, with precision and accuracy.</li> </ul> <p>Applications of <strong>intra-subject rigid body registration</strong> can be found in various medical fields, including <strong>radiation therapy</strong>, where it’s crucial to <strong>ensure that the radiation beam is accurately targeted at the same area during multiple treatment sessions</strong>, or in <strong>longitudinal studies to monitor disease progression</strong> or <strong>response to treatment within the same patient</strong>.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JpzfBT9.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In summary, <strong>intra-subject rigid body registration</strong> involves <strong>aligning and comparing images of the same individual taken at different times</strong> or <strong>under different conditions</strong>. It’s a fundamental technique in medical image analysis used for <strong>monitoring, diagnosis, and treatment planning</strong>, where precise <strong>spatial alignment of anatomical structures</strong> is essential.</p> <h5 id="affine-registration---mathematcically">Affine registration - mathematcically</h5> <p>However, basically, the scaling is considered as affine transformation.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/CRsio4B.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In what kind of applications to we want to use affine registrations?</p> <p>E.g. <strong>inter-subject</strong></p> <p><strong>Inter-Subject</strong></p> <ul> <li>“Inter” means <strong>between or among</strong>, and “subject” typically refers to <strong>different individuals</strong> or <strong>patients</strong> in the context of medical imaging.</li> <li>Therefore, “inter-subject” means that <strong>you are dealing with images acquired from different individuals or patients</strong>.</li> </ul> <p><strong>Affine Transformation</strong>:</p> <ul> <li>An affine transformation is a type of transformation that <strong>includes translation, rotation, scaling</strong>, and <strong>shearing</strong> but does not allow for <strong>deformation or non-linear warping of the image</strong>. Essentially, it’s a more flexible transformation than rigid body transformations (which only include translation and rotation) but still <strong>preserves the basic shape of objects</strong>.</li> </ul> <p><strong>Inter-Subject Affine Transformation</strong>:</p> <ul> <li>This refers to the process of <strong>aligning and registering images acquired from different individuals or patient</strong>s.</li> <li>In medical imaging, you might have a set of images from different patients, such as MRI scans of different brains, CT scans of different chests, or X-rays of different limbs.</li> <li>Inter-subject affine transformation allows you to <strong>align and compare these images</strong>, despite variations in size, orientation, and scaling between individuals.</li> <li>The primary goal is to <strong>establish a common spatial coordinate system that enables meaningful comparisons</strong>, such as in population-based studies, group analyses, or template-based approaches.</li> </ul> <p>Applications of <strong>inter-subject affine transformation</strong> can be found in various areas of medical research and clinical practice, such as <strong>comparing anatomical structures across a population</strong>, <strong>creating population-based atlases or templates</strong>, and <strong>performing group-level statistical analyses</strong>. It’s particularly useful when you need to make comparisons or perform analyses that involve images from <strong>different individuals</strong> while accounting for variations in size, orientation, and scaling.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/ngmeDyx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In summary, inter-subject affine transformation involves <strong>aligning and registering images from different individuals or patients</strong>, allowing for <strong>meaningful comparisons and analyses across a population</strong>. This technique plays a crucial role in various medical image analysis applications, particularly in group studies and population-based research.</p> <h5 id="example-of-affine-registration-across-several-subjects">Example of affine registration across several subjects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/XNxsaN5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h5 id="non-rigid-registration---mathematically">Non-rigid registration - mathematically</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Vw4yIRX.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In what kind of applications to we want to use non-rigid registrations?</p> <p>E.g. <strong>intra or inter-subject non-rigid body</strong></p> <p>A 3x3 matrix that represents <strong>a non-rigid transformation</strong>. <strong>Each element in the matrix affects the deformation</strong> of the image in different ways. The exact values of these elements determine <strong>how the image is deformed locally</strong>. Here’s a brief explanation of the elements:</p> <ul> <li><code class="language-plaintext highlighter-rouge">m1</code>, <code class="language-plaintext highlighter-rouge">m5</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control <strong>scaling</strong> factors in the x, y, and z directions, respectively.</li> <li><code class="language-plaintext highlighter-rouge">m2</code>, <code class="language-plaintext highlighter-rouge">m4</code>, <code class="language-plaintext highlighter-rouge">m6</code>, and <code class="language-plaintext highlighter-rouge">m8</code> control <strong>shearing</strong> or skewing effects.</li> <li><code class="language-plaintext highlighter-rouge">m3</code>, <code class="language-plaintext highlighter-rouge">m7</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control the <strong>actual deformation or warping</strong>.</li> </ul> <p><strong>1. Why Use Non-Rigid Transformations?</strong></p> <ul> <li><strong>Non-rigid transformations</strong> are used when you need to account for <strong>more complex changes in the shape and structure of objects or regions within the image</strong>. Rigid and affine transformations are too restrictive for scenarios where local deformations occur, such as changes in the shape of organs or tissues, as in medical image analysis.</li> </ul> <p><strong>2. Applications of Non-Rigid Registrations (Intra or Inter-Subject):</strong></p> <ul> <li><strong>Intra-Subject Non-Rigid Registration</strong>: In this case, you are dealing with images of the <strong>same individual or patient at different time points</strong>. <strong>Non-rigid registration</strong> can be used to account for <strong>anatomical changes over time</strong>, such as organ deformation during respiration, muscle contractions, or gradual changes in tissue structures due to disease progression.</li> <li><strong>Inter-Subject Non-Rigid Registration</strong>: When you have images from <strong>different individuals (inter-subject)</strong>, <strong>non-rigid registration</strong> becomes valuable for <strong>aligning images that exhibit significant anatomical variations</strong>. Examples include aligning MRI scans of different brains, which have <strong>varying shapes and sizes</strong>, or comparing images of different patients with congenital or acquired deformities.</li> </ul> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JPahdsm.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In both <strong>intra-subject</strong> and <strong>inter-subject</strong> scenarios, <strong>non-rigid registration allows you to account for the complex and local variations in anatomy</strong>, enabling <strong>accurate comparisons, analysis, and medical interventions</strong>. It’s particularly important when dealing with deformable structures and when rigid or affine transformations are insufficient to capture the necessary deformations. Applications range from disease monitoring to surgical planning and population studies.</p> <p><strong>1. 为什么要使用非刚性变换？</strong></p> <ul> <li>当您需要考虑<strong>图像中更复杂的物体或区域的形状和结构发生变化</strong>时，就需要使用<strong>非刚性变换</strong>。对于医学图像分析等领域，刚性和仿射变换对于局部变形场景太过受限制。</li> </ul> <p><strong>2. 非刚性配准的应用（个体内或个体间）：</strong></p> <ul> <li><strong>个体内非刚性配准：</strong> 在这种情况下，您处理的是<strong>同一人或患者在不同时间点的图像</strong>。非刚性配准可用于考虑随时间发生的解剖变化，例如呼吸过程中的器官变形、肌肉收缩或由于疾病进展引起的组织结构逐渐变化。</li> <li><strong>个体间非刚性配准：</strong> 当您有来自不同个体的图像（个体间）时，非刚性配准变得重要，以对齐展示明显解剖差异的图像。例如，对齐具有不同形状和大小的不同大脑的MRI扫描图像，或比较不同患者具有先天性或后天性畸形的图像。</li> </ul> <p>在个体内和个体间的情况下，非刚性配准允许您考虑解剖的复杂和局部变化，从而实现准确的比较、分析和医疗干预。它在处理可变形结构和刚性或仿射变换无法捕捉必要变形的情况下尤为重要。应用范围从疾病监测到手术规划和人群研究。</p> <h5 id="example-of-non-rigid-registration-across-several-subjects">Example of non-rigid registration across several subjects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vY3vg4N.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h4 id="similarity-measures">Similarity measures</h4> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6imIdry.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 7. MSE </div></div> <p>So let’s talk about <strong>similarity measures</strong>.</p> <p>What could we use as <strong>similarity measure</strong>?</p> <ul> <li>Absolute difference</li> <li><strong>Sum of squared difference</strong></li> <li><strong>Cross-correlation</strong></li> <li>Mutual information</li> </ul> <p>There are two types of similarity measures described above? What are the two types?</p> <h5 id="mean-sum-of-squared-differences">Mean sum of squared differences</h5> \[MSE = \frac{1}{n}\frac{1}{m}\sum^{n}_{x=1}\sum^{m}_{y=1}\left(I(x, y)-J(x,y)\right)^2\] <p>We have one of the most obvious similarity measure is the mean sum of squared differences. So what we do would take the same pixel in each image, and compare the intensity difference and we square it and we sum up all squared differences and we divided by the number of pixels we have.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vaWcNyY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 8. MSE calculation </div></div> <p>So here is an example of these two images and the result of similar measure calculation. And the smaller the value, the most similar the images are. So we want to <strong>minimize</strong> it.</p> <h5 id="normalized-sum-of-squared-differences">Normalized sum of squared differences</h5> <p>The sum of squared differences works well if the images have the same intensity ranges. What if they have <strong>a different intensity ranges</strong>?</p> <p>One of the good idea which is used almost always when the pixel-wise comparison is used is <strong>normalization</strong>.</p> <p>What we do is we take our intensities in our image, we compute the mean intensity, we compute the standard deviation. We can test this and we normalise them. From the original images subtract the mean that the new mean will become $0$. And we divide it by the standard deviation.</p> \[I^* = \frac{I-\overline{I}}{\sigma(I)}\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/3w7cKY7.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 9. NMSE </div></div> <p>By doing this, everything is the same except the <strong>intensity range</strong>. And it is more good useful in practical applications.</p> <p>So <strong>normalization</strong> is one of the things which is very commonly used and this is <strong>pixel-wise similarity measure</strong>.</p> <h5 id="normalized-cross-correlation">Normalized cross-correlation</h5> <p>Another idea which we can go further in this area, what we can do is we can do the <strong>cross-correlation</strong>.</p> \[NCC = \frac{\sum_{x, y}\left(\left(I(x, y) - \overline{I}) \cdot(J(x, y)-\overline{J}\right)\right)}{\sqrt{\sum_{x, y}(I(x, y) - \overline{I})^2\sum_{x, y}(J(x, y)-\overline{J})^2}}\] <p>So what we do is to <strong>find the normalized pixel values</strong> and then <strong>multiply each other</strong>, and <strong>find the sum of them</strong>. And then we divide it by the squared root of multiplication of squared difference. So to see how these things behaves, let’s go to the mirror board to see some examples.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/p8i5spT.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 10. Cross-correlation </div></div> <p>So, what we would like to do is would like this expression to get good values when we compare the upper two patches, and to have bad values when compared to the other two. We are going to quickly compute them to see what happens.</p> \[NCC_{good} = \frac{3\cdot3 + 3\cdot(-1)\cdot(-1)}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{12}{12}=1\] <p>What about the bad patches?</p> \[NCC_{bad} = \frac{-3+1+1-3}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{-4}{12}=-\frac{1}{3}\] <p>What we would like the normal cross correlation to be as high as possible because we saw that for the <strong>same patch</strong>, we get $1$. And for two different patches, we get $-\frac{1}{3}$. And we actually see one of the good property of the normal cross-correlation. It seems to be that its values are actually restricted into the $-1$ and $+1$ range. So when we get to same patches, basically the best possible scenario. We get the result with $1$. So probably if we get the opposite patch, we will probably get $-1$.</p> <p>The good thing about is normalise as cross correlation is, that’s why the name comes that our values. They <strong>get normalized</strong> into the $-1$ to $+1$ range. This is very <strong>useful</strong> when we compare two patches.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/M4pqEGG.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 11. Maximization </div></div> <p><strong><em>Will MSE and NCC work for CT-MR image registration?</em></strong></p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/SCQfK8H.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 12. CT-MR </div></div> <p>We want to perform <strong>registration</strong> on <strong>CT-MR</strong> image on two images, which are <strong>very different intenstity patterns</strong>. The edges in these images are kind of <strong>similar</strong>.</p> <p>You can see that there is a <strong>mandible</strong> on the right chart, which is <strong>black</strong> but on the left side, it is <strong>white</strong>. You can see the edges over human skin. They are also kind of more or less visible. But what is important is that the intensities now is completely different. So the intensitity plan is completely different.</p> <p><strong><em>Now the question is if you can use MSE or NCC or something like this for this registration.</em></strong></p> <p>Actually turns out we <strong><u>cannot</u></strong> use them because what MSE and what all of these metrics for what they tried to do is try to <strong><u>match high intensity with high intensity, and low intensity with low intensity</u></strong>.</p> <p>But in our particular case, what we would like to do is we want to <strong>match somehow the patterns the edges within the pixels</strong>. For example, we want to match a bone, where a black and white intensities in different patches. <strong>It’s very difficult to just in advance define this, so we can know what exactly we want to match</strong>. =&gt; <em>But we want to match patterns not individual intensities.</em></p> <h5 id="mutual-image-information">Mutual image information</h5> <p>For this reason, there is another metric which is called <strong>mutual image information</strong>.</p> <p>The metric is most commonly used in <strong>image registration</strong>. It requires a lot of computation resources, but at the same time, it is a very <strong>versatile</strong> can be applied for many different images. I do not need to know many things about images in advance. We can just apply it and it will most likely work.</p> <p>Here is an equation how mutual information works.</p> \[MI = E(I, J)\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6FN99jJ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 13. Joint entropy </div></div> <p>So we need to compute the entropy issues and we will compute in histogram manner.</p> <p>Below is the illustration.</p> <p>Let’s say we have different images which we want to match each other. Let’s assume these are the images of some object.</p> <p>What we do is we create a 2D plot where at a lower dimension.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oosMzEx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 14. Illustration </div></div> <p>Then, we will do the same thing for the next pixel, and ….. it will finally come with a cloud of points. And we are interested to how <strong>compact</strong> is this cloud.</p> <p>From this, if there are two images just inversion of each other, our plots will be very compact. It means, we just need very small regions to cover all the possible pixels. But when the images were random, the pixels which would plot it on this 2D plot, they would distribute to the space in a completely manner. They could be occupied a lot of area.</p> <p>So what we can do and the idea of the mutual information is to complete a histogram of the pixels in both axises $x$ and $y$ and which is a probability histogram. And then compute probability histogram for joint probability.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/K8wN4pA.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 15. Illustration </div></div> <p><strong><u>So the more compact the more higher will be mutual information.</u></strong></p> <p>And here is basically how they look like for this particular image.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Wuk9Ahc.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 16. Mutual information </div></div> <p>These are with the mutual information for these two images which are just inversion of each other.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/GFL1GWk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 17. Rotation mutual information </div></div> <p>Here is the rotated version. And you can see that the plot now is no longer compact as it was. It occupies a lot space. And that’s why the mutual information is way lower. And our ideas is to <strong>maximize mutual information</strong>.</p> <h4 id="interpolation">Interpolation</h4> <p>Another concept which we need to know is the concept of <strong>interpolation</strong>.</p> <p>What different interpolation techniques do you know?</p> <ul> <li>Nearest neighbour</li> <li>Linear interpolation</li> <li>Barycentric interpolation</li> <li>Spline interpolation</li> </ul> <h5 id="interpolation-effects">Interpolation effects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/1EVjwqL.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 18. Interpolation effects </div></div> <p>Different interpolation techniques work on diff. images, the choice of interpolation technique can have varying effects or perform differently depending on the characteristics and content of the input images.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YanUjxl.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 19. Effects of different interpolation techniques </div></div> <p>不同插值技术中提到的 RMSE（均方根误差）值可以作为一种量化指标，用于评估每种插值方法应用于特定图像时的质量或准确性。RMSE 是一种常用指标，用于量化原始图像与插值（转换）图像之间的差异。</p> <p>The <strong>interpolation</strong> in image registration plays two roles.</p> <p>One of them is for <strong>computing the values of pixels</strong>. Let’s say if the pixels has a coordinate which do not match original coordinates. For example, one pixel has a coordinate $(0, 0)$. But if we would like to move it a little bit, $(-0.1, 0.3)$. <strong>What kind of value of this pixel have?</strong> Should we just around these numbers to get $(0, 0)$ or should we do something more intellectual.</p> <p>所以就是当我们做配准时，我们移动了一点点图像，那么原先点上的像素应该是变成多少呢？使用插值法来计算。</p> <p>Actually we take account the neighboring pixels if they can play some role in the value of a new pixel. There is one way how we can compute it.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YYy2YJ4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 20. Example of interpolation </div></div> <p>We have four points $(0,0), (1,0), (0,1)$ and $(1,1)$. Let’s say there are pixels $1$, $2$, $3$ and $4$. And would like to find the value of a pixel which will be located here. And then the question is <strong>what will be the value of the pixel</strong>? We know the intensities of these four pixels, and also the corresponding $(x, y)$ of the target pixel.</p> <p>So the way to do it is to <strong>calculate the contribution</strong>. The contribution of this pixel to our new pixel. Let’s just imagine that this $x, y$ will be very close to $1$.</p> \[x, y\approx 0.999\] <p>So how similar should be the contribution of the pixel? Each of these four pixels or contribute something to the intensity.</p> <p>The contribution of $I(0, 0)$:</p> <ul> <li>Proportional to the opposite “rectangle”.</li> </ul> \[(1-x)\cdot(1-y)\cdot I(0, 0)\] <p>The same logical goal for other four pixels. So for pixel $(1,1)$, it’s contribution to the intensity of our new pixel through the proportional to this.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/J0lWvji.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 21. Contribution illustration </div></div> \[\begin{align} I(\cdot) &amp;= (1-x)\cdot(1-y)\cdot I(0. 0) \\ &amp; + x\cdot (1-y)\cdot I(1, 0) \\ &amp; + (1-x)\cdot y \cdot I(0, 1) \\ &amp; + x \cdot y \cdot I(1, 1) \end{align}\] <p>So the rule is that we compute the <strong>size of these boxes</strong> and then <strong>multiplying them</strong> by the opposite pixel. Then, sum them up and get the final new value of the pixel.</p> <h4 id="image-registration-is-optimization">Image registration is optimization</h4> <p>We need to try to apply registration algorithm, try to optimise registration process.</p> <p>So what we can learn until now is we now know two pixels in the space and their neighborhood of them, how to compute how similar are they using different similarity measures based on pixel based comparison, like sum of quare differences or normalized cross correlation, or based on some advanced techniques like mutual information.</p> <p>We also know how this image moving will change if we move it 7.5 pixels up, how many intensities will change. =&gt; Interpolation</p> <p>So we know all this and we can try to formulate the registration, how the registration should be done.</p> <p>So what we need to do is we need to <strong>minimize</strong> depending on what kind of similarity measure we use. We want to minimize a distance metric.</p> \[\min\sum_{i, j}d\left(I(i, j) - J\left(x(i, j, \theta), y(i, j, \theta\right)\right)\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/fgLvTG6.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 22. Similarity measure </div></div> <p>So here is our formulation of for registration optimization. What we need to find is to find $\theta$. <strong><em>The question is how to do it.</em></strong></p> <p>The common way to solve it is using an alogrithm called <strong><u>gradient descent algorithm</u></strong>. So let’s just choose the most simple similarity measures we have. So, for $d$, let’s choose the squared intensity difference. So basically, we just take a pixel and find the difference between each pixel individually, and that’s it. No mutual information or anything complex like that.</p> \[d(I(i, j), J(x, y)) = (I(i, j)-J(x, y))^2\] <p>And $\theta$ will only use one transformation - translation. So basically $F$ from $I$ to $J$ is basically a translation we would translate and the $I$ to some value that the $x$ and $J$ to some value.</p> \[f(I, J, \theta) - \text{translation using }\theta \\ f(i, j, \theta) = [i+\theta_x, j+\theta_y]\] <p>And this is one of the most simple configuration we can have, and now the question is <strong>how we get optimized solution</strong>? How we can find the optimal $\theta$? And the way to do it is to use the <strong>gradient descent algorithm</strong>.</p> <p>Then, the formulation of the problem is, <strong><u>we want to optimize</u></strong>:</p> \[E(\theta) = \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta))^2\] <h5 id="gradient-decent-algorithm">Gradient decent algorithm</h5> <p>By using <strong>gradient decent algorithm</strong>:</p> \[\nabla E(\theta) = \frac{\partial E(\theta)}{\partial \theta}\] <p>And move our solution move like take some $\theta$, change our $\theta$ according to the value of the gradient.</p> \[\theta_{t+1} = \theta_{t} + \eta\nabla E(\theta)\] <p>Usually weighting factor is something very small, like $0.01$. In deep learning, it is called, learning rate. So the question is how to find the gradient of the expression?</p> <p>In our case,</p> \[\nabla E(\theta) = \nabla \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))^2 = \\ -\sum_{i, j} 2(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))\cdot\frac{\partial J}{\partial \theta}\] <p>Now, the question is how to differentiate the $J$ against $\theta$.</p> \[\frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial X} \cdot \frac{\partial X}{\partial \theta}\] <p>And this is again a complex function, we need to first differentiate it against its coordinates (transformed coordinates), basically aginst $x$ and $y$.</p> <p>So if you want to differeniate an image against its coordinates, what it means is actually very simple thing.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Lx6vlHx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> \[\frac{\partial J}{\partial x} = \frac{1}{2}(I(x+1, y) - I(x-1, y))\] <p>Find two neighboring values for every pixel, subtract them from each other, and then multiply by 0.5. And then we do the same thing for the vertical direction.</p> \[\frac{\partial J}{\partial y} = \frac{1}{2}(I(x, y+1) - I(x, y-1))\] <p>Below is the illustration.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YQxD2o5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 23. Understanding </div></div> <p>This part is easy, but the question is how to find the last part on the abovementioned differentiation, the gradient of our coordinates against our transformation.</p> \[T(x, y) = \begin{bmatrix}1 \quad 0 \quad \theta_x\\0 \quad 1 \quad \theta_y\\ 0 \quad 0\quad 1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix} = [x+\theta_x, y+\theta_y]\\ \frac{\partial X}{\partial \theta} = \begin{bmatrix} \frac{\partial X}{\partial \theta_x}; \frac{\partial X}{\partial \theta_y}\end{bmatrix} = [1, 1]\] <p>Now, we have everything.</p> \[\frac{\partial J}{\partial \theta} = [\frac{1}{2}\left(I(x+1, y\right) - I(x-1, y), \frac{1}{2}(I(x, y+1) - I(x, y-1))]\] <p>This is just differentiaion of the image because our transformation is just translation. Then, we can compute the whole graient of our system.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/RkzH4GA.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 24. Whole gradient </div></div> <p>Iteration and computation until it stops.</p> <h5 id="problem-with-optimization">Problem with optimization</h5> <p>This is a very expensive procedure to do as these similiarty measures $d(I, J)$ can be complex and expensive to compute (mutual information - regions around pixels).</p> <p>Another issue is that $\theta$ can contain many variables. In reality, $\theta$ can have thousands of variables, so each individual pixel can move individually. So each pixel will have transformations. Unlike in our case, only two variables.</p> <ul> <li> <p>9 for similarity transformations</p> \[\begin{bmatrix} \theta_1 \quad \theta _2 \quad \theta_3\\ \theta_4 \quad \theta_5 \quad \theta_6 \\ \theta_7 \quad \theta_8 \quad \theta_9 \end{bmatrix}\] </li> <li> <p>thousands for non-rigid transformations</p> </li> </ul> <p><strong>How to simplify this?</strong></p> <p>What people usually do is they use so-called <strong>multi-resolution pyramid registration</strong>.</p> <h5 id="multi-resolution-pyramid-registration">Multi-resolution pyramid registration</h5> <ul> <li>Downscale (and smooth) reference and moving images x16 (types)</li> <li>Register them and memorize transformation</li> <li>Downscale (and smooth) reference and moving images x8, apply memorized transformation</li> <li>Register images and combine transformations from both steps</li> <li>Continue</li> </ul> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/PiCMkeP.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 25. Multi-resolution pyramid registration </div></div> <p>“多分辨率金字塔配准” 是一种在图像配准中使用的技术，特别是用于将两个具有不同分辨率或尺度的图像对准。它采用分层方法逐步对齐和优化两幅图像之间的配准。让我解释一下您提供的要点，以说明这个技术的工作原理：</p> <ol> <li><strong>将参考图像和移动图像降采样（并平滑）16倍（类型）</strong>： <ul> <li>首先，参考图像和移动图像都会降低分辨率，降低16倍，并可选择平滑以减少噪音。</li> <li>这一步创建了图像的低分辨率版本。</li> </ul> </li> <li><strong>将它们进行配准并记忆变换</strong>： <ul> <li>对降采样的参考图像和移动图像进行配准，即调整它们的空间对齐以找到最佳匹配。</li> <li>记忆或保存调整这些低分辨率图像的变换。</li> </ul> </li> <li><strong>将参考图像和移动图像降采样（并平滑）8倍，应用记忆的变换</strong>： <ul> <li>参考图像和移动图像再次降低分辨率，这次降低8倍。</li> <li>先前记忆的变换（来自步骤2）被应用于这些低分辨率图像。</li> <li>这一步创建了更高分辨率但仍然减小比例的对齐图像。</li> </ul> </li> <li><strong>对图像进行配准并结合来自两个步骤的变换</strong>： <ul> <li>来自步骤3的更高分辨率参考图像和移动图像进一步配准，进一步精细调整它们的对齐。</li> <li>从这个配准步骤获得的变换与步骤2中的记忆的变换组合在一起。</li> <li>这些变换的组合确保图像在更高分辨率下准确对齐。</li> </ul> </li> <li><strong>继续</strong>： <ul> <li>这个过程以迭代方式继续，图像逐渐降低分辨率，逐步对齐和变换。</li> <li>在每个级别上，变换得到进一步的精细调整和组合。</li> <li>这一过程持续进行，直到达到最高所需分辨率或达到收敛条件。</li> </ul> </li> </ol> <p><strong>关键点</strong>：</p> <ul> <li>多分辨率金字塔配准是一种处理具有不同细节水平和分辨率的图像的常见策略。</li> <li>它从低分辨率开始进行粗略对齐，然后随着考虑到更高分辨率逐步优化对齐。</li> <li>这种方法可以显着加快配准过程，提高对齐的准确性，特别是在处理大型或复杂图像时。</li> </ul> <p>总的来说，多分辨率金字塔配准是一种强大的技术，用于稳健地对齐具有不同尺度或细节级别的图像，尤其在医学影像、计算机视觉和遥感等领域，其中图像可能具有不同的分辨率或包含细节信息。</p> <p>When you even do pyramid approach, there is still a problem in computation.</p> <p>In the pyramid approach, people don’t do it for every pixel, people do it for a grid. So you just define a grid of points. For example, the size of the image is $100 \times 100$, but the number of grid points is $10 \times 10$. So we just place great place grid points at $10 \times 10$, skip every pixels and base with grids. And what we do is we compute our similarity measure only for this points. And we compute the optimization only for these points. And then we somehow inerpolate the transformation for the internal points.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DV1yHKh.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 26. Grid-based registration </div></div> <ul> <li>Do not perform registration per pixel but use a sparse grid</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Learning note for medical image registration.]]></summary></entry><entry><title type="html">Segmentation Basics</title><link href="https://liuying-1.github.io/blog/2023/segmentation-basics/" rel="alternate" type="text/html" title="Segmentation Basics"/><published>2023-09-21T08:00:00+00:00</published><updated>2023-09-21T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/segmentation-basics</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/segmentation-basics/"><![CDATA[<p><b><i>Disclaimer: All the material below is refered to the course, Medical Image Analysis, delievered by the University of Copenhagen.</i></b></p> <h4 id="todays-learning-objectives">Today’s Learning Objectives</h4> <ul> <li>Define what segmentation means</li> <li>What kind of segmentation methods exist</li> <li>Explain how connected component decomposition works</li> <li>How can you use dilation-erosion operations</li> </ul> <p>First, I will tell you <u>what segmentation means</u> and then I will familliarize you with some <u>existing simple segmentation methods</u>. <u>These methods can be used as</u> and are used very often despite very simple, not usually as independent solutions, but actually, as <u>auxiliary methods</u>. And I will also show you a couple of useful tools, and one of them will be the connected component decomposition and another one will be more for logical operations. And I will show you how we use them in practice, how you can augment medical image segmentation with these methods.</p> <h4 id="topics-of-interest">Topics of Interest</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/iFssnJ1.png" alt="image-20230923063424696" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Topics of interest in segmentation conference </div> <p>In this chart, I tried to recover from my own memory, and one of the medical image conferences the organizer presented a separation of all submissions into different topics. Of course, you can separate this submissions separately, but as far as I remeber, the separation is approximately like this. You can see that, <u>segmentation</u> actually occupies the majority of submissions which are presented at conferences. So <u>this is a topic which attracts the most interest in the community</u>.</p> <h5 id="reconstruction">Reconstruction</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/5ctIAih.png" alt="image-20230923064608513" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Topics of interest - Reconstruction </div> <p>When the 3D images of human body are required, the human body are typically positioned inside a tubular room, and the detector goes around the human body, spins around and acquire a series of 2D image projections. <u>And these images are stacked one after another into 3D volume</u>, which further doesn’t look like a 3D volume of human body. It looks like the image on the left, which is the <u>spectrum</u> of the image. In CT, it’s called <u>sinogram</u>. And there is an <u>algorithm, which can then reconstruct</u> the true 3D volume from this sinogram, and there is a theory which says, <u>it is always possible for a perfect accurate sinogram to reconstruct a perfect accurate 3D volume</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/G33TMox.png" alt="image-20230923070541523" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. Spectrum </div> <p>However, in real life, we <u>never have a perfect smooth and complete image of spectrum or sinogram</u>. And the reason is very simple, <u>we can not acquire images at every possible step</u> (degree) when the detector spins around the human body. And we have to do some sacrifices. We have to acquire images from some steps, and due to these steps, <u>due to a lower number of images acquired</u>, we first of all <u>speed up the acquisition time</u> where reduce the radiation dose delievered to the patient. For example, we require CT images, and at the same time, we kind of <u>make our CT scanners less expensive</u>.</p> <p>And <u>the problem what we are facing is</u>, <u>what is the minimum number of images need to reconstruct to another anatomy</u>. It turns out that currently, if we cannot use really record high-quality motion life images which we may need sometimes. For example, to record how heart moves, how human breaths, and so on. And this problem <u>results in two options</u>. Either we have <u>a poor quality image with motions</u>, or we cannot acquire motion at all. We require <u>static images, which can be affected by motion artifacts</u>. The researchers start to think, what if we acquire less images, <u>what if we start from a poor solution</u>, poor quality images, and try to develop algorithms. Knowing something about human anatomy, try to <u>develop algorithms to improve image reconstruction</u>. Maybe we can use the existing scanners, poor quality scanners, and see if we can get good images with using computational methods. And here is some example from work from Nvidia where they only take 10% of 2D projection images generated by a MR scanner, and apply <u>deep learning to generate a reconstruction</u> (center above). So you can see that, by using existing techniques, it can work well.</p> <h5 id="synthesis">Synthesis</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/bJBcsRU.png" alt="image-20230923072252122" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Topics of interest - Synthesis </div> <p>The next topic of interest is <u>image synthesis</u>. The idea of image synthesis is to <u>generate artificial image modalities from other modalities</u> of the same patient.</p> <p>Let’s say we have <u>a CT image of a patient</u>. And <u>image synthesis algorithm try to generate a MR image of a same patient in the same position having in the CT image</u>. Why is this useful?</p> <p>Let’s say we have <u>a solution with toolbox</u>, which require CT and MRI images to <u>diagnose the disease</u>. And let’s say we install this toolbox in a place where there is no MRI scanner. In such a place, we may <u>either wait when the MRI scanner gets installed too so that we start to use this toolbox</u>. Or we can <u>apply the synthesis to generate the artificial MR scanners</u>, and try to use this toolbox. This is one of the way how synthesis can be used.</p> <p>Another way of synthesis where synthesis is useful is <u>detection of abnormalities</u>. Let’s say we have a CT image of a patient. As you can see from the illustration (above), some of the structures of the brain, some of them are soft tissues are very poorly visible in CT. And this means, if there is a small tumor, it’s possible that we will not even see it in the CT image. At the same time, this tumor will be very well visible in MR. And what we can try to do is we can try to use CT image if we have both CT and MR images of the same patient, we can try to take a CT image and generate a diffierential MR image. During this generation, that newly generated MR, will have no tumor because the tumor is almost invisible in CT, because the algorithm doesn’t actually know where is the tumor. And it will generate us a more or less clean image of overhead. And when we subtract our true MR from the artificially synthesised MR, it is possible that tumor will be highlighted because there is no tumor under artificial synthesised MR image, there is one in the true one.</p> <h5 id="image-guided-interventions">Image-guided interventions</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/use2VlQ.png" alt="intenvention" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Topics of interest - Synthesis </div> <p>Another topic of interest is a very large one, it is <u>Image-guided procedures</u>. Usually, image procedures require the use of many combinational tools, including image segmentation, and image registration. The key think that we separate this topic of interest is that they are much centered at a specific procedure. They are very target. Actually protocal is procedure. And one good example is image-guided procedure, image intervention is insertion of medical screws into the patient vertebra.</p> <p>Why do we even need this procedure? Some patients have very severe form of scoliosis or we have spine damage. In order to fix the spine curvature, it might be necessary to insert screws into the patient’s spine. And put a back cord for the screws so that the spine will be tilted and straighted. According to the anatomical requirements of human body, and insert screws is a procedure will actually actively straighten this spine and allow the patient to function properly. When we do this insertion, we need to be very careful that the screws do not go into wrong place. First of all, the screws should not break the vertebra. If we break the vertebra, it is possible that we instead of help patient but more serverely than he was already injured. Another problem is that the screws should not go into the spinal cords. We should be very careful that screws go into the spinal cords, otherwise, the outcome might be devastating. And also, the screws inserted properly may get loosen, so the procedure should be repeated again. To answer all these questions, we need to analyse images, we need to see how the anatomy of the patient is organized. In order to position these screws perfectly through particles of vertebra.</p> <p>There are many other applications of Image-guided interventions, for example, epidemic treatment, femoral head replacement, and some surgeries.</p> <h5 id="computer-aided-diagnose">Computer-aided diagnose</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/R7C89xq.png" alt="image-20230923090634240" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 5. Topics of interest - Computer aided diagnose </div> <p>One of the topics that has been gaining a lot of interest recently is computer-aided diagnoses. For example, it could be a paper which was published in nature on skin image analysis by stanford. The idea here is that they take an image of the skin, take photos of them, and pass them for neural network, try to predict different skin dieases. Other applications could be prediction of diabetes, and some other applications like prediction of long pathologies from just xrays.</p> <h5 id="registration">Registration</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/eKF7zGm.png" alt="image-20230923091229536" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 6. Topics of interest - Image Registration </div> <p>During our course, you will have several lectures of image registration, because if not knowing what image registration is or how it works, it is impossible to work in the field of medical image analysis.</p> <p>And the image registration, what it tries to do is, it tries to find a transformation from one image to another. As you can see in this example, what we try to do is we’ll try to take the image which is located at the left up top corner, and slowly step by step with deform it so that the it can come to the image located at the right bottom corner. And we slowly step by step deform it until it turns into again into the image at the left top. And the practical accountability of image registration is significant. One of the ways we can use registration is to monitor disease progression.</p> <p>Let’s say the patient has been imaged, and the doctor prescribed him a treatment. And after some time, after several months, he would like to follow up how this diease is managed by treatment. And let’s say these dieases affect some structures in the human body, maybe it’s a tumor, which should shrink. Let’s see if the treatment is chemotherapy or regular therapy or maybe it is a disease, like brain, some degenerative diease in the brain like dementia, which results in degradation of certain brain structures. And what you would like to know is quantatively assess how much the tumor or some brain starts just change. What we would do is to take the image of the patient which came at the time of diagnosis. Annotate the structure there, and deform it towards a patient image which we see after at the follow-up time. A lot of changes will tell us how fast it is progressing and whether it’s a treatment has been affected. Another way to use image registratin is to segment structures of interest through registration.</p> <p>Let’s say we have a patient whose structure inside his body has been segmentated manually and very lightly. We’re interested to segment the same structure on a new image. What we can do is we can manually do it take a instrument like paint brush in some toolbox, and manually outline the structure which is very time-consuming and might be imprecise. But the other way of doing it is to take an image which we have and deform this image of other patients towards our new image and simultaneously deform the structures. Use these deformation to understand how structures are depicted.</p> <h4 id="segmentation-applications">Segmentation Applications</h4> <p>After familiarizing you with a main topics of interest in the field, let me go directly to image segmentation.</p> <hr/> <ul> <li>Process of partitionig an image into distinct regions</li> <li>Why do we need segmentation in computer vision or medical imaging</li> </ul> <hr/> <p>So topic which receives the most attention by researchers, the topic which used the most and contribute the most, and different applications in the field.</p> <p>What definition for segmentation is basically personalization of the image into meaninful regions. And if you think about it, such personalization is extremely valuable, not only in medical imaging. Everytime when we analyze image, when we try to understand what is the depicted in the image, we perform segmentation. At least we can perform segmentation at least what is depicted.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6UVfnkP.png" alt="image-20230925192552726" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 7. Segmentation </div> <p>Let’s say here is an image of a dog. To understand this is a dog, what we can actually apply an algorithm can develop analogy which will segment which will basically contour the dog on even images, and <u>this contouring can be used to actually find out where the object is located</u>, what is in the background and so on.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/TFxtpS1.png" alt="image-20230925203509741" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 8. Self-driving </div> <p>Another application a more close to industry application is <u>self-driving cars</u>. These days like self-driving cars, is really flying there and not much time will pass before the self-driving cars will become the main way of transportation. And the way the car, the algorithm inside the car understands where to turn and how to interact. It usually requires <u>segmentation</u>, the algorithm needs to know <u>what happens around it</u>. Where are other cars? Where are the traffic lights? Where is the pedestrians? Where obstacles like shops or buildings. To do this, we need <u>segmentation</u>. We need to take an image around us and partition into meaningful regions in order to understand what happens around us.</p> <h5 id="automation-of-manual-work">Automation of manual work</h5> <p>One of the ways we can use segmentation is for <strong>automation of manual work</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/ZYZB2Mz.png" alt="image-20230925203703772" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 9. Automation of manual work </div> <p>In medical image, it is sometimes needed to segment organs to understand somethimg about human anatomy. And if you can see that many different areas, this is one area where this is done the constantly old time and takes a lot of time over for human. And this area is <u>radiation therapy planning</u>.</p> <p><u>Why the segmentation is needed there?</u></p> <p>Here is an example of two radiant therapist, one is having neuron therapy and one is liver radiant therapy. What we do when we deliver radiation to patient? What we do is we don’t only radiate tumor but actually radiation <u>does not differentiate between tumor and healthy structures</u>.</p> <p>So if we shoot radiation beam, it will pass through the human body and it will radiate everything on its way. <u>How can we be sure that we kill the tumors selves but actually do not kill or minimally damage the surrounding issues?</u></p> <p>The way to do it is actually to <u>use several beams to shoot from different directions</u>. In such way that <u>all of these beams face centered to the tumor</u>, so that radiation delivered to surrounding structures is somehow really distributed. We <u>have high dose delivered to the tumor</u> and <u>not such a high dose delivered to all surrounding structures</u>.</p> <p>When we do this, we still want to be sure that <u>no critical structure is damaged</u>. This is something very difficult to achieve. Let’s say a tumor is located very close to some critical structures like in the brain or close to spinal cord.</p> <p>Does it mean that we can simply position beams equidistantly around the human body and shoot as long as we deliver enough dose to the tumor, do not care how much dose is distributed surrounding structures?</p> <p>Well actually, that’s not true. <u>It turns out that different human organs have different radiation doses activity</u>. Let’s say if we irritate spinal cord, it’s actually way worse in contrast to irrigating the same volume of the ear. All of these relationships is divided of different organs has been carefully evaluated during the last couple of decades by oncologists and they established a set of rules saying that 30% of liver who received those below certian level or tge maximum dose delivered to spinal cord should be below such and such level.</p> <p>But how we can be sure that this constraints are satisfied?</p> <p><u>The way to do it is to segment all organs which are at risk to receive a dose.</u> In the image, segment tumor and plan the treatment before I actually shooting the doses plan treatment and see, what is the volume? How much dose each organ receives? Have the segmentation, we can measure what is the volume in certain millimeters of inhibitors of what is the volume? What is the dose each organ receives and this segmentation of organs is performed continously in the medical field is doing every day now and it takes hours to do this. And many companies like Philips, they focus on developing solutions for <strong><u>segmentation of organs for radiation therapy planing</u></strong>.</p> <h5 id="computer-aided-diagnosis">Computer-aided diagnosis</h5> <p>Another area of interest is computer-aided diagnosis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pojc5mU.png" alt="image-20230925213407477" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 10. Heart segmentation </div> <p>In before, the radiation therapy is treament planning. Here, we don’t know if patient is sick or not, but we can use segmentation to do that. We can use segmentation to estimate if the patient is sick. For example, a cardiacally enlargement of heart can be detected on X-ray images. To do that, what they do is they matter these two lines like basically you can see in the center of an image. A measure the length of this, the height of a heart and the width of the heart in X-ray and from this measurements. They can say if a patient has a normal heart. <u>However, instead of measuring this two lines, it's way better and it might even be more precise and useful to actually segment the whole heart and estimate this volume extremely from segmentation instead of measuring it's height and width</u>. And here, we need segmentation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/FJH1ETW.png" alt="image-20230925213407477" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 11. Computer-aided diagnosis </div> <p><u>Another example is from detection of a vertebra block fractures</u>. When we age, our bones weaken and some vertebra these can collapse. And in some cases, we need to perform a treatment, we need to maybe strengthen the vertebra. If the patient is going to develop very serious complication affect the vertebra about to break or it’s too weak.</p> <p>The question is how do we estimate the state of the vertebra?</p> <p>What the doctors do is they take 3D image of a patient and they measure some metrics, they measure height of vertebral points at the front area in the middle and at the back. And there are tables which says what should be the height and what should be the propulsion of these heights respect to each other for healthy vertebra and for pathological vertebra.</p> <p><u>Instead of measuring these distances manually, what we can do is we can apply an algorithm for measuring them automatically.</u> And such algorithms actually perform better than human because instead of taking one point in the image, they can segment the whole vertebra. And measure the height of the level of compression more comprehensively, not just from one distance.</p> <p>Another application of computer aided diagnosis is, for example, <u>writing an automated reports</u>. Let’s say we have a lung field image. And the doctor or maybe a machine annotated pathologist on this image, for example, annotated regions which are suspicious to be cancer or some other opacities or pneumatics or pneumonia or covid. And they would like to generate an automated report from this, what will may need is actually segmentation of lungs in order to say where exactly the pathology is located. From segmentation, we can say that, the pathology is located at the lower part of our right lock or at the upper lobe of a left clunk. And for this, we need segmentation.</p> <h5 id="image-guided-procedures">Image-guided procedures</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YfisbTL.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 12. Image-guided procedures </div> <p>Here is the screenshot from the software from <u>planning dental implants</u>.</p> <p>When a patient comes for the dental implant, it’s especially if implant requires insertion of screws into paitent’s jaw and the 3D image of patient jaw is required. And my doctor, when he sees how a patient’s jaw looks like, and he sees the place for the implant to be inserted.</p> <p>He faces a couple of challenges, one of them is he needs to design an implant wihch will fit patients jaw perfectly. One way to do is, to have a collection of teeth, basically, image collection of different teeth. And take an average teeth from population, average shape for a specific tooth from population, and put it artificially into the jaw, adjust the shape of his tooth towards to the patient. Unfortunately this procedure is very lengthy because average tooth will never fit very well to a specific patient. We have very different tooth.</p> <p>The way better strategy is to <u>use the patients anatomy to generate an implant</u>. What we can do is we can use a <u>symmetry of patient's jaw</u>. It is known that a specific tooth will be very similar to the same tooth on the other side of the jaw. So what we can do is we can apply segmentation algorithm, extract the same tooth from other parts of the jaw. And then we can reflect it and position, then start to adjust. This usually require way of less time.</p> <p>Another thing which should take into account is when we insert screws into the jaw, we should not injure the nerves. Otherwise, the patient will have serious complications, or his muscles in jaw get paralyzed. And here we can do segmentation to be sure to understand how this nerve is going.</p> <h5 id="motion-analysis">Motion analysis</h5> <p>Another area of significant interest is motion analysis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/TkAscm9.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 13. Motion analysis </div> <p>It is often the case then we image some moving structures like heart or lungs, we would like to know how exactly they move. We don’t only want their static appearance but also dynamic appearance. And there are image modalities which allows us to do that. For example, we can do that with MR images which captures the motion.</p> <p>ANd when we acquire such an image, it might do very beneficial to understand the anatomy, to understand the move patterns, and with the segmentation for that. For example, here on the right there is segmentation of heart, then we use heart segmentation to model how blood travels inside of a heart so that to understand if there is any problem of heart. If any vessel is weakened, and then we need segmentation to do that.</p> <h4 id="segmentation-methods">Segmentation Methods</h4> <p>There are already a lot different versions of segmentation methods, and segmentation has been developed for many many years now. The common and best way to separate the methods into two categories is,</p> <ul> <li>Supervised -&gt; Annotated Database</li> <li>Unsupervised -&gt; Using common sense without use annotation manually</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/RT3wiJ2.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 14. Segment Inspiration </div> <h5 id="thresholding">Thresholding</h5> <p>This is some technique involved in the course <a href="https://liuying-1.github.io/blog/2023/histogram"><em>Signal and Image Processing</em></a>, check it in detail to help recall.</p> <ul> <li>Histogram of intensity distribution</li> <li>One or multiple thresholds are used for classifying image pixels into sub partitions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/C7h6mPg.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 15. Thresholding </div> <p><strong>Thresholding: gaussian mixture model</strong></p> <ul> <li>Usually, you need to know in advance how many gaussians you need</li> <li>A good initial guess can help significantly</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Dv1bS5L.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/QHBgmXz.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 16. Intensity Histogram and original image </div> <p><strong>Gaussian Mixture Model (GMM)</strong></p> <p>A Gaussian Mixture Model is a probabilistic model that represents <u>a mixture of multiple Gaussian (normal) distributions.</u> In image segmentation, GMM is used to <u>model the distribution of pixel intensities in an image</u>. <u>Instead of assuming a single global threshold</u> like in simple thresholding, GMM assumes that <u>pixel intensities</u> in an image <u>come from a mixture of several Gaussian distributions</u>.</p> <p><strong>Why use GMM?</strong></p> <ul> <li><strong>Complex Distributions</strong>: GMM is <u>more robust</u> when dealing with complex, <u>multi-modal intensity distributions</u> in an image. It can <u>model situations where the pixel intensities don't follow a simple bimodal distribution</u>.</li> <li><strong>Flexibility</strong>: It allows for a more flexible and probabilistic approach to modeling image data, which can be particularly useful when objects in an image have varying intensity characteristics.</li> <li><strong>Adaptive Thresholding</strong>: GMM can adapt to local variations in intensity, making it useful for images with non-uniform lighting conditions.</li> <li><strong>Soft Segmentation</strong>: GMM can <u>provide soft (probabilistic) segmentations</u>, where each pixel is assigned a probability of belonging to different classes. This can be useful in cases where hard segmentation may be ambiguous.</li> </ul> <p>In summary, Gaussian Mixture Models are used when the intensity distribution in an image is not well-suited for a <u>simple global thresholding approach</u>. GMM provides a more flexible and probabilistic way to model the pixel intensity distribution, which can lead to better segmentation results, especially in cases with complex or non-uniform intensity distributions. However, GMM is <u>computationally more expensive</u> than thresholding, and the choice between these techniques depends on the specific characteristics of the image and the requirements of the segmentation task.</p> <ul> <li>Expectation Maximization (EM) algorithm</li> <li>Iterative improvement of log-likelihood function</li> </ul> <p>The Expectation-Maximization (EM) algorithm is <u>a statistical iterative optimization technique used for finding maximum likelihood estimates of parameters in probabilistic models</u>, especially when dealing with incomplete or missing data. The EM algorithm consists of <u>two main steps</u>, the <u>Expectation</u> (E-step) and the <u>Maximization</u> (M-step), which are <u>iteratively performed until convergence</u>. Here’s an overview of how the EM algorithm works:</p> <ol> <li><strong>Initialization</strong>: Start with an <u>initial guess</u> for the parameters of the model.</li> <li><strong>Expectation (E-step)</strong>: <ul> <li>Compute <u>the expected value (expectation) of the log-likelihood of the data</u> with respect to the <u>current estimate of the parameters</u>. This step involves estimating the values of hidden or unobserved variables, given the observed data and the current parameter estimates. It is called the “E-step” because it computes the expected values of these hidden variables.</li> <li>These expected values are also called the “posterior probabilities” or “responsibilities” and represent the probability that each data point belongs to a particular component of the mixture model.</li> </ul> </li> <li><strong>Maximization (M-step)</strong>: <ul> <li>Update the model parameters to maximize the expected log-likelihood computed in the E-step. This involves <u>finding new parameter estimates</u> that make the observed data more likely given the estimated hidden variables.</li> <li>In many cases, this step involves solving optimization problems to find the best-fitting parameters.</li> </ul> </li> <li><strong>Iteration</strong>: <ul> <li>Repeat the E-step and M-step iteratively until the algorithm converges. Convergence typically occurs when the change in the estimated parameters between iterations becomes small.</li> </ul> </li> <li><strong>Final Estimates</strong>: <ul> <li>Once the EM algorithm converges, the final estimates of the model parameters are obtained.</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DmxDXO0.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/P7i535h.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 17. EM algorithm and Resulting segmentation </div> <h5 id="region-growing">Region growing</h5> <p>What we do with thresholding? We actually do not take into account the neighboring pixels. So the region growing is based on that if two pieces are neighboring, two neighboring pixels <u>have similar intensities</u>, they probably belong to the same object.</p> <p>And what <u>region growing</u> does is <u>it's simply adds pixels one by one and merge together in order to get homogeneous regions</u>. Despite being very simple, it can be very useful and relatively accruate in some specific situations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DoWc2Az.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 18. Region growing Lung Fields </div> <p>Let’s see we have an example of this lung fields. We can try to <u>have some seeds</u>, see the regions which are marked in green and <u>slowly add pieces of the pixel surrounding lungs</u> and <u>add into the mask fluxes</u>.</p> <p><strong>Neighboring Pixels</strong></p> <hr/> <ul> <li>You will <u>need seed pixels</u> that are known to belong to the object (objects)</li> <li>Add <u>neighboring pixels</u> $p_t$ as long as $P(p_{t-1}, p_t)=1$</li> <li><u>Iterate</u> until no further pixels could be added</li> <li><u>Move to new seed pixels</u> if there is any</li> </ul> <hr/> <p>The basic idea of region growing is connectivity. In 2D, there are two types of connectivity,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/NDoHGoW.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 19. Neighboring Pixel </div> <ul> <li> <p>4-connectivity</p> </li> <li> <p>8-connectivity</p> </li> </ul> <p>We can extend the connectivity into this <u>distance-based connectivity</u>, you can see in the same way that pixels are connected <u>if a distance between them is less than 2 millimeters</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/sxsnJ2U.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 20. Distance-based connectivity </div> <p>In this way, we will <u>add more pixels</u> into our neighborhood.</p> <p>The original region growing is extremely simple. We have <u>seed points</u>, and we have a rule which says that <u>if two neighboring pixels have distance less than $x$</u>. They belong to the same class. And what we do is we <u>slowly iteratively expand our seed pixels</u> until we cannot expand any longer.</p> <p>Let’s look at this toy example.</p> <ul> <li>$P(x, y) = \mid x-y\mid \leq 3$</li> <li>We do not need to have both seeds; the result will be the same</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UBP64xH.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 21. Toy Example </div> <p>In the first, we have a pixel $1$ with intensity $1$, marked as dark gray. And pixel $7$ marked as light gray. And the expansion rule is that if neighboring pixels have distance less than $3$, then we connected together.</p> <p>In the first iteration we add all neighbors to pixel $7$ because the distance betweem them is less than $3$, and we also add some numbers to pixel $1$. And we can just continuous process until we cannot add any more pixels and we get a pretty logical separation of darker object at the left of our image and brighter object at the right of our image.</p> <p><u>Region growing works good only in very specific circumstances</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/AnDvxYO.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 22. Decent region growing </div> <p>For example, segmentation of lungs. You can see that when we give some seeds inside lungs, and we apply region growing and the results are pretty decent.</p> <p>Of course, <u>region growing is imperfect</u>. It can <u>not work well for complex anatomy</u>.</p> <p>For example, here is an attempt of segmenting lung cells and vertebra by using region growing. Due to its similicity alogirthms, it cannot usually produces somooth anatomically relevant borders. So you can see <u>it suffer from leaks</u>, and it also has a <u>problem of noise</u>. If there is a noisy pixel, it cannot penetrate it.</p> <hr/> <p>Problems:</p> <ul> <li>Leaks</li> <li>Noise</li> </ul> <hr/> <p><strong>Split/merge</strong></p> <p>Another way of doing region growing is approach it from different direction instead of adding pixels to our seeds.</p> <p>What we can try to do is we can <u>separate image into pieces slowly into most dissimilar pieces</u>.</p> <p>For example, we label our whole image as one object. And the first iteration, what we try to do is <u>separating this image into four squares</u>. And we see <u>how similar with this average intensity individual squares.</u> If average intensity is <u>similar, we keep the squares to be the same object</u>. But if is <u>too different, we separate them into pieces</u>. And then we go into each of the squares, and try to <u>separate it further</u>. And we <u>continue until we cannot separate any longer</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pFz9xlY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 23. Quadtree </div> <hr/> <ul> <li>First, the whole image is one initial region</li> <li>In each iteration: <ul> <li>Merge neighboring regions that are too similar $P(R, \hat) = 1$</li> <li>Split regions that are too different $P(R,\hat{R})=0$</li> </ul> </li> <li>Stop when no split or merge can be performed</li> </ul> <p>$P(R, \hat{R})$ can be based on the standard deviation of the pixel intensities.</p> <hr/> <p>For the reason of simplicity split/merge happens according to the image quadtree:</p> <ul> <li>For pixels in a $2\times 2$ square belong to a split node of a higher level</li> <li>For squares belong to a split node of a higher level etc.</li> </ul> <p>In such a way, we can get a three potential separations at three of different connected and disconected regions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/x78VJvd.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 24. Split and Merge Example </div> <p>Here is an example of how this algorithm work.</p> <p>On the left is the result of applying of starting from the labeling the whole image to be one object. And then starting separating the image into squares, and connected each other. However, the problem with such idea is that <u>once we separated to rectangle regions, they cannot anymore merge together.</u> And it might be needed, for example, our rectangle boxes, they never pass perfectly at the border of different structures and it might be needed to go back into some levels and merge some smaller regions together.</p> <p><u>We call it split and merge region growing where we not only separate them into smaller pieces but we can merge them back some of the smaller pieces back if needed.</u></p> <p>And you can see that a separation with split and merge region growing is better than the one with only split region growing.</p> <p><strong><em>You do not have to use thresholding or region growing on raw image intensities.</em></strong></p> <p>The thing about the region growing and thresholding is that these days we don’t use them on raw image intensities because organs are much more complicated than just regions of homogeneous region of specific intensity. But what we can do is we can apply a more sophisticated algorithms, for example, deep learning to enhance organs which we are interested about.</p> <p>For example, here’s is the enhancement of kidney. And then when we enhance the kidney, we can apply thresholding. To analyse this enhanced image, it’s very easier to separate the background from object on the enhanced image instead of separating the kidney on the raw image.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/85DDZnK.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 25. Auxiliary method </div> <p>And this is how thresholding is currently used. <u>It's more used to analyse results of more sophisticated algorithms as an auxiliary method to analyse this.</u></p> <h5 id="other-similar-segmentation-methods">Other similar segmentation methods</h5> <ul> <li>Watersheds</li> <li>Level sets</li> <li>K-means</li> <li>Various thresholding (Otsu, Huang, etc)</li> </ul> <h4 id="example">Example</h4> <p>Let’s have a problem of lung fields segmentation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/EuGOPR4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/W4rM9tt.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 26. Lungs Segmentation </div> <p>Let’s say we have a 3D image of lung fields of a human body and our aim is to segment lung fields from this image. And the lung fields are darker than other structures around. Considering this fact, what we can do is we can use <u>thresholding to separate the lungs</u>. So we can take a histogram and use this histogram to separate <u>according to the first valley which separates air from soft tissues</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/MdVwUIk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/RuvoEre.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 27. Resulting image of first valley </div> <p>And here is what we get, it’s perfect but it’s already somehow decent trade. Let’s see if we can improve it by using some simple logical ideas which come down in mind.</p> <p>So here is an illustration of 3D render of our segmentation. Although it looked very nice on the slice we saw on a cross section, we’ve tried to visualize the threshold of mask in 3D, what we see is that apart from the lung fields which are full with air, we also segment that the air outside the human body which doesn’t belong to human body.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/NzGLYB0.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 28. 3D illustration </div> <p>So the first thing we need to think about is how to remove this air outside the human body?</p> <h4 id="must-have-tools">Must-have tools</h4> <h5 id="connected-component-decomposition">Connected component decomposition</h5> <p>And the way to do is by using a tool which is called <u>connected component decomposition</u>.</p> <p>So if you look again to our example fo segmentation of lung fields, what you can see is that the air outside the human body is a very large connected object. So when all areas connected to each other always the air outside and its size is quite significant. And at the same time, lungs are also significant size. And when you look more closely into the image, you find out that not only the air outside the lung fields a human body is also segmentated by thresholding but also air in the stomach is selected. Also air around some image artifacts is also segmented.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pv7Nlcw.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 29. 3D illustration with useless air </div> <p>And the way to solve it is to use anatomical knowledge which we have prior to solving, to addressing this problem. And what we know is that the air outside with very large volume. And the air outside which was supposed to the largest which we will see in the image. Then, the next largest object is lungs. So for lungs, as the second largest acquistion of air inside of a human body. So what we can do is to find the object which is the second largest and hopefully this object will be lung fields.</p> <p>To do that, we need an algorithm which is called <u>connected component decomposition</u>.</p> <p>What the algorithm tries to do is tries to connect and label the same label all pixels in the mask of we have valued $1$ and connect with which are connected to each other using 8 or 4 connectivity in 2D or 27 or 6 connectivity in 3D.</p> <hr/> <p>Separating segmentation results into regions:</p> <ul> <li>Removes noise from segmentation</li> <li>Separates segmentation into regions that can be further analysed</li> </ul> <p>We have a binary image (0-background)</p> <p>Connected component:</p> <ul> <li>All pixels have intensity value 1</li> <li>There is a path between any two pixels using 4-connectivity or 8-connectivity</li> <li>No pixel can be added</li> </ul> <hr/> <p>Here is a little bit of algorithmical depth of connected component decomposition.</p> <p>It will be skipped as it is not listed in the learning outcomes.</p> <h5 id="morphological-dilationerosion">Morphological dilation/erosion</h5> <p>The results probably not that satisfied because the lung fields are not only the air. The lungs also contain vessels inside of them. And these vessels will not be segmented by thresholding. However, we still would likt to add these vessels into our mask, then the segmentation will be complete.</p> <p>And the problem is that in some cases, especially in X-ray images, the air in the stomach comes very close to lung fields and sometimes it may come be very connected to lung fields and our lung fields mask can sometimes leak inside of a stomach. And we don’t want these to happen.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/R9YtBEe.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 30. Problems </div> <p>So here is an example of the vessels inside of lung fields, and also example of artificial leak. We don’t want them to be present in our mask. And the way to do it is to apply mathematical operations.</p> <p>There are two types of operations, one is morphological <strong>erosion</strong> and the other one is morphological <strong>dilation</strong>.</p> <p>So the erosion is <u>removes one layer of pixels from our mask</u>. So <u>our masks shrinks one layer inside</u>. So <u>we cut one layer from our mask, for every pixel which has at least one neighbour which is not mask will remove this pixel.</u></p> <p>For dilation is opposite, what it does is, it adds a few layer of pixels into our mask. So for every known mask pixel, which has a mask pixel in our original mask for every such pixel, we include this pixel into our mask.</p> <p>What happens when we apply the erosion and dilation, we don’t return to the original mask.</p> <hr/> <p>Informally:</p> <ul> <li>Dilation - expanding of binary mask</li> <li>Erosion - shrinking of binary mask</li> </ul> <p>Dilation/erosion is useful:</p> <ul> <li>To remove all internal noise pixels in segmentation mask</li> <li>To remove all boundary artifacts</li> <li>To smooth the boundaries</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/cnTll6C.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 31. Explaination </div> <hr/> <p><strong>Algorithms</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/2R64uYF.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 32. Erosion </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DsRTkbD.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 32. Dilation </div> <p>So, let’s look at this example.</p> <p>We have an original mask which looks like this.There is a hole inside, value $0$. Let’s see what will happen if we apply the dilation around this mask.</p> <p>All pixels which had value $0$ in our original image $I$, all pixels which had value $0$ and at least one neighbor which has 1 in the 4 neighborhood, they all turn into $1$ into our new mask. =&gt; Expand our original mask a little bit.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/jFbdP0K.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 33. Dilation -&gt; Erosion </div> <p>So for every pixel which has a label 1 and at least one pixel is neighborhood is 0, and also consider the image border to have 0 labels. We turn this pixel into 0. If it has one neighborhood which is 0 and then we turn it into 0. So all these pixels at the border may turn into 0 because we have at least on neighborhood which has 0. But the pixel which it didn’t turn back into 0 is the one which is in the middle. It was 0 in the very begining, but after the dilation, all its neighbors now are 1. So it cannot anymore turn into 0.</p> <p><u>So what we get is after applying dilation over, we've got almost the same mask except one pixel in the middle, except for one noisy pixel in the middle, which we turn into one by these operation.</u></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/LUP1lR5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 34. Erosion -&gt; Dilation </div> <p>Let’s see what happens if we look at the opposite situation.</p> <p>All the pixels which have at least one pixel with label 0 as neighbor will be removed, including this guy who is close to border. After this, we will only have two pixels left as label 1. Then, we play dilation. The result shows that compared to the original one, something bottom is not needed for us will be removed.</p> <p>In some situations, we also lost some pixels and for such a small mask, it’s a significant loss. But you can imagine in our real life, our mask is very larger. So losing only some pixels at the borders will not be that critical in comparison to removing the noise and the comparison to remove the leaks which we don’t want to have.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vIo7Jsn.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 35. Result of dilation -&gt; erosion </div> <p>5 times dilation and then 5 times erosion. =&gt; we add all the vessels into our lung field mask, and then the borders of our mask get smoothed. Some inhomogeneity of border is disappeared.</p> <p><u>So we not only included the vessels into our mask, which we wanted the beginning, but we also get an opportunity to remove noisy at the borders.</u></p> <p>This note will be fixed in soon once I am not that busy.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Learning note for medical image segmentation.]]></summary></entry><entry><title type="html">Inpainting</title><link href="https://liuying-1.github.io/blog/2023/inpainting/" rel="alternate" type="text/html" title="Inpainting"/><published>2023-09-19T08:00:00+00:00</published><updated>2023-09-19T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/inpainting</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/inpainting/"><![CDATA[<p>Inpainting, as known as <strong>predict missing pixels</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/KLK2ZIN.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Context Encoders: Feature Learning by Inpainting (Pathak et al., 2016) </div> <p><strong><em>Disclaimer: Below are the extraction from the original paper.</em></strong> It will be reposted once I finished read all of them when I am available.</p> <p><strong><em>Reference: Context Encoders: Feature Learning by Inpainting</em></strong></p> <hr/> <h4 id="abstraction">Abstraction</h4> <p>We present <strong><u>an unsupervised visual feature learning algorithm</u></strong> driven by context-based pixel prediction.</p> <p>Context Encoders - a <strong>convolutional neural network</strong> trained to <strong>generate the contents of an arbitrary image region conditioned on its surroundings</strong>.</p> <p>To succeed at this task, context encoders need to both <strong>understand the content of the entire image,</strong> as well as <strong>produce a plausible hypothesis for the missing parts</strong>.</p> <p>When training context encoders, we have experimented with both <strong>a standard pixel-wise reconstruction loss</strong>, as well as <strong>a reconstruction plus an adversarial loss</strong>.</p> <p>The <strong>latter produces much sharper results</strong> because it can <strong>better handle multiple modes</strong> in the output.</p> <p>A context encoder learns <strong>a representation that captures not just appearance</strong> but also the <strong>semantics of visual structures</strong>. ==&gt; <em>Question</em></p> <p>Context encoders can be used for <strong>semantic inpainting tasks</strong>, either <strong>stand-alone</strong> or <strong>as initialization for non-parametric methods</strong>.</p> <h4 id="introduction">Introduction</h4> <p>Our visual world is very diverse, yet <strong>highly structured</strong>, and humans have an uncanny ability to make sense of this structure.</p> <p>This work is to explore <strong>whether state-of-the-art computer vision algorithms can do the sam</strong>e.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/nFaEBX3.png" alt="image-20230919205428308" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Demonstration </div> <p>Although the center part of the image is missing, most of us can easily imaging its content from the surrounding pixels, without having ever seen that exact scene. Some of us can even draw it.</p> <p>This ability comes from the fact that <strong>natural images, despite their diversity, are high structured</strong>. In this case, the regular pattern of windows on the facade. We <strong>humans</strong> are able to <strong>understand this structure</strong> and <strong>make visual predictions</strong> even when seeing <strong>only parts of the scene</strong>.</p> <p>In this paper, we show that <strong>it is possible to learn and predict this structure</strong> using <strong>convolutional neural networks</strong> (CNNs), a class of models that have recently shown success across a variety of <strong>image understanding tasks</strong>.</p> <p>Given <strong>an image with a missing region</strong>, we train a convolutional neural network to <strong>regress to the missing pixel values</strong>. <em>Question</em></p> <p>We call our model <strong>context encoder</strong>, as it consists of <strong>an encoder capturing the context of an image into a compact latent feature representation</strong> and <strong>a decoder which uses that representation to produce the missing image content</strong>.</p> <p>The context encoder is closely related to auto encoders, as it shares a <strong>similar encoder-decoder architecture</strong>.</p> <p><strong>Autoencoders</strong> <strong>take an input image</strong> and try to <strong>reconstruct it</strong> after it passes through a low-dimensional “bottleneck” layer, with the aim of <strong>obtaining a compact feature representation</strong> is likely to <strong>just compresses the image content without learning a semantically meaningful representation</strong>. ==&gt; Encoder has done the abovementioned, the decoder is to map back to the original datashape.</p> <hr/> <p>ChatGPT gives the following explaination to autoencoder.</p> <ol> <li>“<strong>Autoencoders</strong>”: Autoencoders are a class of neural network architectures used for unsupervised learning. They consist of an encoder and a decoder, and their <u>primary purpose is to learn a compressed representation of the input data</u>.</li> <li>“<strong>Take an input image</strong>”: Autoencoders typically operate on input data, which in this case is an image. The image is passed through the network.</li> <li>“<strong>Reconstruct it</strong>”: The <u>core task of an autoencoder is to take the input image</u> and attempt to <strong><u>generate an output (reconstruction) that is as close as possible to the original input</u></strong>. <strong><u>In essence, it tries to replicate the input</u></strong>.</li> <li>“<strong>After it passes through a low-dimensional ‘bottleneck’ layer</strong>”: Autoencoders have an intermediate layer called the bottleneck or latent space, which has a lower dimension than the input data. This layer forces the network to <strong><u>learn a compressed representation of the input</u></strong> data.</li> <li>“<strong>With the aim of obtaining a compact feature representation</strong>”: The primary objective of this compression process is to <strong><u>obtain a compact feature representation of the input</u></strong> data. This means that the autoencoder is trying to <u>capture the most important and salient features of the input</u> data in a condensed form.</li> <li>“<strong>Is likely to just compresses the image content without learning a semantically meaningful representation</strong>”: This part of the statement suggests <u>a potential limitation of traditional autoencoders</u>. In some cases, when using basic autoencoder architectures and simple loss functions like mean squared error, the learned compressed representation may indeed <strong><u>focus more on compressing the raw image content</u></strong> (e.g., pixel values) rather than <strong><u>capturing higher-level, semantically meaningful features</u></strong> (e.g., object shapes or meaningful abstractions).</li> </ol> <p>In other words, <strong>traditional autoencoders</strong> may prioritize achieving a low reconstruction error (<strong><u>faithfully reproducing the input</u></strong>) over learning abstract, semantic information. However, this limitation can be addressed or mitigated by modifying the architecture, loss functions, or training strategies, as mentioned in the previous response. <strong><u>More advanced autoencoder</u></strong> variants, like variational autoencoders (VAEs) or denoising autoencoders, <strong><u>are designed to encourage the learning of semantically meaningful representations</u></strong>.</p> <hr/> <p><strong>Denoising autoencoders</strong> address this issue by <strong>corrupting the input image</strong> and requirng the network to <strong>undo</strong> the damage. However, this corruption process is typically very <strong>localized and low-level</strong>, and does <strong>not require much semantic information</strong> to undo.</p> <hr/> <p>ChatGPT explains what are localized and low-level. :point_down:</p> <ol> <li>“<strong>Localized</strong>”: When it is said that the corruption process is “localized,” it means that the noise or distortion introduced to the input data <strong>tends to affect only a small portion of the data</strong>. In the context of images, for example, localized corruption might involve adding noise to individual pixels or small regions of the image. It does not involve large-scale changes or alterations to the entire image.</li> <li>“<strong>Low-level</strong>”: The term “low-level” refers to the nature of the corruption. Low-level corruptions are simple and typically involve basic operations on the data, such as adding random noise, blurring, or altering individual data points (e.g., pixel values in an image). These corruptions do not carry complex semantic meanings or involve high-level abstractions. They are often applied at the level of raw data elements.</li> </ol> <p>In the context of denoising autoencoders, the idea is that <strong>the noise or distortions introduced to the input</strong> data are <strong>relatively simple</strong> and <strong>affect only small parts</strong> of the data. Because these <strong>corruptions are not complex or semantically rich</strong>, the network may <strong>not need to learn deep</strong>, <strong>high-level semantics to undo</strong> the effects of the corruption. Instead, it can focus on relatively shallow and low-level operations to recover the clean data. This is why the statement suggests that denoising autoencoders may not necessarily learn highly meaningful semantic representations since the corruption process itself is not very demanding in terms of semantic understanding.</p> <hr/> <p>In contrast, our context encoder needs to solve <strong><u>a much harder task</u></strong>: <strong><u>to fill in large missing areas of the image, where it can't get "hints" from nearby pixels.</u></strong></p> <p>This requires <strong>a much deeper semantic understanding of the scene</strong>, and the <strong>ability to synthesize high-level features over large spatial extents</strong>.</p> <p>For example, an entire window needs to be <strong>conjured up “out of thin air”</strong> in Figure 2 above.</p> <p>This is similar in spirit to wrd2vec which <strong><u>learns word representation from natural language sentences by predicting a word given its context</u></strong>.</p> <p>Like autoencoders, <strong><u>context encoders</u></strong> are <strong><u>trained in a completely unsupervised manner</u></strong>.</p> <p>Our results demonstrate that in order to succeed at this task, a model needs to <strong><u>both understand the content of an image</u></strong>, as well as <strong><u>produce a plausible hypothesis for the missing parts</u></strong>.</p> <p>This task, however, is inherently <strong>multi-modal</strong> as there are <strong>multiple ways</strong> to <strong>fill the missing region while also maintaining coherence with the given context</strong>.</p> <p>We <strong>decouple this burden</strong> in our loss function by <strong>jointly training our context encoders</strong> to <strong>minimize both a reconstruction loss and adversarial loss</strong>.</p> <p>The reconstruction loss (L2) <strong>captures the overall structure</strong> of the missing region in relation to the context, while the adversarial loss has the effect of <strong>picking a particular mode</strong> from the distribution.</p> <ul> <li>“<strong>Reconstruction loss (L2)</strong>”: This loss function measures how well the generated output (the completed or inpainted region) matches the original data. It’s often calculated as the mean squared difference between the generated output and the ground truth (original data). In other words, it captures how well the generated content resembles the actual missing region in terms of overall structure.</li> <li>“<strong>Adversarial loss</strong>”: This type of loss function is typically associated with GANs. It involves a discriminator network that tries to distinguish between real (ground truth) data and generated data. The adversarial loss <strong>encourages the generated data to be more realistic</strong> and closer to the distribution of real data. In this context, it is mentioned that the adversarial loss has the effect of selecting a particular mode from the distribution, which means <strong>it encourages the generated content to be more similar to specific instances of real data</strong> (modes) rather than just any possible output.</li> </ul> <p>So, the overall strategy described here is to <strong>simultaneously train context encoders</strong> to <strong>generate missing regions</strong> in a way that <strong>not only captures the overall structure</strong> (reconstruction loss) but also <strong>encourages the generated content to resemble specific instances of real data</strong> (adversarial loss). This approach aims to <strong>strike a balance between maintaining coherence with the context and producing realistic</strong>, data-driven details in the inpainted or completed regions.</p> <p>Figure 2 shows that using only the <strong>reconstruction loss produces blurry results</strong>, whereas <strong>adding the adversarial loss results in much sharper predictions</strong>.</p> <p><strong><u>Evaluate the encoder and the decoder independently.</u></strong></p> <p>On the encoder, we show that <strong>encoding</strong> just the <strong>context of an image patch</strong> and using the <strong>resulting feature</strong> to <strong>retrieve nearest neighbor contexts from a dataset</strong> produces patches which are semantically similar to the original (unseen) patch.</p> <ul> <li>After encoding the context of an image patch, the authors take the <strong>resulting feature representation</strong> and <strong>use it to find other image patches in a dataset that have similar feature representations</strong>. In other words, they <strong>look for other patches that share similar contextual characteristics</strong> based on the extracted features.</li> <li>The key result or observation here is that when they retrieve image patches from the dataset based on the feature representation of the context, the <strong>retrieved patches are found to be “semantically similar” to the original, unseen patch</strong>. This means that <strong>the retrieved patches share meaningful visual or structural characteristics with the original patch</strong>, even though the encoder processed only the context of the patch.</li> </ul> <p><strong>By encoding only the context of an image patch</strong> and using the <strong>resulting features to find similar contexts in a dataset</strong>, they can <strong>retrieve other patches that semantically similar to the original patch</strong>. This suggests that the context encoder is <strong>effective at capturing meaningful contextual information from images</strong>, even without considering the patch itself.</p> <p>We further validate the quality of the learned feature representation by fine-tuning the encoder for a variety of image understanding tasks, including classfication, object detection, and semantic segmentation.</p> <p>We are competitive with the state-of-the-art unsupervised/self-supervised methods on those tasks.</p> <p>On the decoder side, we <strong>show that our method is often able to fill in realistic image content</strong>.</p> <p>Indeed, to the best of our knowledge, ours is the <strong>first parametric inpainting algorithm</strong> that is able to <strong>give resonable results for semantic hole-filling</strong> (large missing regions).</p> <p>The context encoder can also be useful as a better visual feature for computing nearest neighbors in non-parametric inpainting methods.</p> <h4 id="related-work">Related work</h4> <p>Computer vision has made tremendous progress on <strong>semantic image understanding tasks</strong> such as classification, object detection, and segmentation in the past decade.</p> <p>Recently, <strong>Convolutional Neural Networks (CNNs)</strong> have greatly advanced the performance in these tasks. The success of such models on image classification paved the way to tackle harder problems, including unsupervised understanding and generation of natural images.</p> <p>We briefly review the related work in each of the sub-fields pertaining to this paper.</p> <h5 id="unsupervised-learning"><strong>Unsupervised learning</strong></h5> <p><strong>CNNs trained for ImageNet classification</strong> with over a million <strong>labeled examples</strong> learn features which generalize very well across tasks. However, <strong>whether such semantically informative and generalizable features can be learned from raw images alone, without any labels</strong>, remains an open question. Some of the earliest work in deep unsupervised learning are autoencoders. Along similar lines, denoising autoencoders reconstruct the image from local corruptions, to make encoding robust to such corruptions. While context encoders <strong>could be thought of as a variant of denoising autoencoders</strong>, the corruption applied to the model’s input <strong>is spatially much larger</strong>, requiring <strong>more semantic information</strong> to undo.</p> <h5 id="weakly-supervised-and-self-supervised-learning">Weakly-supervised and self-supervised learning</h5> <p>Very recently, there has been significant interesting in learning meaningful representations using weakly-supervised and self-supervised learning. One useful source of supervision is to <strong>use the temporal information</strong> contained in videos. <strong>Consistency across temporal frames</strong> has been used as supervision to learn embeddings which perform well on a number of tasks. Another way to use consistency is to track patches in frames of video containing task-relevant attributes and use the coherence of tracked patches to guide the training.</p> <p>Most closely related to the present paper are efforts at <strong>exploiting spatial context as a source of free and plentiful supervisory signal</strong>. Recently, Doersch et al. used the task of <strong>predicting the relative positions of neighboring patches within an image</strong> as a way to train an unsupervised deep feature representations.</p> <p>Our context encoder solves <strong>a pure prediction problem</strong> (what pixel intensities should go in the hole?).</p> <p>In contrast, <strong>context encoders can be applied to any unlabeled image database</strong> and <strong>learn to generate images based on the surrounding context</strong>.</p> <p>The later part in the article is not that related to my interests. I will repost this blog as soon as possible to form my own interpretation focus on the technique, <strong>inpainting</strong>.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[Learning note for inpainting.]]></summary></entry></feed>