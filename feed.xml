<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://liuying-1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://liuying-1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-13T20:13:31+00:00</updated><id>https://liuying-1.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Graduation</title><link href="https://liuying-1.github.io/blog/2024/graduation/" rel="alternate" type="text/html" title="Graduation"/><published>2024-06-28T10:15:00+00:00</published><updated>2024-06-28T10:15:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/graduation</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/graduation/"><![CDATA[<p>I am thrilled to announce that I have successfully defended my master thesis and received my Masterâ€™s Degree in Computer Science from the Department of Computer Science (DIKU), University of Copenhagen.</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/akAyC97.jpeg" alt="graduation-photo" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/7Ee4Raf.jpeg" alt="graduation-photo-2" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <p>The title of my thesis is â€œExploration of Self-Supervised Learning Methods for Longitudinal Image Analysisâ€, specifically focusing on the application of self-supervised learning methods, which are BYOL and SimSIAM, in medical image analysis for longitudinal studies (i.e., tumor progression) on lung cancer. The 3D ResNet model is first pre-trained on the LIDC-IDRI dataset, and then the model is tasked with predicting the tumor volume at the future timepoint of the same subject, on the Longitudinal 4D Lung dataset. The results show that the self-supervised learning methods cannot be used to train 3D ResNet to learn meaningful representations for longitudinal image analysis, and the learned representations are not significantly correlated with the actual tumor volume, verified by the linear probing task. This failure is attributed to the following reasons: 1. the volume featured by the LIDC-IDRI dataset (nodules) is with substatial differences from the Longitudinal 4D Lung dataset (tumors), 2. the current pre-processing pipeline does not isolate the lung region perfectly, 3. the model is not fine-tuned on the longitudinal downstream task, and 4. the invovled augmentations impair the modelâ€™s ability to learn precise tumor volume representations.</p> <div class="row mt-3 mb-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/16tSUKF.png" alt="2831691531627_.pic" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <p>I am grateful for the support and guidance from my supervisor, friends, and family, along the way. Thank you to everyone who has been a part of this journey with me. ğŸ“ğŸ‰</p> <p>Additionally, I am very excited to share the news that I am going to work as an AI engineer in a company, to embark on the next chapter of my life and look forward to new opportunities and challenges. ğŸš€</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="fun"/><summary type="html"><![CDATA[Congratulations to my graduation from the DIKU, University of Copenhagen.]]></summary></entry><entry><title type="html">Mordern Family E1S1</title><link href="https://liuying-1.github.io/blog/2024/modern-family-e1s1/" rel="alternate" type="text/html" title="Mordern Family E1S1"/><published>2024-06-28T10:15:00+00:00</published><updated>2024-06-28T10:15:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/modern-family-e1s1</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/modern-family-e1s1/"><![CDATA[<p>ready to go</p>]]></content><author><name></name></author><category term="study"/><category term="language"/><category term="english"/><category term="dictation"/><summary type="html"><![CDATA[This is a daily dictation 3 for English improvement.]]></summary></entry><entry><title type="html">Review of SwAV</title><link href="https://liuying-1.github.io/blog/2024/swav/" rel="alternate" type="text/html" title="Review of SwAV"/><published>2024-02-12T10:31:00+00:00</published><updated>2024-02-12T10:31:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/swav</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/swav/"><![CDATA[]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the learning of the paper SwAV.]]></summary></entry><entry><title type="html">Review of SimSiam</title><link href="https://liuying-1.github.io/blog/2024/simsiam/" rel="alternate" type="text/html" title="Review of SimSiam"/><published>2024-02-12T09:31:00+00:00</published><updated>2024-02-12T09:31:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/simsiam</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/simsiam/"><![CDATA[]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the learning of the paper SimSiam.]]></summary></entry><entry><title type="html">Extracts from BYOL</title><link href="https://liuying-1.github.io/blog/2024/byol/" rel="alternate" type="text/html" title="Extracts from BYOL"/><published>2024-02-11T08:00:00+00:00</published><updated>2024-02-11T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/byol</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/byol/"><![CDATA[<p>BYOL is the abbr. of <u>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</u>.</p> <h3 id="cookbook-for-byol">Cookbook for BYOL</h3> <p>First of all, BYOL method probably the first one removes the clustering step, introduces a predictor and projector network, fefines the continuous targets as the output of a momentum network, renormalize each sample representation by its \(l_2\)-norm and leverage positive pairs. The predictor acts as a whitening operator preventing collapse, and momentum network can be applied only to the projector.</p> <p>It first introduced self-distillation as a means to avoid collapse. It usees two networks along with a predictor to map the outputs of one network to the other. The network predicting the output is called the online or student network while the network producing the target is called the target or teacher network. Each network receives a different view of the same image formed by image transformations including random resizing, cropping, color jittering, and brightness alterations. The student network is updated throughout training using gradient descent. The teacher network is updated with an exponential moving average (EMA) updates of the weights of the online network. The slow updates induced by exponential moving average creates an asymmetry that is crucial to BYOLâ€™s success.</p> <h3 id="original-paper">Original Paper</h3> <p>Please check the original paper here <a href="https://arxiv.org/abs/2006.07733">BYOL</a>.</p> <h4 id="abstract">Abstract</h4> <ul> <li><strong>BYOL</strong>, a <strong>new</strong> apporach to <strong>self-supervised learning</strong>.</li> <li>BYOL relies on two neural networks, referred to as the <strong>online</strong> network and the <strong>target</strong> network, that <strong>interact</strong> and <strong>learn from each other</strong>.</li> <li>From <strong>an augmented view of an image</strong>, we train the <strong>online network to predict the target network representation of the same image under a different augmented view</strong>.</li> <li>At the same time, we <strong>update the target network with a slow-moving average of the online network</strong>.</li> <li>(Back then), while state-of-the-art methods <strong>rely on negative pairs</strong>, BYOL achieves a new state of the art <strong>without them</strong>.</li> </ul> <hr/> <p>Online first updates, updates of target come from online by EMA.</p> <p>Without negative pairs can be competative as well.</p> <p>Train the online to predict the target network representation.</p> <hr/> <h4 id="introduction">Introduction</h4> <ul> <li>(Back then,) state-of-the-art <strong>contrastive methods</strong> are trained by <strong>reducing the distance between representations of different augmented views of the same image (â€˜positive pairsâ€™)</strong>, and <strong>increasing the distance between representations of augmented views from different images (â€˜negative pairsâ€™)</strong>.</li> <li>These methods need <strong>careful treatment of negative pairs</strong> by either <strong>relying on large batch sizes</strong>, <strong>memory banks</strong> or <strong>customized mining strategies</strong> to retrieve the negative pairs.</li> <li>In addition, their performance <strong>critically</strong> depends on the <strong>choice of image augmentations</strong>.</li> </ul> <hr/> <p>BYOL can overcome the previous shortages from the previous methods, which are: <strong>no need negative pairs</strong>, <strong>careful treatment of negative pairs</strong>, <strong>choice of hyper-parameters</strong>, and <strong>choice of augmentations</strong>.</p> <hr/> <ul> <li>BYOL achieves higher performance than state-of-the-art contrastive methods <strong>without using negative pairs</strong>.</li> <li>It <strong>iteratively</strong> <strong>bootstraps the outputs of a network to serve as targets for an enhanced representation</strong>.</li> <li>BYOL is <strong>more robust</strong> to the <strong>choice of image augmentations</strong> than contrastive methods; we suspect that <strong>not relying on negative pairs</strong> is one of the leading reasons for its <strong>improved robustness</strong>.</li> <li>We propose to <strong>directly bootstrap the representations</strong>. ==&gt; Different from previous methods.</li> <li>In particular, <strong>BYOL uses two neural networks, referred to as online and target networks, that interatct and learn from each other.</strong></li> <li>Starting from <strong>an augmented view of an image</strong>, BYOL <strong>trains its online network</strong> to <strong>predict the target networkâ€™s representation</strong> of <strong>another augmented view</strong> of the <strong>same image</strong>.</li> <li>While this objective <strong>admits collapsed solutions</strong>, e.g., <strong>outputting the same vector for all images</strong>, we empirically show that <strong>BYOL does not converage to such solutions</strong>.</li> <li>We hypothesize that the <strong>combination</strong> of (i) the <strong>addition of a predictor to the online network</strong> and (ii) the <strong>use of a slow-moving average of the online parameters as the target network</strong> encourages <strong>encoding more and more information within the online projection</strong> and <strong>avoids collapsed solutions</strong>.</li> </ul> <hr/> <ul> <li>BYOL achieves state-of-the-art results under the <strong>linear evaluation protocol on ImageNet without using negative pairs</strong>.</li> <li>Our learned representation outperforms the state of the art on semi-supervised and transfer benchmarks.</li> <li>BYOL is <strong>more resilient</strong> to <strong>changes in the batch size</strong> and <strong>in the set of image augmentations</strong> compared to its contrastive counterparts.</li> <li>In particular, BYOL sufferes a much smaller performance drop than SimCLR, a strong contrastive basline, when only using random crops as image augmentations.</li> </ul> <h4 id="related-work">Related work</h4> <ul> <li>Most <strong>unsupervised methods for representation learning</strong> can be categorized as either <strong>generative</strong> or <strong>discriminative</strong>.</li> <li><strong>Generative approaches to representation learning</strong> build <strong>a distribution over data and latent embedding</strong> and <strong>use the learned embeddings as image representations</strong>.</li> <li>Many of these apporaches <strong>rely either on auto-encoding of images</strong> or on <strong>adversarial learning</strong>, jointly modelling data and representation.</li> <li>Generative methods typically operate <strong>directly in pixel space</strong>.</li> <li>This however is <strong>computationally expensive</strong>, and the <strong>high level of detail required for image generation</strong> may <strong>not be necessary</strong> for representation learning.</li> </ul> <hr/> <p>The <strong>intro</strong> and <strong>drawbacks</strong> of generative methods.</p> <hr/> <ul> <li>Among <strong>discriminative</strong> methods, <strong>contrastive methods</strong> currently achieves state-of-the-art performance in self-supervised learning.</li> <li><strong>Contrastive methods often</strong> require comparing each example with many other examples to work well, <strong>prompting the question of whether using negative pairs is necessary</strong>.</li> </ul> <hr/> <p><strong>Contrastive methods</strong> belongs to <strong>discriminative methods</strong>, and are best, while <strong>prompting the question of negative pairs</strong>.</p> <hr/> <ul> <li><strong>DeepCluster partially answers this question</strong>.</li> <li>While <strong>avoiding the use of negative pairs</strong>, this requires <strong>a costly clustering phase and specific precautions to avoid collapsing to trivial solutions</strong>.</li> </ul> <hr/> <p>Might <strong>not necessary</strong> for the use of <strong>negative pairs</strong> if under suitable settings.</p> <hr/> <ul> <li>Some self-supervised methods are <strong>not contrastive but rely on using auxiliary handcrafted prediction tasks</strong> to learn their representation.</li> <li>Yet, even with suitable architectures, these methods are being <strong>outperformed by contrastive methods</strong>.</li> </ul> <hr/> <p>Some other methods might <strong>better</strong> than contrastive methods.</p> <hr/> <ul> <li>Our approach <strong>has some similarities</strong> with <em>Predictions of Bootstrapped Latents</em> (PBL).</li> <li>PBL <strong>jointly trains the agentâ€™s history representation</strong> and <strong>an encoding of future observations</strong>.</li> <li><strong>Unlike PBL</strong>, <strong>BYOL uses a slow-moving average of its representation to provide its targets, and does not require a second network.</strong></li> </ul> <hr/> <p>BYOL stems from PBL but unlike PBL.</p> <hr/> <ul> <li>The <strong>idea of using a slow-moving average target network to produce stable targets</strong> for the online network was inspired by deep RL.</li> <li> <p>Target networks stabilize the bootstrapping updates provided by the Bellman equation, <strong>making them appealing to stabilise the bootstrap mechanism in BYOL</strong>.</p> </li> <li>While <strong>most RL methods use fixed target networks</strong>, BYOL uses <strong>a weighted moving average of previous networks in order to provide smoother changes in the target representation</strong>.</li> </ul> <hr/> <p>The origin of the moving weights.</p> <hr/> <ul> <li>In the semi-supervised â€¦. Among these methods, mean teacher also uses a slow-moving average network, called teacher, to produce targets for an online network, called student.</li> <li><strong>In contrast, BYOL introduces an additional predictor on top of the online network, which prevents collapse.</strong></li> </ul> <hr/> <p>The use of predictor.</p> <hr/> <ul> <li>Finally, in self-supervised learning, MoCo uses a slow-moving average network (momentum encoder) to maintain consistent representations of negative pairs drawn from a memory bank.</li> <li>Instead, <strong>BYOL uses a moving average network to produce prediction targets</strong> as a means of stabilizing the bootstrap step.</li> </ul> <h4 id="method">Method</h4> <ul> <li>Many such approaches <strong>cast the prediction problem directly in representation space</strong>: the <strong>representation of an augmented view of an image</strong> should be <strong>predictive of the representaion of another augmented view of the same image</strong>.</li> <li>However, <strong>predicting directly in represntation space</strong> can lead to <strong>collapsed representations</strong>.</li> <li>Contrastive methods circumvent this problem by reformulating the prediction problem into one of discrimination. â€¦ and the <strong>representations of augmented views of different images</strong>.</li> <li>In this work, we thus <strong>tasked ourselves to find out whether these negative examples are indispensable to prevent collapsing while preserving high performance.</strong></li> </ul> <hr/> <p>cross-view èƒ½ï¼šå›¾åƒçš„å¢å¼ºè§†å›¾çš„è¡¨å¾èƒ½é¢„æµ‹åŒä¸€å›¾çš„å¦ä¸€ä¸ªå¢å¼ºçš„è¡¨å¾ï¼Œä½†æ˜¯ä»£ä»·æ˜¯æ€»èƒ½é¢„æµ‹è‡ªå·±æ¥å¯¼è‡´ trivial solutionsï¼ˆåå¡Œï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬çš„æ–¹æ³•æ¥éªŒè¯èƒ½å¦ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œè´Ÿæ ·æœ¬çš„å­˜åœ¨æ˜¯å¦å¿…é¡»ã€‚</p> <hr/> <ul> <li>To <strong>prevent collapse, a straightforward solution is to use a fixed randomly initialized network</strong> to poduce the <strong>targets</strong> for our predictions.</li> <li>While avoidng collapse, it <strong>empircally does not result in very good representations</strong>.</li> <li>It is interesting to note that the <strong>representation obtained using this procedure</strong> can already be <strong>much better than the initial fixed represntation</strong>.</li> <li>This experimental finding is the <strong>core motivation</strong> for BYOL: <strong>from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation.</strong></li> <li>We can expect to <strong>build a sequence of representations of increasing quality by iterating this procedure</strong>, <strong>using subsequent online networks as new target networks for further training.</strong></li> <li>In practice, BYOL <strong>generalizes this bootstrapping procedure</strong> by <strong>iteratively refining its representation</strong>, but <strong>using a slowly moving exponential average of the online network</strong> as the target network <strong>instead of fixed checkpoints</strong>.</li> </ul> <hr/> <p>ç»éªŒè§‰å¾— fixed ä¸è¡Œï¼Œä½†æ˜¯å·²ç»æ•ˆæœä¸é”™äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬åŸºäºæ­¤è®¾è®¡äº†æ–°çš„ã€‚ä»ä¸€ä¸ªç»™å®šçš„è¡¨å¾ï¼ˆtargetï¼‰è®­ç»ƒä¸€ä¸ªæ–°çš„åŠ å¼ºçš„è¡¨å¾ï¼ˆonlineï¼‰æ¥é¢„æµ‹ target çš„è¡¨å¾ã€‚</p> <hr/> <h5 id="description-of-byol">Description of BYOL</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/7aUmfsC.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. BYOL's architectures </div> <ul> <li> <p>BYOLâ€™s <strong>goal</strong> is to <strong>learn a representation</strong> \(y_\theta\) <strong>which can then be used for downstream tasks</strong>. ==&gt; <strong>representation</strong>.</p> </li> <li> <p>BYOL uses <strong>two neural networks</strong> to learn: the <strong>online</strong> and <strong>target networks</strong>.</p> </li> <li> <p>The <strong>online network is defined by a set of weights</strong> \(\theta\) and is comprised of <strong>three stages</strong>: an <em>encoder</em> \(f_\theta\), a projector \(g_\theta\) and a predictor \(q_\theta\)â€‹.</p> </li> <li> <p>The <strong>target network has the same architecture as the online network</strong>, but <strong>uses a different set of weights</strong> \(\xi\).</p> </li> <li> <p>The <strong>target</strong> network provides the <strong>regression targets to train the online network</strong>, and its parameters \(\xi\) are an <strong>exponential moving average</strong> of the <strong>online</strong> parameters \(\theta\).</p> </li> <li> <p>Given a <strong>target decay rate</strong> \(\tau\in [0, 1]\)â€‹, after each training step we perform the following update,</p> \[\xi \leftarrow \tau\xi + (1-\tau)\theta.\] </li> </ul> <hr/> <p>Online: encoder \(encoder(f_\theta) + prejector(g_\theta) + predictor(q_\theta)\), target has the same architecture but with different parameters comes from the above mentioned equation based on online.</p> <hr/> <ul> <li> <p>Given <strong>a set of images</strong> \(D\), an <strong>image</strong> \(x\sim D\) sampled uniformly from \(D\), and <strong>two distributions of image augmentations</strong> \(\mathcal{T}\) and \(\mathcal{T'}\), BYOL <strong>produces two augmented views</strong> \(v \triangleq t(x)\) and \(v'\triangleq t'(x)\) from \(x\) by <strong>applying respectively image augmentations</strong> \(t \sim \mathcal{T}\) and \(t' \sim \mathcal{T'}\). ==&gt; ä»ä¸¤ä¸ªç›¸åŒæµç¨‹çš„å¢å¼ºç­–ç•¥ä¸Šéšæœºé€‰ä¸¤å¥—ï¼Œå› ä¸ºå‚æ•°å¤§æ¦‚ç‡ä¸åŒï¼Œæ‰€ä»¥ä¸¤å¥—æµç¨‹çš„å‚æ•°ä¸åŒï¼Œæ‰€ä»¥æ˜¯ä¸¤ä¸ªä¸åŒçš„ viewã€‚</p> </li> <li> <p>From the <strong>first augmented view</strong> \(v\), the <strong>online network outputs</strong> a <em>representation</em> \(y_\theta \triangleq f_\theta(v)\) and a <strong>projection</strong> \(z_\theta\triangleq g_\theta(y)\).</p> </li> <li> <p>The <strong>target</strong> network outputs \(y'_\xi \triangleq f_\xi(v')\) and the <em>target projection</em> \(z'_\xi \triangleq g_\xi(y')\) from the <strong>second augmented view</strong> \(v'\)â€‹.</p> </li> <li> <p>We then output a <strong>prediction</strong> \(q_\theta(z_\theta)\) of \(z'_\xi\) and \(l_2\)-normalize both \(q_\theta(z_\theta)\) and \(z'_\xi\)â€‹ to</p> \[\overline{q_\theta}(z_\theta) \triangleq q_\theta(z_\theta)/\vert\vert q_\theta(z_\theta)\vert\vert_2\] <p>and</p> \[\overline{z'_\xi}/\vert\vert z'_\xi\vert\vert_2^2.\] </li> <li> <p>Note that this <strong>predictor is only applied to the online branch</strong>, making the <strong>architecture asymmetric between the online and target pipeline</strong>.</p> </li> <li> <p>Finally we dine the following mean squared error between the normalized predictions and target projections,</p> \[\mathcal{L}_{\theta, \xi} \triangleq \vert\vert{\overline{q_\theta}(z_\theta) - \overline{z'}_\xi}\vert\vert_2^2 = 2 - 2\cdot \frac{&lt;q_\theta(z_\theta), z'_\xi&gt;}{\vert\vert q_\theta(z_\theta)\vert\vert_2 \cdot \vert\vert z'_\xi\vert\vert_2}.\] </li> <li> <p>We <strong>symmetrize the loss above</strong> by separaetely feeding \(v'\) to the online network and \(v\) to the target network to compute the other one. ==&gt; å¯¹ç§°çš„æ¢ä¸€ä¸‹ä¸¤ä¸ªç½‘ç»œçš„ viewï¼Œç„¶åå°†ä¸¤ä¸ªç›¸åŠ å½¢æˆæœ€ç»ˆçš„ lossã€‚</p> </li> <li> <p>At <strong>each training step</strong>, we perform a stochastic optimization step to minimize</p> \[\mathcal{L}_{\theta, \xi}^{\text{BYOL}} = \mathcal{L}_{\theta, \xi} + \widetilde{\mathcal{L}}_{\theta, \xi}\] <p>with respect to \(\theta\) only, but not \(\xi\).</p> \[\theta \leftarrow \text{optimizer}(\theta, \nabla_\theta\mathcal{L}_{\theta, \xi}^{\text{BYOL}}, \eta),\\ \xi \leftarrow \tau\xi + (1-\tau)\theta\] <p>where optimizer is an optimzer and \(\eta\)â€‹ is a learning rate.</p> </li> <li> <p>At the end of training, we <strong>only keep the encoder</strong> $f_{\theta}$.</p> </li> </ul> <h5 id="implementation-details">Implementation details</h5> <ul> <li>Image augmentations: <ul> <li>BYOL uses the same set of image augmentations as in SimCLR.</li> <li>First a <strong>random patch of the image</strong> is selected and <strong>resized</strong> to \(224\times 224\)â€‹ with a random <strong>horizontal</strong> flip, followed by a <strong>color distrotion</strong>, consisting of a <strong>random sequence of brightness, contrast, saturation, hue adjustments, and an optional grayscale conversion</strong>. Finally, <strong>Gaussian blur</strong> and <strong>solarization</strong> are applied to the <strong>patches</strong>.</li> </ul> </li> <li><strong>Architecture</strong> <ul> <li><strong>ResNet</strong>-50 as our base parametric <strong>encoders</strong> \(f_\theta\) and \(f_\xi\)</li> <li>The <strong>representation</strong> $y$ <strong>corresopnds to the output of the final average pooling layer</strong>, which has a feature dimension of \(2048\).</li> <li>Contrary to SimCLR, the <strong>output</strong> of this MLP is <strong>not batch normalized</strong>. The <strong>predictor</strong> \(q_\theta\) uses the <strong>same</strong> <strong>architecture</strong> as \(g_\theta\)â€‹.</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li><strong>LARS</strong> <strong>optimizer</strong> with a <strong>cosine decay learning rate schedule</strong>, <strong>without restarts</strong>, over 1000 epochs, with a <strong>warm-up period of 10 epochs</strong>.</li> <li>the <strong>base learning rate</strong> to \(0.2\), scaled linearly with the batch size (Learning rate = \(0.2 \times \text{BatchSize}/256\))</li> <li>A global weight decay parameter of \(1.5\cdot 10^{-6}\) while excluding the biases and batch normalization parameters from both LARS adaptation and weight decay.</li> <li>A <strong>batch size of 4096</strong></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[Review of the paper BYOL and to extract the main points]]></summary></entry><entry><title type="html">Cookbook Reading - 1</title><link href="https://liuying-1.github.io/blog/2024/cookbook-1/" rel="alternate" type="text/html" title="Cookbook Reading - 1"/><published>2024-01-30T08:00:00+00:00</published><updated>2024-01-30T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2024/cookbook-1</id><content type="html" xml:base="https://liuying-1.github.io/blog/2024/cookbook-1/"><![CDATA[<p>All the following text are stemmed from the source book, please refer to the original copy - <em><a href="https://arxiv.org/pdf/2304.12210.pdf">A Cookbook of Self-Supervised Learning</a></em>. Additionally, there will be a large amount of Chinese translations on it since it will be used for me to recall something, sorry in advance.</p> <h4 id="what-is-self-supervised-learning-and-why-bother">What is Self-Supervised Learning and Why Bother?</h4> <p>è‡ªç›‘ç£å­¦ä¹ ï¼Œä¹Ÿè¢«ç§°ä¸º the dark matter of intelligenceã€‚<u>å®ƒæ˜¯ä¸€ä¸ªæå‡æœºå™¨å­¦ä¹ çš„åº·åº„å¤§é“ã€‚</u>ä¸ç›‘ç£å­¦ä¹ ä¸ä¸€æ ·ï¼Œç›‘ç£å­¦ä¹ å—åˆ¶äºæœ‰æ ‡ç­¾æ•°æ®çš„æ•°é‡ï¼Œè€Œ<u>è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿä»å¤§é‡æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®é‡Œé¢è¿›è¡Œå­¦ä¹ </u>ã€‚è‡ªç›‘ç£å­¦ä¹ æ˜¯æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—æˆåŠŸçš„åŸºç¡€ï¼Œå®ƒæ¨åŠ¨äº†ä»è‡ªåŠ¨æœºå™¨ç¿»è¯‘åˆ°åœ¨æ— æ ‡è®°æ–‡æœ¬çš„ç½‘ç»œè§„æ¨¡è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚åœ¨è®¡ç®—æœºè§†è§‰é‡Œé¢ï¼ŒSSLé€šè¿‡åœ¨10äº¿å¼ å›¾åƒä¸Šè®­ç»ƒçš„SEERç­‰æ¨¡å‹ï¼Œå°†<u>æ•°æ®è§„æ¨¡æ¨å‘äº†æ–°çš„æé™</u>ã€‚ä¸€äº›é’ˆå¯¹è®¡ç®—æœºæ•°æ®çš„SSLæ–¹æ³•ï¼Œå³ä½¿åœ¨ä¸€äº›ç«äº‰ç›¸å¯¹æ¿€çƒˆçš„Benchmarksä¸Šï¼ˆe.g., ImageNetï¼‰<u>å·²ç»èƒ½å¤Ÿå’Œä¸€äº›åœ¨æœ‰æ•°æ®æ ‡ç­¾çš„è®­ç»ƒçš„ç›‘ç£å­¦ä¹ ä¸Šå¾—åˆ°çš„æ¨¡å‹ç›¸åª²ç¾ç”šè‡³æ›´å¥½</u>ã€‚SSLä¹Ÿå·²ç»è¢«æˆåŠŸè¿ç”¨åœ¨<u>å„ä¸ªæ¨¡æ€æ•°æ®</u>ä¸Šï¼Œæ¯”å¦‚è§†é¢‘ï¼ŒéŸ³é¢‘ï¼Œå’Œä¸€äº›æ—¶é—´åºåˆ—ã€‚</p> <p>è‡ªç›‘ç£å­¦ä¹ æ ¹æ®ä½æ ‡ç­¾çš„è¾“å…¥<u>å®šä¹‰äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡æ¥äº§å‡ºdescriptive and intelligibleçš„ç‰¹å¾</u>ã€‚åœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œä¸€ä¸ªå¸¸è§çš„SSLçš„ç›®æ ‡å°±æ˜¯å»é®ä½ï¼ˆmaskï¼‰ä¸€ä¸ªæ–‡æœ¬ä¸­çš„å•è¯ï¼Œç„¶åé¢„æµ‹å‘¨å›´çš„å•è¯ã€‚è¿™ç§é¢„æµ‹ä¸€ä¸ªè¯å‘¨å›´çš„ä¸Šä¸‹æ–‡çš„ç›®æ ‡é¼“åŠ±äº†æ¨¡å‹ä¸ä¾èµ–ä»»ä½•æ ‡ç­¾ä¹Ÿèƒ½å»æ•æ‰æ–‡æœ¬ä¸­è¯æ±‡ä¸­çš„å…³ç³»ã€‚åŒæ ·çš„ç»è¿‡SSLæ¨¡å‹å­¦åˆ°çš„è¡¨å¾ä¹Ÿèƒ½è¢«ç”¨ä½œä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œæ¯”å¦‚ä¸åŒè¯­è¨€é—´çš„æ–‡æœ¬ç¿»è¯‘ï¼Œæ€»ç»“ï¼Œç”šè‡³äº§ç”Ÿæ–‡æœ¬ï¼Œå½“ç„¶ä¹Ÿè¿˜æœ‰ä¸€äº›åˆ«çš„ã€‚åœ¨CVä¸­ï¼Œ<strong>MAE</strong>æˆ–<strong>BYOL</strong>è¿™äº›æ–¹æ³•ä¹Ÿå­˜åœ¨ç›¸ä¼¼çš„ç›®æ ‡ï¼Œé€šè¿‡å­¦ä¹ æ¥é¢„æµ‹ä¸€ä¸ªå›¾åƒè¢«é®ä½çš„patchæˆ–è€…å…¶è¡¨å¾ã€‚ä¹Ÿæœ‰åˆ«çš„SSLç›®æ ‡ï¼Œé€šè¿‡æ·»åŠ é¢œè‰²æˆ–è€…è£å‰ªå›¾ç‰‡æ¥å½¢æˆåŒä¸€ä¸ªå›¾ç‰‡çš„ä¸¤ä¸ªä¸åŒè§†è§’ï¼Œç„¶åé¼“åŠ±æ¨¡å‹èƒ½å­¦åˆ°æ˜ å°„åˆ°ç›¸ä¼¼è¡¨å¾çš„èƒ½åŠ›ã€‚</p> <p>æœ‰äº†èƒ½åŠ›æ¥åœ¨å¤§é‡çš„æœªæ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¸¦æ¥äº†å¾ˆå¤šå¥½å¤„ã€‚ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•é€šå¸¸æ˜¯æ ¹æ®ç°æœ‰çš„æœ‰æ ‡ç­¾æ•°æ®å¯¹å·²çŸ¥çš„ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼ˆå…ˆéªŒçš„ï¼‰ï¼Œè€Œ <u>SSL åˆ™æ˜¯å­¦ä¹ å¯¹è®¸å¤šä»»åŠ¡éƒ½æœ‰ç”¨çš„é€šç”¨è¡¨å¾</u>ã€‚<u>SSL åœ¨åŒ»å­¦ç­‰é¢†åŸŸå°¤å…¶æœ‰ç”¨</u>ï¼Œå› ä¸ºåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ ‡ç­¾çš„æˆæœ¬å¾ˆé«˜ï¼Œæˆ–è€…æ— æ³•äº‹å…ˆçŸ¥é“å…·ä½“çš„ä»»åŠ¡ã€‚è¿˜æœ‰è¯æ®è¡¨æ˜ï¼ŒSSL æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¯¹å¯¹æŠ—æ€§ç¤ºä¾‹ï¼ˆ<u>adversarial examples</u>ï¼‰ã€æ ‡ç­¾æŸåï¼ˆ<u>label corruption</u>ï¼‰å’Œè¾“å…¥æ‰°åŠ¨ï¼ˆ<u>input perturbations</u>ï¼‰æ›´ç¨³å¥ï¼ˆ<u>robust</u>ï¼‰çš„è¡¨å¾ï¼Œè€Œä¸”ä¸æœ‰ç›‘ç£çš„æ¨¡å‹ç›¸æ¯”æ›´å…¬å¹³ã€‚å› æ­¤ï¼ŒSSLæ˜¯ä¸€ä¸ªè¶Šæ¥è¶Šå—å…³æ³¨çš„é¢†åŸŸã€‚ç„¶è€Œï¼Œå’Œåšé¥­å¾ˆåƒï¼Œè®­ç»ƒSSLæ–¹æ³•æ˜¯ä¸€ä¸ªé—¨æ§›å¾ˆé«˜çš„å¾®å¦™è‰ºæœ¯ã€‚</p> <h5 id="why-a-cookbook-for-self-supervised-learning">Why a Cookbook for Self-Supervised Learning?</h5> <p>è™½ç„¶ SSL çš„è®¸å¤šç»„æˆéƒ¨åˆ†éƒ½ä¸ºç ”ç©¶äººå‘˜æ‰€ç†Ÿæ‚‰ï¼Œä½†è¦æˆåŠŸè®­ç»ƒ SSL æ–¹æ³•ï¼Œå´éœ€è¦ä»<u>å€Ÿå£ä»»åŠ¡åˆ°è®­ç»ƒè¶…å‚æ•°</u>ç­‰ä¸€ç³»åˆ—ä»¤äººçœ¼èŠ±ç¼­ä¹±çš„é€‰æ‹©ã€‚SSLç ”ç©¶æœ‰ä¸€ä¸ªé«˜çš„é—¨æ§›ï¼Œä¸€æ˜¯å› ä¸º<u>è®¡ç®—æˆæœ¬</u>ï¼ŒäºŒæ˜¯<u>æ²¡æœ‰å®Œå…¨é€æ˜çš„è®ºæ–‡</u>è¯¦ç»†è®²è¿°å…¶ä¸­èƒ½å¤Ÿå……åˆ†å‘æŒ¥SSLçš„æ½œèƒ½çš„å¤æ‚çš„å®ç°æ–¹æ³•ï¼Œä¸‰æ˜¯<u>ç¼ºä¹SSLç»Ÿä¸€çš„è¯æ±‡å’Œç†è®ºè§‚ç‚¹</u>ã€‚å› ä¸ºSSLä»ä¼ ç»Ÿçš„reconstruction-basedæ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆe.g.ï¼Œdenoising variational autoencodersï¼‰å»ºç«‹äº†ä¸€å¥—ç‹¬ç‰¹çš„èŒƒå¼ï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€è§†è§’ä¸‹ç†è§£ SSL çš„è¯æ±‡æ˜¯æœ‰é™çš„ã€‚äº‹å®ä¸Šï¼Œåœ¨ä¸€ä¸ªå•ä¸€è§†è§’ä¸‹ç»Ÿä¸€SSLæ–¹æ³•åœ¨2021ä¹Ÿä»…ä»…æ‰å¼€å§‹å‡ºç°ã€‚å¦‚æœæ²¡æœ‰ä¸€ä¸ªå…±åŒçš„åŸºç¡€æ¥æè¿° SSL æ–¹æ³•çš„ä¸åŒç»„æˆéƒ¨åˆ†ï¼Œç ”ç©¶äººå‘˜åœ¨å¼€å§‹ç ”ç©¶ SSL æ–¹æ³•æ—¶å°±ä¼šé¢ä¸´æ›´å¤§çš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒSSLç ”ç©¶ä¹Ÿæ€¥éœ€æ–°çš„ç ”ç©¶äººå‘˜ï¼Œå› ä¸ºSSLç°åœ¨å·²ç»éå¸ƒæˆ‘ä»¬çš„ç”Ÿæ´»äº†ã€‚ç„¶è€Œï¼Œå…³äº SSL çš„<u>é€šç”¨æ€§ä¿è¯ã€å…¬å¹³æ€§å±æ€§ä»¥åŠå¯¹å¯¹æŠ—æ€§æ”»å‡»</u>ç”šè‡³<u>è‡ªç„¶å‘ç”Ÿçš„å˜å¼‚</u>çš„é²æ£’æ€§ï¼Œä»æœ‰è®¸å¤šæœªè§£å†³çš„ç ”ç©¶é—®é¢˜ã€‚è¿™ç±»é—®é¢˜å¯¹äºSSLæ–¹æ³•çš„ä¾èµ–ç¨‹åº¦ä¹Ÿå¾ˆå…³é”®ã€‚</p> <p>æ­¤å¤–ï¼ŒSSL æ˜¯ç”±<u>ç»éªŒé©±åŠ¨</u>çš„ï¼Œå®ƒæœ‰è®¸å¤šæ´»åŠ¨éƒ¨ä»¶ï¼ˆä¸»è¦æ˜¯<u>è¶…å‚æ•°</u>ï¼‰ï¼Œè¿™äº›éƒ¨ä»¶å¯èƒ½ä¼šå½±å“æœ€ç»ˆè¡¨å¾çš„å…³é”®å±æ€§ï¼Œè€Œä¸”åœ¨å·²å‘è¡¨çš„ä½œå“ä¸­<u>ä¸ä¸€å®šæœ‰è¯¦ç»†è¯´æ˜</u>ã€‚å³ï¼Œä¸ºäº†å¼€å§‹ç ”ç©¶SSLæ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»<u>é¦–å…ˆå¯¹è¿™äº›æ–¹æ³•è¿›è¡Œè¯¦å°½çš„å®è¯ç ”ç©¶</u>ï¼Œæ‰èƒ½å……åˆ†æŒæ¡<u>æ‰€æœ‰è¿™äº›æ´»åŠ¨éƒ¨ä»¶çš„å½±å“å’Œè¡Œä¸º</u>ã€‚è¿™ç§ç»éªŒç›²ç‚¹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå·²æœ‰çš„å®è·µç»éªŒï¼Œå› æ­¤å…·æœ‰å¾ˆå¼ºçš„å±€é™æ€§ã€‚æ€»ä¹‹ï¼ŒSOTA çš„æ€§èƒ½æ¥è‡ªçœ‹ä¼¼ä¸åŒä½†åˆç›¸äº’é‡å çš„æ–¹æ³•ï¼Œç°æœ‰çš„ç†è®ºç ”ç©¶å¾ˆå°‘ï¼Œè€Œå®é™…åº”ç”¨å´å¾ˆå¹¿æ³›ï¼Œå› æ­¤éœ€è¦ä¸€æœ¬ç»Ÿä¸€æŠ€æœ¯åŠå…¶â€œé…æ–¹â€çš„æŒ‡å—ï¼Œè¿™å¯¹é™ä½ SSL çš„ç ”ç©¶é—¨æ§›è‡³å…³é‡è¦ã€‚</p> <p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºäº†é™ä½ä»äº‹SSLç ”ç©¶çš„é—¨æ§›ï¼Œé€šè¿‡é“ºè®¾åŸºç¡€å’Œä¸€äº›ç›¸å¯¹è¾ƒæ–°çš„SSLæ–¹æ³•ï¼Œä»¥ä¸€ç§cookbookçš„é£æ ¼æ¥å‘ˆç°ã€‚ä¸ºäº†â€œçƒ¹é¥ªâ€æˆåŠŸï¼Œä½ å¿…é¡»é¦–å…ˆå­¦ä¹ åŸºæœ¬æŠ€æœ¯ï¼šåˆ‡èœï¼Œç‚’èœç­‰ã€‚åœ¨ç¬¬ 2 èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆ<u>ä½¿ç”¨é€šç”¨è¯æ±‡ä»‹ç»è‡ªç›‘ç£å­¦ä¹ çš„åŸºæœ¬æŠ€æœ¯</u>ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ä»ç»Ÿä¸€çš„è§†è§’æ¥<u>æè¿°è¿™äº›æ–¹æ³•ç³»åˆ—ä»¥åŠå°†å®ƒä»¬çš„ç›®æ ‡è”ç³»èµ·æ¥çš„ç†è®ºä¸»çº¿</u>ã€‚æˆ‘ä»¬åœ¨æ¦‚å¿µæ¡†ä¸­çªå‡ºäº†æŸå¤±é¡¹æˆ–è®­ç»ƒç›®æ ‡ç­‰å…³é”®æ¦‚å¿µã€‚ç„¶åï¼Œä¸€ä¸ªå¨å¸ˆå¿…é¡»å­¦ä¼šè¿ç”¨æŠ€æœ¯å»åšæˆä¸€é“ç¾å‘³çš„èœï¼Œè¿™ä¹Ÿå°±éœ€è¦å­¦ä¹ ç°æœ‰çš„èœè°±ï¼Œç»„åˆé£Ÿæï¼Œè¿˜æœ‰å“å‘³èœå“ã€‚åœ¨ç¬¬ä¸‰èŠ‚ä¸­ï¼Œæˆ‘ä»¬<u>ä»‹ç»äº†å®é™…æˆåŠŸå®ç°SSLæ–¹æ³•ä¸­ï¼Œéœ€è¦è€ƒè™‘çš„ç‚¹ã€‚æˆ‘ä»¬è®¨è®ºäº†é€šå¸¸çš„è®­ç»ƒèœè°±ï¼ŒåŒ…æ‹¬è¶…å‚æ•°çš„é€‰æ‹©ï¼Œå¦‚ä½•ç»„åˆä¸åŒç»„ä»¶ï¼ˆæ¯”å¦‚ç»“æ„å’Œä¼˜åŒ–å™¨ï¼‰ï¼Œè¿˜æœ‰å¦‚ä½•è¯„ä»·SSLæ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å°†åˆ†äº«é¡¶å°–ç ”ç©¶äººå‘˜å°±å¸¸è§åŸ¹è®­é…ç½®å’Œé™·é˜±æå‡ºçš„å®ç”¨å»ºè®®</u>ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æœ¬æ‰‹å†Œèƒ½æˆä¸ºæˆåŠŸè®­ç»ƒå’Œæ¢ç´¢è‡ªç›‘ç£å­¦ä¹ çš„å®ç”¨åŸºç¡€ã€‚</p> <h4 id="the-families-and-origins-of-ssl">The families and origins of SSL</h4> <p>ç”±äº<u>å¤§è§„æ¨¡çš„ç‰¹åˆ«å¤§çš„æ•°æ®é›†å’Œé«˜å†…å­˜çš„GPUçš„å¯è·å¾—æ€§</u>ï¼Œè‡ª2020å¹´ä»¥æ¥ï¼ŒSSLæ–¹æ³•å°±å¼€å§‹å¤å…´äº†ã€‚ä½†æ˜¯SSLçš„èµ·æºå¯ä»¥<u>è¿½æº¯åˆ°æ·±åº¦å­¦ä¹ å¹´ä»£çš„æœ€å¼€å§‹</u>ã€‚</p> <h5 id="origins-of-ssl">Origins of SSL</h5> <p>å½“ä»Šçš„æ–¹æ³•éƒ½å»ºç«‹äºæˆ‘ä»¬ä»<u>æ—©æœŸå®éªŒ</u>ä¸­è·å¾—åˆ°çš„çŸ¥è¯†ã€‚åœ¨è¿™ä¸€ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™äº†ä¸€ä¸ªåœ¨2020å¹´ä»¥å‰å…³äºSSLç‚¹å­çš„ç®€è¿°ã€‚è™½ç„¶è®¸å¤šå…·ä½“æ–¹æ³•å·²ç»ä¸å†è¢«ä¸»æµä½¿ç”¨ï¼Œå› ä¸ºå®ƒä»¬ä¸å†èƒ½åœ¨åŸºå‡†é—®é¢˜ä¸Šæä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä¹Ÿä¸ä¼šå¯¹å®ƒä»¬è¿›è¡Œè¯¦ç»†è®¨è®ºï¼Œä½†è¿™äº›è®ºæ–‡ä¸­çš„è§‚ç‚¹æ„æˆäº†è®¸å¤šç°ä»£æ–¹æ³•çš„åŸºç¡€ã€‚æ¯”å¦‚ï¼Œrestoring missing or distorted parts of an input or contrasting two views of the same imageçš„æ ¸å¿ƒç›®æ ‡å½¢æˆäº†ç°ä»£SSLæ–¹æ³•çš„åŸºç¡€ã€‚SSL <u>æ—©æœŸçš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨ä»¥ä¸‹å‡ ç±»</u>ï¼ˆæœ‰æ—¶ç›¸äº’é‡å ï¼‰æ–¹æ³•çš„å¼€å‘ä¸Šï¼š</p> <ol> <li> <p><strong>Information restoration</strong>: å¤§é‡çš„æ–¹æ³•è¢«å¼€å‘æ¥<u>é®ä½æˆ–è€…ç§»é™¤ä¸€ä¸ªå›¾ç‰‡ä¸­çš„ä¸€äº›ä¸œè¥¿</u>ï¼Œç„¶å<u>è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ¢å¤ç¼ºå¤±éƒ¨åˆ†çš„ä¿¡æ¯</u>ã€‚åŸºäºä¸Šè‰²çš„SSLæ–¹æ³•ï¼ˆ<u>Colorization-based</u>ï¼‰SSL methodsæŠŠä¸€ä¸ªå›¾ç‰‡<u>è½¬æ¢æˆç°åº¦å›¾</u>ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥<u>é¢„æµ‹åŸæœ¬çš„RGBå€¼</u>ã€‚ç”±äºç€è‰²éœ€è¦äº†è§£å¯¹è±¡çš„è¯­ä¹‰å’Œè¾¹ç•Œï¼Œå› æ­¤<u>ç€è‰²è¢«è¯æ˜æ˜¯æ—©æœŸçš„å¯¹è±¡åˆ†å‰² SSL æ–¹æ³•</u>ã€‚è¿™ç§ä¿¡æ¯æ¢å¤æœ€ç›´è§‚ç®€å•çš„åº”ç”¨æ˜¯é®ä½å³<u>æŒªå»å›¾åƒçš„ä¸€éƒ¨åˆ†ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥ç»™ç¼ºå¤±çš„åƒç´ ç€è‰²</u>ã€‚è¿™ä¸ªç‚¹å­è¿›åŒ–æˆäº†Masked auto-encoding methodsï¼Œå…¶ä¸­ï¼Œ<u>æ©è”½åŒºåŸŸæ˜¯å›¾åƒpatchesçš„ç»“åˆä½“ï¼ˆunionï¼‰ï¼Œå¯ä»¥ä½¿ç”¨transfomerè¿›è¡Œé¢„æµ‹</u>ã€‚</p> </li> <li> <p><strong>Using temporal relationships in video</strong>: å°½ç®¡è¿™ä¸ªå›é¡¾çš„ç„¦ç‚¹æ˜¯åœ¨å›¾åƒï¼ˆè€Œéè§†é¢‘ï¼‰å¤„ç†ä¸Šï¼Œä¸€ç³»åˆ—ä¸“é—¨æ–¹æ³•å·²ç»è¢«å¼€å‘å‡ºæ¥ï¼Œé€šè¿‡å¯¹<u>è§†é¢‘è¿›è¡Œé¢„è®­ç»ƒæ¥å­¦ä¹ å•å›¾åƒè¡¨å¾ï¼ˆsingle-image representationsï¼‰</u>ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ<u>ä¿¡æ¯è¿˜åŸï¼ˆinformation restorationï¼‰æ–¹æ³•å¯¹è§†é¢‘ç‰¹åˆ«æœ‰ç”¨</u>ï¼Œå› ä¸ºè§†é¢‘åŒ…å«å¤šç§å¯èƒ½è¢«å±è”½çš„ä¿¡æ¯æ¨¡å¼ã€‚Wang and Gupta ä½¿ç”¨<u>ä¸‰é‡æŸå¤±ï¼ˆtriplet lossï¼‰å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒ</u>ï¼Œä»¥<u>æé«˜ä¸¤ä¸ªä¸åŒå¸§ä¸­åŒä¸€ä¸ªç‰©ä½“è¡¨å¾çš„ç›¸ä¼¼æ€§</u>ã€‚æœ€ç»ˆçš„ç»“æœæ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹ä¸Šè¡¨ç°ä¸é”™ã€‚Pathak et al. è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹æ¥<u>é¢„æµ‹ç‰©ä½“åœ¨å•ä¸€å¸§ä¸­çš„è¿åŠ¨</u>ï¼Œè°ƒæ•´ç”±æ­¤äº§ç”Ÿçš„featuresæ¥è§£å†³å•ä¸€å¸§ï¼ˆsingle-frameï¼‰æ£€æµ‹é—®é¢˜ã€‚Agrawal et al. <u>é¢„æµ‹å¤šå¸§å›¾åƒä¸­æ‘„åƒæœºçš„è‡ªæˆ‘è¿åŠ¨</u>ï¼ˆego-motionï¼‰è½¨è¿¹ã€‚Owens et al. æå‡º<u>ä»è§†é¢‘ä¸­å»æ‰éŸ³è½¨ï¼Œç„¶åé¢„æµ‹ç¼ºå¤±çš„å£°éŸ³</u>ã€‚å¯¹äºæ·±åº¦æ˜ å°„ï¼ˆdepth mappingï¼‰ç­‰ä¸“ä¸šåº”ç”¨ï¼Œæœ‰äººæå‡ºäº†è‡ªç›‘ç£æ–¹æ³•ï¼Œä»<u>æœªæ ‡æ˜çš„å›¾åƒå¯¹ä¸­å­¦ä¹ å•ç›®æ·±åº¦</u>ï¼ˆmonocular depthï¼‰æ¨¡å‹ï¼Œç„¶å<u>ä»å•æ‘„åƒå¤´è§†é¢‘ï¼ˆsingle-camera videoï¼‰ä¸­å­¦ä¹ å¸§</u>ã€‚æ­¤ç±»æ–¹æ³•ä»æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚</p> </li> <li> <p><strong>Learning spatial context</strong>: è¿™ä¸ªç§ç±»çš„æ–¹æ³•æ˜¯<u>è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥ç†è§£ç‰©ä½“åœ¨ä¸€ä¸ªåœºæ™¯çš„ç›¸å¯¹ä½ç½®å’Œæ–¹å‘</u>ã€‚RotNet é€šè¿‡éšæœºæ—‹è½¬æ¥<u>æ©ç›–é‡åŠ›æ–¹å‘ï¼Œç„¶åè¦æ±‚æ¨¡å‹é¢„æµ‹æ—‹è½¬</u>ã€‚Doersch et al. æ˜¯æœ€æ—©çš„ SSL æ–¹æ³•ä¹‹ä¸€ï¼Œå®ƒå¯ä»¥<u>ç®€å•åœ°é¢„æµ‹å›¾åƒä¸­ä¸¤ä¸ªéšæœºé‡‡æ ·çš„patchçš„ç›¸å¯¹ä½ç½®</u>ã€‚è¿™ä¸ªæ–¹æ³•åæ¥<u>è¢«Jigsawæ–¹æ³•å–ä»£</u>äº†ã€‚Jigsawæ–¹æ³•å°±æ˜¯æŠŠ<u>ä¸€ä¸ªå›¾ç‰‡åˆ†æˆä¸€åˆ—disjointçš„patchesç„¶åé¢„æµ‹ç›¸äº’çš„ç›¸å¯¹ä½ç½®</u>ã€‚ä¸€ä¸ªä¸åŒçš„ç©ºé—´ä»»åŠ¡æ˜¯å­¦ä¹ æ•°æ•°ï¼šæ¨¡å‹æ˜¯åœ¨è‡ªç›‘ç£æ–¹å¼è¢«è®­ç»ƒæ¥<u>è¾“å‡ºä¸€ä¸ªå›¾åƒä¸­ç‰©ä½“çš„æ•°é‡</u>ã€‚</p> </li> <li> <p><strong>Grouping similar images together</strong>: é€šè¿‡å°†<u>è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒåˆ†ç»„</u>ï¼Œå¯ä»¥å­¦ä¹ åˆ°<u>ä¸°å¯Œçš„ç‰¹å¾</u>ã€‚K-meansèšç±»å°±æ˜¯åœ¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ä¸­æœ€å¹¿æ³›è¿ç”¨çš„æ–¹æ³•ä¹‹ä¸€ã€‚è®¸å¤šç ”ç©¶éƒ½å¯¹ k-means è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥ä¾¿åˆ©ç”¨ç¥ç»æ¨¡å‹åš SSLã€‚<u>æ·±åº¦èšç±»æŠ€æœ¯é€šè¿‡åœ¨ç‰¹å¾ç©ºé—´ï¼ˆfeature spaceï¼‰ä¸­æ‰§è¡Œ k-means æ¥äº¤æ›¿ä¸ºå›¾åƒåˆ†é…æ ‡ç­¾</u>ï¼Œå¹¶<u>æ›´æ–°æ¨¡å‹ä»¥respectè¿™äº›åˆ†é…çš„ç±»åˆ«æ ‡ç­¾</u>ã€‚è¿™ä¸ªæ–¹æ³•æœ€è¿‘çš„å¤„ç†æ–¹å¼æ˜¯ä½¿ç”¨<u>mean-shiftæ›´æ–°æ¥æŠŠfeaturesæ¨åˆ°ä»–ä»¬çš„èšç±»ä¸­å¿ƒ</u>ï¼Œè€Œä¸”å·²ç»è¢«è¯æ˜å¯ä»¥ç”¨æ¥<u>è¡¥å……BYOL</u>æ–¹æ³•ã€‚BYOLæ˜¯ä¸€ç§åŸºäºä¸¤ä¸ªç½‘ç»œçš„æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯é¢„æµ‹<u>æ¯ä¸ªæ ·æœ¬çš„ä¼ªæ ‡ç­¾</u>ã€‚å¯¹æ·±åº¦èšç±»çš„åˆ«çš„æå‡åŒ…æ‹¬åœ¨ç‰¹å¾ç©ºé—´ç”¨optimal transport methodsæ¥åˆ›å»ºæ›´æœ‰ä¿¡æ¯çš„èšç±»ã€‚</p> </li> <li> <p><strong>Generative models</strong>: ä¸€ç§æ—©æœŸçš„æœ‰å½±å“åŠ›çš„SSLæ–¹æ³•æ˜¯è´ªå©ªå±‚çº§ï¼ˆgreedy layer-wiseï¼‰é¢„è®­ç»ƒï¼Œå…¶ä¸­ï¼Œæ·±åº¦ç½‘ç»œçš„å„å±‚ä½¿ç”¨autoencoder lossè¿›è¡Œé€å±‚è®­ç»ƒã€‚å½“æ—¶çš„ä¸€ç§ç±»ä¼¼æ–¹æ³•æ˜¯ä½¿ç”¨å—é™ç»å°”å…¹æ›¼æœº (RBMs)ï¼Œè¿™ç§æœºå™¨å¯ä»¥è¿›è¡Œåˆ†å±‚è®­ç»ƒå’Œå †å ï¼Œä»¥åˆ›å»ºæ·±åº¦ä¿¡å¿µç½‘ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å·²è¢«æ”¾å¼ƒï¼Œè½¬è€Œé‡‡ç”¨æ›´ç®€å•çš„åˆå§‹åŒ–ç­–ç•¥å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä½†å®ƒä»¬åœ¨å†å²ä¸Šå¯¹ SSL çš„ä½¿ç”¨äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œå› ä¸ºå®ƒä»¬ä¿ƒæˆäº†ç¬¬ä¸€æ‰¹ â€œæ·±åº¦ â€œç½‘ç»œçš„è®­ç»ƒã€‚åæ¥åœ¨auto-encodersçš„è¡¨å¾å­¦ä¹ çš„èƒ½åŠ›ä¸Šçš„æå‡æœ‰åŒ…æ‹¬denoising autoencodersï¼Œcross-channel predictionï¼Œå’Œdeep canoncically correlated autoencodersã€‚ä½†æ˜¯æœ€ç»ˆå‘ç°ï¼Œå½“è¦æ±‚auto-encoderæ¢å¤å…¶è¾“å…¥çš„ç¼ºå¤±éƒ¨åˆ†æ—¶ï¼Œè¡¨å¾è½¬ç§»æ€§ä¼šæ›´å¥½ï¼Œè¿™å°±å½¢æˆäº† SSL æ–¹æ³•ä¸­çš„ â€œä¿¡æ¯æ¢å¤ â€œç±»åˆ«ã€‚</p> <p>ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œç”±ä¸€ä¸ªå›¾åƒç”Ÿæˆå™¨å’Œä¸€ä¸ªåˆ¤åˆ«å™¨ç»„æˆï¼Œåˆ¤åˆ«å™¨åŒºåˆ†çœŸå›¾åƒå’Œç”Ÿæˆçš„å‡å›¾åƒã€‚è¿™å¯¹æ¨¡å‹çš„ä¸¤ä¸ªç»„æˆéƒ¨åˆ†éƒ½å¯ä»¥åœ¨æ²¡æœ‰ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸”ä¸¤è€…éƒ½å¯èƒ½åŒ…å«å¯¹è¿ç§»å­¦ä¹ æœ‰ç”¨çš„çŸ¥è¯†ã€‚æ—©æœŸçš„GANsçš„è®ºæ–‡ä½¿ç”¨ GAN ç»„ä»¶è¿›è¡Œä¸‹æ¸¸å›¾åƒåˆ†ç±»å®éªŒã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸“é—¨çš„ç‰¹å¾å­¦ä¹ ç¨‹åºæ¥ä¿®æ”¹åˆ¤åˆ«å™¨ã€æ·»åŠ ç”Ÿæˆå™¨æˆ–å­¦ä¹ ä»å›¾åƒåˆ°æ½œç©ºé—´çš„é¢å¤–æ˜ å°„ï¼Œä»¥æ”¹è¿›è¿ç§»å­¦ä¹ ã€‚</p> </li> <li> <p><strong>Multi-view invariance</strong>: è®¸å¤šç°ä»£ SSL æ–¹æ³•ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­é‡ç‚¹è®¨è®ºçš„æ–¹æ³•ï¼Œéƒ½ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆcontrastive learningï¼‰æ¥åˆ›å»º<u>ä¸å—ç®€å•å˜æ¢å½±å“çš„ç‰¹å¾è¡¨å¾</u>ã€‚å¯¹æ¯”å­¦ä¹ çš„ç†å¿µæ˜¯é¼“åŠ±æ¨¡å‹<u>ä»¥ç›¸ä¼¼çš„æ–¹å¼è¡¨ç¤ºè¾“å…¥çš„ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬</u>ã€‚åœ¨å¯¹æ¯”å­¦ä¹ è¢«å¹¿æ³›é‡‡ç”¨ä¹‹å‰ï¼Œæœ‰è®¸å¤šæ–¹æ³•ä»¥å„ç§æ–¹å¼å¼ºåˆ¶ä¿æŒä¸å˜æ€§ï¼Œä»è€Œå¼•é¢†äº†è¿™ä¸€æ–¹å‘ã€‚</p> <p>ä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ çš„æœ€æµè¡Œæ¡†æ¶ä¹‹ä¸€ï¼Œæ˜¯ä½¿ç”¨<u>å¼±è®­ç»ƒç½‘ç»œä¸ºå›¾åƒæ·»åŠ ä¼ªæ ‡ç­¾</u>ï¼ˆpseudolabelsï¼‰ï¼Œç„¶åä»¥æ ‡å‡†<u>ç›‘ç£æ–¹å¼ä½¿ç”¨è¿™äº›æ ‡ç­¾è¿›è¡Œè®­ç»ƒ</u>ã€‚è¿™ç§æ–¹æ³•åæ¥å¾—åˆ°äº†æ”¹è¿›ï¼Œ<u>åŠ å¼ºäº†å¯¹å˜æ¢çš„ä¸å˜æ€§</u>ï¼ˆinvariance to transformationsï¼‰ã€‚è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒï¼ˆvirtual adversarial trainingï¼‰<u>åˆ©ç”¨å›¾åƒçš„ä¼ªæ ‡ç­¾å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒ</u>ï¼Œæ­¤å¤–è¿˜è¿›è¡Œ<u>å¯¹æŠ—è®­ç»ƒ</u>ï¼Œä½¿å­¦ä¹ åˆ°çš„ç‰¹å¾å‡ ä¹ä¸å—è¾“å…¥å›¾åƒå¾®å°æ‰°åŠ¨ï¼ˆsmall perturbationsï¼‰çš„å½±å“ã€‚åæ¥çš„å·¥ä½œé‡ç‚¹æ˜¯<u>ä¿æŒæ•°æ®å¢å¼ºï¼ˆaugmentationï¼‰å˜æ¢çš„ä¸å˜æ€§</u>ï¼ˆmaintain invariance to data augmentation transformsï¼‰ã€‚è¿™ç±»æ—©æœŸçš„é‡è¦æ–¹æ³•åŒ…æ‹¬ MixMatchï¼Œè¯¥æ–¹æ³•é€šè¿‡<u>å¹³å‡</u>ï¼ˆaverageï¼‰ç½‘ç»œåœ¨<u>å¤šä¸ªä¸åŒéšæœºå¢å¼º</u>ï¼ˆaugmentationï¼‰<u>è®­ç»ƒå›¾åƒä¸Šçš„è¾“å‡º</u>æ¥é€‰æ‹©<u>ä¼ªæ ‡ç­¾</u>ï¼Œä»è€Œå¾—åˆ°<u>å¢å¼ºä¸å˜çš„æ ‡ç­¾</u>ã€‚å¤§çº¦åœ¨åŒä¸€æ—¶é—´ï¼Œäººä»¬å‘ç°ï¼Œé€šè¿‡<u>è®­ç»ƒç½‘ç»œä½¿ä¸åŒè§†è§’ä¸‹çš„å›¾åƒè¡¨å¾ä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–</u>ï¼Œå¯ä»¥å®ç°<u>è‰¯å¥½çš„ SSL æ€§èƒ½</u>ã€‚è¿™äº›ä»¥å¢å¼ºä¸ºåŸºç¡€ï¼ˆaugmentation-basedï¼‰çš„æ–¹æ³•åœ¨ä¸Šè¿°æ—§æ–¹æ³•å’Œæœ¬æ–‡é‡ç‚¹è®¨è®ºçš„ç°ä»£æ–¹æ³•ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚</p> </li> </ol> <p>æœ‰äº†è¿™äº›æ¸Šæºï¼Œæˆ‘ä»¬ç°åœ¨å°† SSL åˆ†æˆå››å¤§ç³»åˆ—ï¼šæ·±åº¦åº¦é‡å­¦ä¹ ï¼ˆDeep Metric Learningï¼‰ç³»åˆ—ã€è‡ªé¦åˆ†ï¼ˆSelf-Distillationï¼‰ç³»åˆ—ã€å…¸å‹ç›¸å…³åˆ†æï¼ˆCanonical Correlation Analysisï¼‰ç³»åˆ—å’Œå±è”½å›¾åƒå»ºæ¨¡ï¼ˆMasked Image Modelingï¼‰ç³»åˆ—ã€‚</p> <p>åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œå°†ä¼šä»¥åŸè‘—çš„æ–¹å¼å‘ˆç°æ‘˜å½•æ¥å½¢æˆç¬”è®°ã€‚</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[This is the first part of this book.]]></summary></entry><entry><title type="html">Advanced Image Registration</title><link href="https://liuying-1.github.io/blog/2023/advanced-image-registration/" rel="alternate" type="text/html" title="Advanced Image Registration"/><published>2023-10-10T08:00:00+00:00</published><updated>2023-10-10T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/advanced-image-registration</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/advanced-image-registration/"><![CDATA[<h4 id="todays-learning-objectives">Todays Learning Objectives</h4> <ul> <li> <p>What does <strong><u>regularization</u></strong> do in the context of <strong><u>registration</u></strong>?</p> <p>In the context of image registration, regularization refers to a technique used to <strong>improve</strong> the <strong>accuracy</strong> and <strong>robustness</strong> of the registration process. Hereâ€™s <strong>what regularization does</strong>:</p> <ul> <li><strong>Control Deformation</strong>: Image registration aims to <strong><u>align two images by finding a transformation that maps one image onto the other</u></strong>. <strong><u>Regularization</u></strong> helps control the <strong><u>degree of deformation or warping allowed during this alignment</u></strong>. It <strong>prevents</strong> overly <strong><u>complex and unrealistic deformations</u></strong> that may lead to poor results.</li> <li><strong>Penalize Irregularities</strong>: Regularization introduces <strong><u>a penalty term</u></strong> in the registration objective function. This penalty term <strong><u>discourages irregular, non-smooth deformations</u></strong>. By doing so, it <strong><u>promotes smooth and plausible transformations</u></strong> between the images.</li> <li><strong>Balancing Trade-Off</strong>: Regularization <strong><u>strikes a balance</u></strong> between <strong><u>fitting the images together accurately</u></strong> and ensuring that the <strong><u>deformation remains physically plausible</u></strong>. Without regularization, registration can result in <strong><u>overly complex or noisy transformations</u></strong> that may not align images effectively.</li> <li><strong>Controlling Sensitivity</strong>: It also helps control the <strong><u>sensitivity</u></strong> of the registration process <strong><u>to noise or outliers</u></strong> in the data. Regularization can make the registration process <strong><u>more stable and less prone to small variations</u></strong> in the images.</li> </ul> <p>In summary, regularization in image registration ensures that the <strong><u>alignment process is not overly complex</u></strong> and <strong><u>produces physically meaningful transformations</u></strong> that are both accurate and stable.</p> </li> <li> <p><u>Name</u> and <u>explain</u> at least two <u>clinical applications of registration</u></p> <ol> <li><strong>Image-Guided Surgery</strong>: Image registration is widely used in surgery. It involves <strong><u>aligning preoperative medical images</u></strong> (such as CT scans or MRI scans) with the <strong><u>patient's actual anatomy</u></strong> during surgery. This allows surgeons to <strong><u>visualize the patient's internal structures</u></strong>, <strong><u>plan surgical procedures more accurately</u></strong>, and navigate in real-time during surgery. Image-guided surgery improves <strong><u>precision</u></strong> and <u>**reduces the invasiveness of procedures**</u>.</li> <li><strong>Radiation Therapy</strong>: In radiation therapy for <strong><u>cancer treatment</u></strong>, image registration is crucial. It involves <strong><u>registering a patient's diagnostic images with images taken during the treatment</u></strong> (such as cone-beam CT scans). This ensures that <strong><u>radiation beams precisely target the tumor while sparing healthy tissues</u></strong>. Accurate registration is essential to <strong><u>maximize treatment effectiveness</u></strong> and <strong><u>minimize side effects</u></strong>.</li> </ol> </li> <li> <p>Handle <u>open data sets</u></p> <p>Handling open datasets involves <strong><u>working with publicly available datasets</u></strong> for research or educational purposes. Hereâ€™s how you can handle open datasets:</p> <ol> <li><strong>Data Access</strong>: <strong><u>Find open datasets relevant to your research or learning objectives</u></strong>. These datasets are often available through websites, data repositories, or research organizations. Ensure that you have the necessary permissions to access and use the data.</li> <li><strong>Data Download</strong>: <strong><u>Download the datasets</u></strong> following the provided guidelines and terms of use. Some datasets may require registration or citation, so make sure to adhere to any specific requirements.</li> <li><strong>Data Preprocessing</strong>: Depending on the dataset, you may need to <strong><u>preprocess the data, clean it, and format it for your specific analysis or research goals</u></strong>. This step might involve data cleaning, normalization, and transformation.</li> <li><strong>Data Analysis</strong>: <strong><u>Perform the desired data analysis</u></strong>, which could include image registration, image analysis, or other tasks related to your research or learning objectives. Ensure you follow good data analysis practices and documentation.</li> <li><strong>Ethical Considerations</strong>: <strong><u>Always handle open datasets with ethical considerations</u></strong>. Respect privacy, follow legal regulati</li> </ol> </li> </ul> <h4 id="regularization">Regularization</h4> <p><strong>Terms</strong></p> <p><strong>Fixed Image</strong>:</p> <ul> <li>The â€œfixedâ€ image is the <strong>reference image</strong> or the image that <strong>serves as the target</strong> for the registration process.</li> <li>Itâ€™s the image <strong>you want the â€œmovingâ€ image to be aligned</strong> with or transformed to match.</li> <li>In medical image analysis, the fixed image is often an anatomical image, such as a CT scan or an MRI scan, which provides the reference structure for alignment.</li> </ul> <p><strong>Moving Image</strong>:</p> <ul> <li>The â€œmovingâ€ image is the image that <strong>you want to align with the fixed image</strong>.</li> <li>Itâ€™s the image that <strong>undergoes a transformation</strong> to bring it into spatial alignment with the fixed image.</li> <li>In medical image analysis, the moving image could be another image of the same patient acquired at a different time or with a different modality.</li> </ul> <p>The goal of image registration is to <strong>find a transformation</strong> that can <strong>map or align</strong> the <strong>moving image to match the fixed image</strong> as closely as possible. This process is commonly used in fields like medical image analysis to compare images taken at different times, with different modalities, or for other clinical or research purposes. The fixed and moving images help <strong>establish a spatial relationship between different images, enabling comparisons, analyses, and clinical decision-making</strong>.</p> <p><strong>Deformation</strong> refers to the process of <strong>changing the <u>shape</u>, <u>size</u>, or <u>spatial arrangement</u> of an object or a structure</strong>. In the context of image registration, deformation typically involves <strong><u>altering the spatial configuration of an image or a part of an image</u></strong> to bring it into <strong><u>alignment with another image</u></strong>. Deformation can be applied to individual pixels or voxels within an image to achieve this alignment. Itâ€™s a fundamental concept in image registration because <strong><u>it describes how one image is modified to match the other</u></strong>.</p> <p><strong>Transformation</strong>, on the other hand, is a broader term that <strong><u>encompasses any mathematical operation</u></strong> applied to an image to change its position, orientation, scale, or shape. A transformation can be rigid (only involving translation and rotation), affine (involving translation, rotation, scaling, and shearing), or non-rigid (involving deformation). Transformation is a <strong><u>more general term</u></strong> that includes deformation as a specific case.</p> <p><strong>Deformed Moving</strong> in the context of image registration means that the <strong>â€œmovingâ€ image, which was originally not aligned with the â€œfixedâ€ image</strong>, has undergone a deformation or transformation to achieve spatial alignment with the fixed image. This term indicates that <strong>the moving image has been modified or adjusted to fit or match the spatial characteristics of the fixed image</strong>. Deforming the moving image is a key step in the registration process, where the transformation is applied to bring the two images into spatial concordance.</p> <p>Here we have an example,</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/0MY6HKQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 1. Fixed (reference) and moving (ongoing) images </div> <p>Here is the <strong>deformed moving</strong> image.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/QQqviCR.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 2. Deformed moving image </div> <p><strong><em>Is this a good transformation field?</em></strong></p> <p><strong>Definitely not</strong>. There are a lot of <strong>strange internal distortions</strong>. The transformation field might not be ideal or that the registration process did not achieve the desired result.</p> <p><strong><em>Solution</em></strong></p> <p><strong>Regularization</strong></p> <ul> <li>We make the algorithm <strong>pay the price</strong> for <strong>too much deformations</strong></li> <li><strong>Price</strong> should <strong>grow nonlinearly</strong> (better <strong>move many a little bit</strong> than <strong>one a lot</strong>)</li> </ul> <p>This phrase is explaining the <strong>rationale behind the nonlinear growth of the cost</strong>. It suggests that itâ€™s <strong>more desirable to apply small deformations to multiple regions</strong> of an image rather than <strong>applying a very significant deformation to just one region</strong>. This is often because <strong>small deformations are more likely to preserve the overall structure and quality</strong> of the image.</p> <p>To put it simply, in image registration with regularization, the <strong>cost associated with deformation should encourage small, gradual changes across the entire image</strong> rather than allowing or favoring large, abrupt changes in just one area. The idea is to <strong>ensure that the transformation or deformation is smooth and physically meaningful</strong>, which can lead to <strong>better registration results</strong> and <strong>more realistic alignments</strong>. This approach helps <strong>prevent unrealistic distortions or artifacts</strong> in the transformed image.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/VQqGWET.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 3. Definition of points </div> <p>The idea behind is that <strong>we would like to preserve distances between</strong>:</p> <p>Â· Point $i, j$</p> <p>Â· Point from $\Omega_{i, j}$</p> <p>And we would like to <strong><u>preserve distances between points</u></strong> in original and deformed grid</p> <p>The goal of â€œ<strong>preserving distances between points</strong>â€ is to ensure that, <strong>after the deformation</strong>, the <strong><u>distances or spatial relationships between points on the deformed grid remain as close as possible to the distances between the corresponding points on the original grid</u></strong>. In other words:</p> <ul> <li>If <strong><u>point A is originally a certain distance away from point B on the original grid</u></strong>, we want to ensure that <strong><u>after deformation</u></strong>, <strong><u>point A remains a similar distance away from point B on the deformed grid</u></strong>.</li> <li>This preservation of distances helps <strong><u>maintain the local structure and spatial relationships in the image</u></strong>. It ensures that important <strong><u>anatomical or structural features</u></strong> in the image <strong><u>do not get distorted or altered</u></strong> significantly during the registration process.</li> </ul> <p>Overall, this concept is fundamental in regularization during image registration. It contributes to the smoothness and physical plausibility of the deformation, preventing unrealistic warping and ensuring that the registered image accurately reflects the spatial characteristics of the original image.</p> <p><strong>Problem formulation</strong></p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/P3XM5Gr.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 4. Formulation of problem </div> <p><strong>Original optimization formulation:</strong></p> \[F(I, J, \theta) = \min\sum_{i, j}d\left(I(i, j) - J(x(i,j,\theta), y(i, j, \theta))\right)\] <p><strong>Formulation with regularization:</strong></p> \[F(I, J, \theta) = \min\left(\sum_{i, j}d(I(i, j) - J(x(i, j, \theta), y(i, j, \theta))\right) \\ + \lambda\sum_{i,j}\sum_{k, l\in\Omega_{i, j}}\left(((i-k)\ - (x(i, j, \theta) - x(k, l, \theta))\right)^2\] <p>We add a regularization term to <strong>give a penalty</strong>. And what will this regularization do?</p> \[\sum_{i, j} \sum_{k.l\in\Omega_{i, j}}(\left(1-sign\left((i-k)\cdot(x(i, j, \theta) - x(k, l, \theta))\right)\right) + \left(1-sign\left((j-l)\cdot(y(i, j, \theta)-y(k, l, \theta))\right)\right))\] <p>This regularization term is used to <strong><u>encourage the preservation of distances or relationships</u></strong> between neighboring points in the image grid during the registration process. It <strong><u>penalizes situations</u></strong> where the <strong><u>distances between points change significantly</u></strong> as a result of the deformation caused by the transformation parameter $\theta$.</p> <p>The term 1âˆ’sign(â€¦) essentially <strong><u>imposes a penalty</u></strong> when the transformation causes <strong><u>significant changes in the spatial relationships between points</u></strong>. This encourages the registration process to <strong><u>produce a deformation that maintains the local structure and spatial characteristics of the image</u></strong>.</p> <p>The â€œfoldingâ€ you mentioned likely refers to the effect of this regularization term in <strong><u>preventing excessive deformations that could distort or fold the image</u></strong>. The regularization term <strong><u>discourages such deformations</u></strong>, promoting smoother and more physically plausible transformations.</p> <p>In summary, this regularization term helps <strong><u>maintain the spatial relationships between neighboring points in the image grid</u></strong>, preventing excessive distortion during image registration.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/LbWQENU.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 5. Regularization of this term </div> <p>And here, by using regularization, we get a better deformed result.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/tiWJyvY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 6. Regularized result </div> <h4 id="landmarks">Landmarks</h4> <p>Can we help registration of two squares?</p> <ul> <li> <p>Automatically detect square corners</p> <p>This step involves the automatic identification of corners or key points on the square objects in the images. These corners are distinctive features that can be used as landmarks for registration.</p> </li> <li> <p>Force registration to align corners</p> <ul> <li>This means using the detected square corners as constraints during the registration process. The registration algorithm will be instructed to ensure that the corners of the squares align precisely or as closely as possible.</li> <li>By â€œforcingâ€ the registration to align corners, you are making it a priority to match these specific, distinctive points. This can be especially useful when you have prior knowledge of where these corners should be located in the registered image.</li> </ul> </li> </ul> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/fb1LYL8.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 7. Landmarks </div> <p>Then we have new optimization function.</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UzjYqzQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 8. New optimization function </div> <p>The use of landmarks is a common technique in image registration, and itâ€™s particularly valuable when you have objects with distinctive and easily detectable features, such as square corners. By aligning these landmarks, you can improve the registration accuracy and ensure that specific points of interest are correctly positioned in the registered image.</p> <p><strong>How to ensure the landmarks match?</strong> Validate the results by checking how closely the landmarks in the registered images match. You can calculate and evaluate the distances between corresponding landmarks to assess the quality of registration.</p> <h4 id="summary-algorithm">Summary Algorithm</h4> <ul> <li><strong>Define algorithm settings</strong> <ul> <li>Acceptable transformations (Rigid, Affine, Non-rigid)</li> <li>Similarity measure (Mean Squares, Correlation, Mutual Information)</li> <li>Multi-resolution pyramid, grid schedule</li> <li>Regularization</li> </ul> </li> <li> <p><strong>Prepare input images</strong></p> <ul> <li>Normalization of intensities</li> <li>Rescaling</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li>Compute similarity measure over <strong>grids on both images</strong></li> <li>Compute <strong>gradients on grid</strong> on moving image</li> <li>Move <strong>grid according to the gradients</strong></li> <li><strong>Deform</strong> moving image according to the <strong>grid</strong></li> <li>Use splines to <strong>interpolate</strong> pixels outside the grid</li> </ul> </li> <li>Repeat until convergence</li> </ul> <h4 id="applications">Applications</h4> <h5 id="atlas-based-segmentation">Atlas-based segmentation</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/haDNA69.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 9. Atlas-based segmentation </div> <ul> <li> <p>Advantages:</p> <ul> <li> <p>Anatomically correct segmentation</p> <p>è¿™æ„å‘³ç€ä½¿ç”¨åŸºäºå‚è€ƒå›¾è°±çš„åˆ†å‰²æ–¹æ³•å¯ä»¥è·å¾—è§£å‰–å­¦ä¸Šæ­£ç¡®çš„åˆ†å‰²ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•å‚è€ƒäº†å·²çŸ¥çš„è§£å‰–ç»“æ„ä¿¡æ¯ï¼Œå› æ­¤åˆ†å‰²ç»“æœæ›´æœ‰å¯èƒ½å‡†ç¡®åœ°åŒ¹é…åˆ°å…·ä½“çš„è§£å‰–ç»“æ„ï¼Œå¦‚å™¨å®˜ã€ç»„ç»‡æˆ–åŒºåŸŸã€‚è¿™æœ‰åŠ©äºæé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œè¿™å¯¹äºè¯Šæ–­å’Œç ”ç©¶éå¸¸é‡è¦ã€‚</p> </li> <li> <p>Light training image requirements</p> <p>ä¸æŸäº›å…¶ä»–åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå‚è€ƒå›¾è°±çš„åˆ†å‰²é€šå¸¸éœ€è¦è¾ƒå°‘çš„è®­ç»ƒå›¾åƒã€‚è¿™æ„å‘³ç€ä½ ä¸å¿…æ‹¥æœ‰å¤§é‡çš„æ ‡è®°æ•°æ®æ¥è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚ç›¸åï¼Œä½ å¯ä»¥ä»…ä½¿ç”¨ä¸€ä¸ªæˆ–å‡ ä¸ªé«˜è´¨é‡çš„å‚è€ƒå›¾è°±ï¼Œç„¶åå°†å…¶åº”ç”¨äºå…¶ä»–å›¾åƒï¼Œä»è€Œé™ä½äº†è®­ç»ƒæ•°æ®çš„éœ€æ±‚å’Œæ•°æ®æ ‡è®°çš„å·¥ä½œé‡ã€‚</p> </li> <li> <p>Invariant to the number of target objects</p> <p>è¿™æ„å‘³ç€è¿™ç§æ–¹æ³•çš„æ€§èƒ½ä¸å—éœ€è¦åˆ†å‰²çš„ç›®æ ‡å¯¹è±¡æ•°é‡çš„å½±å“ã€‚ä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å‚è€ƒå›¾è°±å’Œç®—æ³•æ¥åˆ†å‰²ä¸åŒæ•°é‡çš„ç›®æ ‡å¯¹è±¡ï¼Œè€Œä¸éœ€è¦æ ¹æ®ç›®æ ‡æ•°é‡è¿›è¡Œè°ƒæ•´æˆ–é‡æ–°è®­ç»ƒã€‚è¿™ç§ä¸å˜æ€§å¯¹äºå¤„ç†ä¸åŒå›¾åƒæˆ–åœºæ™¯ä¸­çš„å¤šä¸ªç›®æ ‡å¯¹è±¡éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå‡è½»äº†å®šåˆ¶å’Œè°ƒæ•´çš„å·¥ä½œã€‚</p> </li> </ul> </li> <li> <p>Disadvantages:</p> <ul> <li> <p>Slow</p> <p><strong>è®¡ç®—å¤æ‚æ€§</strong>ï¼šåŸºäºå‚è€ƒå›¾è°±çš„åˆ†å‰²æ–¹æ³•é€šå¸¸æ¶‰åŠè®¡ç®—å›¾åƒä¹‹é—´çš„é…å‡†æˆ–å¯¹å‡†ã€‚è¿™åŒ…æ‹¬è®¡ç®—å˜æ¢åœºï¼ˆdeformation fieldï¼‰ä»¥å°†å‚è€ƒå›¾è°±ä¸ç›®æ ‡å›¾åƒå¯¹é½ã€‚è¿™äº›è®¡ç®—åœ¨åƒç´ çº§åˆ«è¿›è¡Œï¼Œç‰¹åˆ«æ˜¯åœ¨éåˆšæ€§ï¼ˆnon-rigidï¼‰å˜æ¢çš„æƒ…å†µä¸‹ï¼Œéœ€è¦å¤§é‡è®¡ç®—ã€‚è¿™ä¼šå¯¼è‡´ç®—æ³•çš„å¤æ‚æ€§å’Œè®¡ç®—æ—¶é—´çš„å¢åŠ ã€‚</p> <p><strong>å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”</strong>ï¼šè™½ç„¶å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”å¯ä»¥æé«˜é…å‡†çš„é²æ£’æ€§ï¼Œä½†ä¹Ÿä¼šå¢åŠ è®¡ç®—çš„å¤æ‚æ€§ã€‚å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”éœ€è¦åœ¨ä¸åŒåˆ†è¾¨ç‡çº§åˆ«ä¸Šæ‰§è¡Œé…å‡†ï¼Œæ¯ä¸ªçº§åˆ«éƒ½éœ€è¦è®¡ç®—å˜æ¢åœºã€‚è¿™æ„å‘³ç€å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”æ–¹æ³•å¯èƒ½éœ€è¦æ›´å¤šçš„è®¡ç®—æ—¶é—´ã€‚</p> <p><strong>æ­£åˆ™åŒ–</strong>ï¼šæ­£åˆ™åŒ–ç”¨äºå¹³æ»‘å˜æ¢åœºä»¥ç¡®ä¿ç‰©ç†ä¸Šåˆç†çš„å½¢å˜ï¼Œä½†å®ƒä¹Ÿä¼šå¼•å…¥é¢å¤–çš„è®¡ç®—ã€‚æ­£åˆ™åŒ–é€šå¸¸éœ€è¦è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™ä¼šå¢åŠ è®¡ç®—æ—¶é—´ã€‚</p> <p><strong>ç¡¬ä»¶é™åˆ¶</strong>ï¼šå›¾åƒåˆ†å‰²é€šå¸¸éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æˆ–å¤§è§„æ¨¡æ•°æ®é›†çš„æƒ…å†µä¸‹ã€‚å¦‚æœè®¡ç®—èµ„æºå—é™ï¼Œé€Ÿåº¦å¯èƒ½ä¼šå—åˆ°é™åˆ¶ã€‚</p> </li> </ul> </li> </ul> <p>See details about <a href="https://liuying-1.github.io/blog/2023/image-registration-l1/">Atlas-based Segmentation</a> :point_left:</p> <h5 id="multi-modal-image-registration">Multi-modal image registration</h5> <p>Improves <strong>visibility of structures</strong> (bones â€“ CT, soft tissues â€“ MR, tumors â€“ PET and MR)</p> <p>â€œMulti-modal image registrationâ€ æŒ‡çš„æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼ŒåŒæ—¶æ³¨å†Œæˆ–é…å‡†ä¸åŒæ¨¡æ€ï¼ˆå¤šæ¨¡æ€ï¼‰çš„åŒ»å­¦å›¾åƒã€‚<strong><u>æ¯ç§æ¨¡æ€</u></strong>çš„åŒ»å­¦å›¾åƒï¼ˆå¦‚CTæ‰«æã€MRIæ‰«æã€PETæ‰«æç­‰ï¼‰<strong><u>å¯¹äººä½“ç»“æ„å’Œç—…å˜æœ‰ä¸åŒçš„ç°åº¦å€¼ã€å¯¹æ¯”åº¦å’Œä¿¡æ¯</u></strong>ã€‚â€Improves visibility of structuresâ€ çš„æ„æ€æ˜¯<strong><u>å¤šæ¨¡æ€å›¾åƒé…å‡†å¯ä»¥æ”¹å–„ä¸åŒæ¨¡æ€ä¸‹çš„ç»“æ„å¯è§†æ€§</u></strong>ã€‚</p> <p>å…·ä½“æ¥è¯´ï¼Œè¿™äº›ä¸åŒæ¨¡æ€çš„å›¾åƒå¯èƒ½æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/SBq4Pxv.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 10. Soft-tissues </div> <ol> <li> <p><strong>CTå›¾åƒ</strong>ï¼šåœ¨CTæ‰«æä¸­ï¼Œ<strong><u>éª¨éª¼ç»“æ„</u></strong>é€šå¸¸æ˜¾ç¤ºå¾—å¾ˆæ¸…æ™°ï¼Œå› ä¸ºå®ƒä»¬æœ‰é«˜å¯¹æ¯”åº¦ã€‚ç„¶è€Œï¼Œå¯¹è½¯ç»„ç»‡çš„å¯è§†æ€§å¯èƒ½è¾ƒå·®ã€‚</p> </li> <li> <p><strong>MRå›¾åƒ</strong>ï¼šMRIå›¾åƒå¯¹äºæ˜¾ç¤º<strong><u>è½¯ç»„ç»‡ç»“æ„</u></strong>éå¸¸å‡ºè‰²ï¼Œä½†å¯¹äºéª¨éª¼ç»“æ„çš„å¯è§†æ€§å¯èƒ½ä¸å¦‚CTã€‚</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/HKTrqHQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 11. Bones </div> </li> <li> <p><strong>PETå›¾åƒ</strong>ï¼šPETæ‰«æç”¨äºæ£€æµ‹ä»£è°¢æ´»è·ƒæ€§ï¼Œé€šå¸¸ç”¨äº<strong><u>è‚¿ç˜¤è¯Šæ–­</u></strong>ã€‚PETå›¾åƒå¯ä»¥æ˜¾ç¤º<strong><u>è‚¿ç˜¤</u></strong>å’Œå…¶ä»–å¼‚å¸¸çš„ç”Ÿç‰©æ´»åŠ¨ï¼Œä½†å¯¹è§£å‰–ç»“æ„çš„ç»†èŠ‚å¯è§†æ€§ç›¸å¯¹è¾ƒä½ã€‚</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/VM8awXw.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 12. Tumors </div> </li> </ol> <p>â€œMulti-modal image registrationâ€ å…è®¸å°†è¿™äº›ä¸åŒæ¨¡æ€çš„å›¾åƒè¿›è¡Œé…å‡†ï¼Œä»¥ä¾¿åœ¨åŒä¸€åæ ‡ç³»ä¸‹å¯¹æ¯”å®ƒä»¬ã€‚è¿™æ ·åšæœ‰ä»¥ä¸‹å¥½å¤„ï¼š</p> <ul> <li><strong>æ”¹å–„å¯è§†æ€§</strong>ï¼šé€šè¿‡å°†ä¸åŒæ¨¡æ€çš„å›¾åƒè¿›è¡Œé…å‡†ï¼Œä½ å¯ä»¥åœ¨ä¸€ä¸ªå›¾åƒä¸­åŒæ—¶æ˜¾ç¤ºä¸åŒæ¨¡æ€ä¸‹çš„ç»“æ„ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨åŒä¸€å›¾åƒä¸­æ¸…æ™°åœ°çœ‹åˆ°éª¨éª¼ï¼ˆæ¥è‡ªCTå›¾åƒï¼‰ã€è½¯ç»„ç»‡ï¼ˆæ¥è‡ªMRå›¾åƒï¼‰å’Œè‚¿ç˜¤æ´»æ€§ï¼ˆæ¥è‡ªPETå›¾åƒï¼‰ï¼Œä»è€Œè·å¾—æ›´å…¨é¢çš„ä¿¡æ¯ã€‚</li> <li><strong>æŒ‡å¯¼è¯Šæ–­å’Œæ²»ç–—</strong>ï¼šå¤šæ¨¡æ€é…å‡†å¯å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç—…ç—‡ã€è§„åˆ’æ‰‹æœ¯å’Œç›‘æµ‹æ²»ç–—è¿›å±•ã€‚åŒ»ç”Ÿå¯ä»¥åœ¨ä¸€ä¸ªå›¾åƒä¸ŠåŒæ—¶æŸ¥çœ‹å¤šä¸ªä¿¡æ¯æ¥æºï¼Œæœ‰åŠ©äºæ›´å…¨é¢åœ°äº†è§£æ‚£è€…çš„æƒ…å†µã€‚</li> </ul> <p>æ€»ä¹‹ï¼Œå¤šæ¨¡æ€å›¾åƒé…å‡†æœ‰åŠ©äºæé«˜åŒ»å­¦å›¾åƒçš„å¯è§†æ€§ï¼Œå¢åŠ ä¿¡æ¯çš„èåˆï¼Œä»è€Œæ›´å¥½åœ°æ”¯æŒåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—è¿‡ç¨‹ã€‚</p> <p>å¤šæ¨¡æ€å›¾åƒé…å‡†æ˜¯å°†æ¥è‡ªä¸åŒå›¾åƒæ¨¡æ€çš„å›¾åƒè¿›è¡Œå¯¹é½ï¼Œä»¥ä½¿å®ƒä»¬åœ¨åŒä¸€åæ ‡ç³»ä¸‹å¯¹åº”åˆ°ç›¸åŒçš„è§£å‰–ç»“æ„æˆ–åŒºåŸŸã€‚è¿™ç§é…å‡†çš„ç›®çš„æ˜¯å…è®¸åŒ»ç”Ÿæˆ–ç ”ç©¶äººå‘˜åŒæ—¶æŸ¥çœ‹ä¸åŒæ¨¡æ€ä¸‹çš„å›¾åƒä¿¡æ¯ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£æ‚£è€…çš„æƒ…å†µã€‚ä»¥ä¸‹æ˜¯å¤šæ¨¡æ€å›¾åƒé…å‡†çš„ä¸€èˆ¬æµç¨‹ï¼š</p> <ol> <li><strong>å›¾åƒè·å–</strong>ï¼šé¦–å…ˆï¼Œä¸åŒæ¨¡æ€çš„å›¾åƒï¼ˆä¾‹å¦‚CTã€MRIã€PETç­‰ï¼‰å¿…é¡»åˆ†åˆ«è·å–ã€‚è¿™äº›å›¾åƒé€šå¸¸åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šè·å¾—ï¼Œå…·æœ‰ä¸åŒçš„å¯¹æ¯”åº¦å’Œä¿¡æ¯ã€‚</li> <li><strong>å›¾åƒé¢„å¤„ç†</strong>ï¼šåœ¨è¿›è¡Œé…å‡†ä¹‹å‰ï¼Œé€šå¸¸éœ€è¦å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å»å™ªã€å¢å¼ºã€å›¾åƒæ ¡æ­£ç­‰æ“ä½œï¼Œä»¥ç¡®ä¿å®ƒä»¬å¤„äºæœ€ä½³çŠ¶æ€ã€‚</li> <li><strong>ç‰¹å¾æå–</strong>ï¼šä»æ¯ç§æ¨¡æ€çš„å›¾åƒä¸­<strong><u>æå–ç‰¹å¾</u></strong>ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æ˜¯è§£å‰–ç»“æ„ã€æ ‡è®°ç‚¹æˆ–å…¶ä»–åœ¨ä¸åŒæ¨¡æ€ä¸‹å®¹æ˜“è¯†åˆ«çš„å›¾åƒç‰¹å¾ã€‚ç‰¹å¾æå–æ˜¯å…³é”®çš„ä¸€æ­¥ï¼Œå› ä¸º<strong><u>å®ƒç¡®å®šäº†é…å‡†è¿‡ç¨‹ä¸­ç”¨äºå¯¹é½çš„å…³é”®ç‚¹</u></strong>ã€‚</li> <li><strong>ç‰¹å¾åŒ¹é…</strong>ï¼šåœ¨ä¸åŒæ¨¡æ€å›¾åƒä¸­æå–çš„ç‰¹å¾è¿›è¡ŒåŒ¹é…ã€‚è¿™å¯ä»¥é€šè¿‡å„ç§è®¡ç®—æœºè§†è§‰ç®—æ³•å®ç°ï¼Œé€šå¸¸æ˜¯åœ¨é…å‡†ç®—æ³•ä¸­çš„ä¸€ä¸ªæ­¥éª¤ã€‚ç‰¹å¾åŒ¹é…çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ç›¸åº”çš„ç‰¹å¾ç‚¹ï¼Œä»¥ä¾¿è¿›è¡Œé…å‡†ã€‚</li> <li><strong>é…å‡†å˜æ¢</strong>ï¼š<strong><u>åŸºäºæ‰¾åˆ°çš„ç‰¹å¾åŒ¹é…ï¼Œè®¡ç®—éœ€è¦åº”ç”¨äºå…¶ä¸­ä¸€ä¸ªå›¾åƒä»¥ä½¿å…¶ä¸å¦ä¸€ä¸ªå›¾åƒå¯¹é½çš„å˜æ¢</u></strong>ã€‚è¿™å¯ä»¥æ˜¯åˆšæ€§ã€ä»¿å°„æˆ–éåˆšæ€§å˜æ¢ï¼Œå…·ä½“å–å†³äºå›¾åƒçš„ç±»å‹å’Œæ‰€éœ€çš„å¯¹é½ç¨‹åº¦ã€‚</li> <li><strong>é…å‡†ä¼˜åŒ–</strong>ï¼šè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿å˜æ¢èƒ½å¤Ÿæœ€å¥½åœ°å¯¹é½å›¾åƒã€‚è¿™åŒ…æ‹¬ä¼˜åŒ–å˜æ¢å‚æ•°ä»¥æœ€å°åŒ–ç‰¹å¾ç‚¹ä¹‹é—´çš„è·ç¦»æˆ–åŒ¹é…è¯¯å·®ã€‚</li> <li><strong>ç”Ÿæˆé…å‡†åçš„å›¾åƒ</strong>ï¼šåº”ç”¨é…å‡†å˜æ¢ï¼Œå°†ä¸åŒæ¨¡æ€çš„å›¾åƒå¯¹é½åˆ°åŒä¸€åæ ‡ç³»ä¸‹ï¼Œä»è€Œç”Ÿæˆé…å‡†åçš„å›¾åƒã€‚è¿™äº›å›¾åƒå¯ä»¥åŒæ—¶æŸ¥çœ‹ï¼Œä»¥è·å¾—æ›´å¤šä¿¡æ¯ã€‚</li> </ol> <p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤šæ¨¡æ€å›¾åƒé…å‡†æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå…¶å¤æ‚æ€§å–å†³äºå›¾åƒçš„ç‰¹æ€§å’Œæ‰€ä½¿ç”¨çš„é…å‡†ç®—æ³•ã€‚å®ƒé€šå¸¸éœ€è¦è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡çš„é…å‡†ç»“æœã€‚è¿™ç§é…å‡†æ–¹æ³•åœ¨åŒ»å­¦å½±åƒå­¦ã€åŒ»å­¦è¯Šæ–­å’Œç ”ç©¶ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚</p> <h5 id="atlas-based-abnormality-detection">Atlas-based abnormality detection</h5> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/wFbSCgB.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 13. Abnormality detection </div> <ul> <li>Register <strong><u>healthy</u></strong> image to <strong><u>potentially abnormal</u></strong></li> <li>Study deformation field to <strong><u>find abnormalities</u></strong></li> </ul> <p>â€œAtlas-based abnormality detectionâ€ æ˜¯ä¸€ç§ç”¨äºæ£€æµ‹åŒ»å­¦å›¾åƒä¸­å¼‚å¸¸æˆ–ç—…å˜çš„æ–¹æ³•ï¼Œå®ƒåŸºäºå·²çŸ¥çš„æ­£å¸¸è§£å‰–ç»“æ„çš„å›¾è°±ï¼ˆatlasï¼‰æ¥è¯†åˆ«å›¾åƒä¸­çš„å¼‚å¸¸æƒ…å†µã€‚</p> <ol> <li><strong>å›¾è°±ï¼ˆAtlasï¼‰</strong>ï¼šå›¾è°±æ˜¯ä¸€ç»„å·²çŸ¥æ­£å¸¸è§£å‰–ç»“æ„çš„å‚è€ƒå›¾åƒã€‚è¿™äº›å›¾åƒé€šå¸¸æ¥è‡ªå¤§é‡çš„<strong><u>å¥åº·æ‚£è€…</u></strong>ï¼Œç”¨äºè¡¨ç¤º<strong><u>æ­£å¸¸çš„ç”Ÿç†ç»“æ„</u></strong>å’Œå™¨å®˜ã€‚å›¾è°±å¯ä»¥åŒ…æ‹¬å„ç§æ¨¡æ€çš„å›¾åƒï¼Œå¦‚CTã€MRIç­‰ã€‚</li> <li><strong>å¼‚å¸¸æ£€æµ‹</strong>ï¼šåœ¨åŒ»å­¦å›¾åƒä¸­ï¼Œå¼‚å¸¸é€šå¸¸æ˜¯æŒ‡ä¸æ­£å¸¸è§£å‰–ç»“æ„ä¸åŒçš„åŒºåŸŸï¼Œå¯èƒ½æ˜¯ç–¾ç—…ã€æŸä¼¤æˆ–å…¶ä»–å¼‚å¸¸æƒ…å†µçš„è¿¹è±¡ã€‚å¼‚å¸¸å¯ä»¥è¡¨ç°ä¸ºå¼‚å¸¸çš„å½¢çŠ¶ã€å¯†åº¦ã€äº®åº¦æˆ–å…¶ä»–ç‰¹å¾ã€‚</li> <li><strong>é…å‡†ä¸å¯¹æ¯”</strong>ï¼šåœ¨è¿›è¡Œå¼‚å¸¸æ£€æµ‹æ—¶ï¼Œé¦–å…ˆå°†<strong><u>å¾…æ£€æµ‹å›¾åƒï¼ˆå¯èƒ½æ˜¯æ‚£è€…çš„å›¾åƒï¼‰ä¸æ­£å¸¸å›¾è°±</u></strong>è¿›è¡Œé…å‡†ã€‚è¿™æ„å‘³ç€å°†å¾…æ£€æµ‹å›¾åƒä¸­çš„ç»“æ„ä¸å›¾è°±ä¸­çš„å¯¹åº”ç»“æ„å¯¹é½ã€‚</li> <li><strong>ç‰¹å¾æå–</strong>ï¼šä¸€æ—¦å®Œæˆé…å‡†ï¼Œå°±å¯ä»¥ä»å¾…æ£€æµ‹å›¾åƒä¸­æå–ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾é€šå¸¸æ˜¯ä¸å¼‚å¸¸ç›¸å…³çš„ç‰¹å¾ï¼Œå¦‚åŒºåŸŸçš„å½¢çŠ¶ã€å¯†åº¦ã€çº¹ç†ç­‰ã€‚</li> <li><strong>å¼‚å¸¸æ£€æµ‹ç®—æ³•</strong>ï¼šä½¿ç”¨ç‰¹å¾å’Œå·²çŸ¥çš„æ­£å¸¸å›¾è°±ä½œä¸ºå‚è€ƒï¼Œå¼‚å¸¸æ£€æµ‹ç®—æ³•å¯ä»¥<strong><u>è¯†åˆ«å¾…æ£€æµ‹å›¾åƒä¸­ä¸æ­£å¸¸å›¾è°±ä¸åŒ¹é…</u></strong>çš„åŒºåŸŸã€‚è¿™äº›ä¸åŒ¹é…çš„åŒºåŸŸå¯èƒ½è¡¨ç¤ºå¼‚å¸¸æˆ–ç—…å˜ã€‚</li> <li><strong>å¯è§†åŒ–å’ŒæŠ¥å‘Š</strong>ï¼šæ£€æµ‹åˆ°çš„å¼‚å¸¸åŒºåŸŸå¯ä»¥ç”¨äºç”Ÿæˆå¯è§†åŒ–ç»“æœï¼Œä¾›åŒ»ç”Ÿæˆ–ç ”ç©¶äººå‘˜æŸ¥çœ‹ã€‚å¼‚å¸¸æ£€æµ‹çš„ç»“æœé€šå¸¸ä»¥å¯è§†åŒ–å›¾åƒæˆ–æŠ¥å‘Šçš„å½¢å¼å‘ˆç°ã€‚</li> </ol> <p>â€œAtlas-based abnormality detectionâ€ çš„ä¼˜åŠ¿åœ¨äºå®ƒä¾èµ–äºå·²çŸ¥çš„æ­£å¸¸å›¾è°±ä½œä¸ºå‚è€ƒï¼Œå› æ­¤å¯ä»¥å¸®åŠ©<strong><u>å¿«é€Ÿè¯†åˆ«æ½œåœ¨çš„å¼‚å¸¸åŒºåŸŸ</u></strong>ã€‚è¿™å¯¹äºåŒ»å­¦è¯Šæ–­ã€ç–¾ç—…ç­›æŸ¥å’Œç ”ç©¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæˆ–ç ”ç©¶äººå‘˜æ›´å¿«é€Ÿåœ°è¯†åˆ«å¼‚å¸¸ï¼Œè€Œä¸éœ€è¦æ‰‹åŠ¨æ£€æŸ¥æ•´ä¸ªå›¾åƒã€‚</p> <p>åœ¨ â€œAtlas-based abnormality detectionâ€ ä¸­ï¼Œé€šå¸¸é‡‡å–çš„æ–¹æ³•åŒ…æ‹¬ï¼š</p> <ol> <li><strong>å°†å¥åº·å›¾åƒé…å‡†åˆ°æ½œåœ¨å¼‚å¸¸å›¾åƒ</strong>ï¼šé¦–å…ˆï¼Œå°†ä¸€ä¸ªæˆ–å¤šä¸ªå¥åº·å›¾åƒï¼ˆæ­£å¸¸å›¾è°±ï¼‰ä¸æ½œåœ¨å¼‚å¸¸å›¾åƒè¿›è¡Œé…å‡†ï¼Œä»¥ä½¿å®ƒä»¬åœ¨ç›¸åŒçš„åæ ‡ç³»ä¸‹å¯¹é½ã€‚è¿™æ„å‘³ç€<strong><u>å°†å¥åº·å›¾åƒä¸­çš„æ­£å¸¸ç»“æ„ä¸æ½œåœ¨å¼‚å¸¸å›¾åƒå¯¹åº”çš„ç»“æ„å¯¹é½</u></strong>ã€‚</li> <li><strong>ç ”ç©¶å˜æ¢åœºä»¥å‘ç°å¼‚å¸¸</strong>ï¼š<strong><u>ä¸€æ—¦è¿›è¡Œäº†é…å‡†ï¼Œå°±å¯ä»¥ç ”ç©¶å˜æ¢åœº</u></strong>ï¼ˆdeformation fieldï¼‰ã€‚å˜æ¢åœºè¡¨ç¤º<strong><u>ä»å¥åº·å›¾åƒåˆ°æ½œåœ¨å¼‚å¸¸å›¾åƒçš„å˜æ¢</u></strong>ï¼Œå®ƒæè¿°äº†<strong><u>æ­£å¸¸ç»“æ„åœ¨æ½œåœ¨å¼‚å¸¸å›¾åƒä¸­çš„å½¢å˜</u></strong>ã€‚é€šè¿‡<strong><u>åˆ†æå˜æ¢åœº</u></strong>ï¼Œå¯ä»¥<strong><u>è¯†åˆ«å‡ºåœ¨æ½œåœ¨å¼‚å¸¸å›¾åƒä¸­ä¸æ­£å¸¸ç»“æ„ä¸åŒ¹é…</u></strong>çš„åŒºåŸŸã€‚</li> </ol> <p>è¿™ä¸ªæ–¹æ³•çš„æ€æƒ³æ˜¯ï¼Œé€šè¿‡å°†å¥åº·å›¾åƒä¸æ½œåœ¨å¼‚å¸¸å›¾åƒå¯¹é½ï¼Œå¯ä»¥å°†æ­£å¸¸ç»“æ„çš„ä½ç½®å’Œå½¢çŠ¶ä¿¡æ¯ä¼ é€’åˆ°æ½œåœ¨å¼‚å¸¸å›¾åƒä¸­ã€‚ç„¶åï¼Œé€šè¿‡æ¯”è¾ƒæ­£å¸¸ç»“æ„çš„æœŸæœ›ä½ç½®å’Œå®é™…ä½ç½®ï¼ˆé€šè¿‡å˜æ¢åœºè¡¨ç¤ºï¼‰ï¼Œå¯ä»¥æ£€æµ‹åˆ°å¼‚å¸¸æˆ–ç—…å˜åŒºåŸŸï¼Œå› ä¸ºè¿™äº›åŒºåŸŸå¯èƒ½ä¼šäº§ç”Ÿå¼‚å¸¸çš„å½¢å˜ã€‚</p> <p>è¿™ç§æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒä¸­ç”¨äºå¼‚å¸¸æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è‡ªåŠ¨è¯†åˆ«æ½œåœ¨å¼‚å¸¸æˆ–ç—…å˜çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/itVoHD4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 14. Small abnormalities </div> <ul> <li>Problem <ul> <li>Difficult to detect small abnormalities</li> </ul> </li> </ul> <p>â€œDifficult to detect small abnormalitiesâ€ æ„å‘³ç€åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŸºäºå›¾è°±çš„æ–¹æ³•æ¥æ£€æµ‹å°å°ºå¯¸å¼‚å¸¸æˆ–ç—…å˜å¯èƒ½é¢ä¸´æŒ‘æˆ˜ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºä»¥ä¸‹åŸå› ï¼š</p> <ol> <li><strong>ç©ºé—´åˆ†è¾¨ç‡é™åˆ¶</strong>ï¼šåŒ»å­¦å›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡æ˜¯æœ‰é™çš„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæŸäº›æˆæœ¬æ˜‚è´µçš„æˆåƒæŠ€æœ¯ã€‚å°å°ºå¯¸çš„å¼‚å¸¸å¯èƒ½åœ¨å›¾åƒä¸­å æ®çš„åƒç´ æ•°é‡å¾ˆå°‘ï¼Œå› æ­¤å¯èƒ½éš¾ä»¥åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸­å‡†ç¡®æ£€æµ‹ã€‚</li> <li><strong>å™ªå£°å’Œä¼ªå½±</strong>ï¼šåŒ»å­¦å›¾åƒå¯èƒ½å—åˆ°å™ªå£°å’Œä¼ªå½±çš„å½±å“ï¼Œè¿™äº›å› ç´ å¯ä»¥å¹²æ‰°å°å¼‚å¸¸çš„æ£€æµ‹ã€‚å™ªå£°å¯ä»¥å¯¼è‡´è™šå‡çš„å¼‚å¸¸æ£€æµ‹ï¼Œè€Œä¼ªå½±å¯èƒ½ä½¿å¼‚å¸¸åŒºåŸŸéš¾ä»¥åˆ†è¾¨ã€‚</li> <li><strong>ç‰¹å¾æå–çš„æŒ‘æˆ˜</strong>ï¼šæ£€æµ‹å°å¼‚å¸¸é€šå¸¸éœ€è¦æ›´é«˜çº§åˆ«çš„ç‰¹å¾æå–å’Œåˆ†æï¼Œä»¥ä¾¿æ•æ‰ç»†å¾®çš„å˜åŒ–ã€‚è¿™å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•å’Œæ›´å¤šçš„è®¡ç®—èµ„æºã€‚</li> </ol> <p>è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•å¯èƒ½åŒ…æ‹¬ï¼š</p> <ul> <li><strong>æé«˜å›¾åƒåˆ†è¾¨ç‡</strong>ï¼šä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„æˆåƒæŠ€æœ¯å¯ä»¥å¸®åŠ©æ›´å‡†ç¡®åœ°æ£€æµ‹å°å°ºå¯¸çš„å¼‚å¸¸ã€‚</li> <li><strong>é™ä½å™ªå£°</strong>ï¼šé‡‡å–å™ªå£°æŠ‘åˆ¶æŠ€æœ¯ï¼Œä»¥å‡å°‘å™ªå£°å¯¹å¼‚å¸¸æ£€æµ‹çš„å¹²æ‰°ã€‚</li> <li><strong>ä½¿ç”¨å¤šæ¨¡æ€ä¿¡æ¯</strong>ï¼šç»“åˆä¸åŒæ¨¡æ€çš„å›¾åƒä¿¡æ¯ï¼Œä»¥æé«˜å¯¹å°å¼‚å¸¸çš„æ£€æµ‹èƒ½åŠ›ã€‚</li> <li><strong>ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•</strong>ï¼šæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥å¸®åŠ©æ£€æµ‹å°å¼‚å¸¸ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥è‡ªåŠ¨å­¦ä¹ å›¾åƒä¸­çš„ç‰¹å¾ã€‚</li> </ul> <p>ç»¼åˆæ¥è¯´ï¼Œæ£€æµ‹å°å¼‚å¸¸æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†å¯ä»¥é€šè¿‡æ”¹è¿›å›¾åƒè´¨é‡ã€é‡‡ç”¨é«˜çº§åˆ†ææ–¹æ³•ä»¥åŠä½¿ç”¨å¤šç§ä¿¡æ¯æºæ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</p> <h5 id="super-resolution-images">Super-resolution images</h5> <p>We have three orthogonal 3D MR images of high in-plane resolution but high slice thickness. How to create a super-resolution 3D MR?</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/AfM6SVk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 15. Super-resolution </div> <p>åˆ›å»ºè¶…åˆ†è¾¨ç‡ 3D MR å›¾åƒé€šå¸¸æ¶‰åŠå°†ç°æœ‰çš„ä½åˆ†è¾¨ç‡å›¾åƒå¢å¼ºåˆ°é«˜åˆ†è¾¨ç‡å›¾åƒçš„è¿‡ç¨‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ æ‹¥æœ‰ä¸‰ä¸ªä¸åŒæ–¹å‘ï¼ˆxã€yã€zï¼‰çš„ 3D MR å›¾åƒï¼Œè¿™äº›å›¾åƒå…·æœ‰é«˜å¹³é¢åˆ†è¾¨ç‡ï¼Œä½†åœ¨åˆ‡ç‰‡ï¼ˆzè½´æ–¹å‘ï¼‰ä¸Šçš„åˆ†è¾¨ç‡è¾ƒä½ã€‚ä¸‹é¢æ˜¯ä¸€ç§å¯èƒ½çš„æ–¹æ³•æ¥åˆ›å»ºè¶…åˆ†è¾¨ç‡ 3D MR å›¾åƒï¼š</p> <ol> <li><strong>æ•°æ®é¢„å¤„ç†</strong>ï¼šé¦–å…ˆï¼Œå¯¹ä½ çš„ä¸‰ä¸ª 3D MR å›¾åƒè¿›è¡Œæ•°æ®é¢„å¤„ç†ã€‚è¿™åŒ…æ‹¬å»å™ªã€ä¼ªå½±å»é™¤ã€æ ¡æ­£ä»¥åŠç¡®ä¿å®ƒä»¬å·²ç»åœ¨ç›¸åŒçš„åæ ‡ç³»ä¸‹å¯¹é½ã€‚</li> <li><strong>åˆ‡ç‰‡æ’å€¼</strong>ï¼šåœ¨ 3D MR å›¾åƒçš„åˆ‡ç‰‡æ–¹å‘ä¸Šï¼Œç”±äºåˆ†è¾¨ç‡è¾ƒä½ï¼Œä½ å¯ä»¥ä½¿ç”¨æ’å€¼æ–¹æ³•æ¥å¢åŠ åˆ‡ç‰‡æ•°é‡ï¼Œä»è€Œæé«˜ z è½´åˆ†è¾¨ç‡ã€‚è¿™å¯ä»¥é‡‡ç”¨ä¸€ç»´æ’å€¼æ–¹æ³•ï¼Œä¾‹å¦‚çº¿æ€§æ’å€¼æˆ–æ ·æ¡æ’å€¼ã€‚è¿™å°†ç”Ÿæˆä¸€ç³»åˆ—æ–°çš„åˆ‡ç‰‡ï¼Œä»¥å¢åŠ åœ¨ z è½´æ–¹å‘ä¸Šçš„åˆ†è¾¨ç‡ã€‚</li> <li><strong>3D é‡å»º</strong>ï¼šä¸€æ—¦ä½ è·å¾—äº†é«˜åˆ†è¾¨ç‡åˆ‡ç‰‡ï¼Œå¯ä»¥å°†å®ƒä»¬ç»„åˆæˆ 3D æ•°æ®å·ã€‚è¿™å¯ä»¥é€šè¿‡é‡‡ç”¨ä½“ç»˜å›¾ï¼ˆVolume Renderingï¼‰æˆ–é€šè¿‡å †å è¿™äº›åˆ‡ç‰‡æ¥å®ç°ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªå…·æœ‰æ›´é«˜åˆ†è¾¨ç‡çš„ 3D MR å›¾åƒã€‚</li> <li><strong>è¶…åˆ†è¾¨ç‡æŠ€æœ¯</strong>ï¼šå¦‚æœä½ å¸Œæœ›è¿›ä¸€æ­¥å¢åŠ å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œå¯ä»¥ä½¿ç”¨è¶…åˆ†è¾¨ç‡æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯åŸºäºç»Ÿè®¡æ–¹æ³•ã€æ·±åº¦å­¦ä¹ æˆ–å…¶ä»–ç®—æ³•ï¼Œå¯ä»¥é€šè¿‡åˆ†æå›¾åƒçš„ç»†èŠ‚æ¥ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„ç‰ˆæœ¬ã€‚è¿™éœ€è¦å…·ä½“çš„è¶…åˆ†è¾¨ç‡ç®—æ³•ï¼Œå¹¶å¯èƒ½éœ€è¦ä½¿ç”¨é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚</li> <li><strong>è¯„ä¼°å’ŒéªŒè¯</strong>ï¼šæœ€åï¼Œä¸€å®šè¦è¯„ä¼°å’ŒéªŒè¯ç”Ÿæˆçš„è¶…åˆ†è¾¨ç‡å›¾åƒçš„è´¨é‡ã€‚è¿™å¯ä»¥é€šè¿‡ä¸é«˜åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œæˆ–è€…ä½¿ç”¨å›¾åƒè´¨é‡æŒ‡æ ‡æ¥è¿›è¡Œè¯„ä¼°ã€‚</li> </ol> <p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¶…åˆ†è¾¨ç‡å›¾åƒçš„è´¨é‡å–å†³äºæ•°æ®çš„è´¨é‡ã€ä½¿ç”¨çš„æ’å€¼å’Œé‡å»ºæ–¹æ³•ï¼Œä»¥åŠæ˜¯å¦ä½¿ç”¨äº†è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œé€šå¸¸éœ€è¦åœ¨åŒ»å­¦å½±åƒé¢†åŸŸå…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„äººå‘˜æ¥æ‰§è¡Œã€‚</p> <ul> <li>We convert orthogonal images to full volume but adding empty space</li> <li>This empty space should not be taken into account during registration</li> <li>Idea of masked registration</li> </ul> <p>åœ¨åˆ›å»ºè¶…åˆ†è¾¨ç‡ 3D MR å›¾åƒæ—¶ï¼Œå°†æ­£äº¤å›¾åƒè½¬æ¢ä¸ºå®Œæ•´çš„ä½“ç§¯å¹¶æ·»åŠ ç©ºç™½ç©ºé—´æ˜¯ä¸€ä¸ªå¸¸è§çš„æ–¹æ³•ã€‚è¿™ç§ç©ºç™½ç©ºé—´ä¸åº”è¯¥åœ¨å›¾åƒé…å‡†è¿‡ç¨‹ä¸­è€ƒè™‘åœ¨å†…ï¼Œå› ä¸ºå®ƒä»…æ˜¯ä¸ºäº†åœ¨ z è½´æ–¹å‘ä¸Šå¢åŠ åˆ†è¾¨ç‡è€Œæ·»åŠ çš„ã€‚</p> <p>ä»¥ä¸‹æ˜¯å…³äºå¦‚ä½•æ‰§è¡Œè¿™ä¸€è¿‡ç¨‹çš„ä¸€èˆ¬æ­¥éª¤ï¼š</p> <ol> <li><strong>åˆ›å»ºå®Œæ•´çš„ 3D MR ä½“ç§¯</strong>ï¼šå°†æ­£äº¤å›¾åƒï¼ˆé«˜åˆ†è¾¨ç‡ä½†åˆ‡ç‰‡åˆ†è¾¨ç‡è¾ƒä½ï¼‰è½¬æ¢ä¸ºå®Œæ•´çš„ 3D MR ä½“ç§¯ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨åˆ‡ç‰‡æ–¹å‘ä¸Šé‡å¤å›¾åƒæ¥å®ç°ï¼Œä»¥å¡«å……ç©ºç™½ç©ºé—´ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ 3D æ•°æ®å·ï¼ŒåŒ…æ‹¬åŸå§‹å›¾åƒå’Œæ·»åŠ çš„ç©ºç™½åˆ‡ç‰‡ã€‚</li> <li><strong>ç”Ÿæˆæ©æ¨¡ï¼ˆMaskï¼‰</strong>ï¼šä¸ºäº†ç¡®ä¿åœ¨å›¾åƒé…å‡†æœŸé—´ä¸è€ƒè™‘ç©ºç™½åˆ‡ç‰‡ï¼Œä½ å¯ä»¥ç”Ÿæˆä¸€ä¸ªæ©æ¨¡ï¼Œå…¶ä¸­ç©ºç™½åˆ‡ç‰‡è¢«æ ‡è®°ä¸ºä¸æ„Ÿå…´è¶£åŒºåŸŸã€‚æ©æ¨¡é€šå¸¸æ˜¯ä¸€ä¸ªä¸å›¾åƒä½“ç§¯å…·æœ‰ç›¸åŒå°ºå¯¸çš„ 3D æ•°æ®ï¼Œå…¶ä¸­æ­£å€¼è¡¨ç¤ºæ„Ÿå…´è¶£åŒºåŸŸï¼Œè´Ÿå€¼æˆ–é›¶è¡¨ç¤ºä¸æ„Ÿå…´è¶£åŒºåŸŸï¼ˆå³ç©ºç™½åˆ‡ç‰‡ï¼‰ã€‚</li> <li><strong>æ©æ¨¡é…å‡†</strong>ï¼šåœ¨è¿›è¡Œå›¾åƒé…å‡†æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨æ©æ¨¡æ¥æŒ‡å®šæ„Ÿå…´è¶£åŒºåŸŸã€‚è¿™æ„å‘³ç€åªæœ‰éç©ºç™½éƒ¨åˆ†çš„å›¾åƒå°†è¢«ç”¨äºé…å‡†ï¼Œè€Œç©ºç™½éƒ¨åˆ†å°†è¢«å¿½ç•¥ã€‚è¿™æœ‰åŠ©äºæé«˜é…å‡†çš„å‡†ç¡®æ€§ï¼Œå› ä¸ºé…å‡†ç®—æ³•å°†é›†ä¸­åœ¨åŒ…å«æœ‰ç”¨ä¿¡æ¯çš„éƒ¨åˆ†ã€‚</li> <li><strong>ç”Ÿæˆé«˜åˆ†è¾¨ç‡ 3D å›¾åƒ</strong>ï¼šä¸€æ—¦å®Œæˆé…å‡†ï¼Œå¯ä»¥ç”Ÿæˆå…·æœ‰æ›´é«˜ z è½´åˆ†è¾¨ç‡çš„ 3D MR å›¾åƒã€‚è¿™å¯ä»¥é€šè¿‡å°†æ­£äº¤å›¾åƒçš„ z è½´æ–¹å‘ä¸Šçš„é«˜åˆ†è¾¨ç‡ä¿¡æ¯ä¸æ©æ¨¡åº”ç”¨äºå·²é…å‡†çš„ä½“ç§¯æ¥å®ç°ã€‚</li> <li><strong>è¯„ä¼°å’ŒéªŒè¯</strong>ï¼šæœ€åï¼Œä¸€å®šè¦è¯„ä¼°å’ŒéªŒè¯ç”Ÿæˆçš„è¶…åˆ†è¾¨ç‡ 3D å›¾åƒçš„è´¨é‡ï¼Œä»¥ç¡®ä¿å®ƒæ»¡è¶³ä½ çš„éœ€æ±‚ã€‚</li> </ol> <p>è¿™ç§æ–¹æ³•å…è®¸ä½ åœ¨ z è½´æ–¹å‘ä¸Šå¢åŠ åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™äº† x å’Œ y è½´æ–¹å‘ä¸Šçš„é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚ä½¿ç”¨æ©æ¨¡æ¥æŒ‡å®šæ„Ÿå…´è¶£åŒºåŸŸæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æŠ€å·§ï¼Œä»¥ç¡®ä¿åªæœ‰æœ‰ç”¨çš„ä¿¡æ¯ç”¨äºå›¾åƒé…å‡†å’Œè¶…åˆ†è¾¨ç‡ç”Ÿæˆã€‚</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/GoKFaDW.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <div class="caption"> Figure 16. Super-resolution - 2 </div> <p>I didnâ€™t get the points here, and there are some other applications afterwards, but I am not going to explore them in detail, for instance, <strong>Adaptive radiotherapy planning</strong>, <strong>Landmark-based registration</strong>, and <strong>2D-3D registration</strong>.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Self-learning note for advanced image registration.]]></summary></entry><entry><title type="html">Image Registration Basics</title><link href="https://liuying-1.github.io/blog/2023/image-registration-l1/" rel="alternate" type="text/html" title="Image Registration Basics"/><published>2023-10-06T08:00:00+00:00</published><updated>2023-10-06T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/image-registration-l1</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/image-registration-l1/"><![CDATA[<p><strong><em>Disclaimer: All notes below are refered to the course â€œMedical Image Analysisâ€ delivered by UCPH.</em></strong></p> <p>So todayâ€™s lecture will be on <strong><u>Image Registration</u></strong>.</p> <p>We have already talked about the topic of registration in our previous lecture. I said that there is some problems of registration. And its idea is to move from one image to another. It;s very popular in our field but I initially said weâ€™re not talk about applications of registration so that you can think about this yourself, and also we can wait till the registration lecture like <u>which is having now</u>.</p> <h4 id="todays-learning-objectives">Todays Learning Objectives</h4> <ul> <li>Why do we need registration?</li> <li>Name and explain at least two similarity measures</li> <li>Describe the difference between rigid, affine and non-rigid registration</li> </ul> <h4 id="what-is-registration">What is Registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/i4jYCLQ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Example of registration </div> <ul> <li><u>Image processing method</u> for <u>aligning two or more images with each other spatially</u> (geometrically)</li> <li>You can <u>make images from different imaging modalities, times, sections, angles</u>, etc, <u>comparable</u>.</li> </ul> <h4 id="why-do-we-need-registration">Why do we need registration?</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/jO6tT6z.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Why we need registration </div> <p><strong><em>Registration geometrically transforms one image into another.</em></strong></p> <p>So what we have is we have an image which is in the most top left. And we have a target which is in the most right bottom.</p> <p>And we slowly transform our image step by step towards the target.</p> <p>So you can see that this patient has some degenerative disease and changing in the brain, so the ventricles are growing, and you can see they slowly step by step grow when we use the image of the patient. Grow and grow until they reach this level.</p> <p>But the question is, how does that happen? What mathematical process lies behind allows us to nicely deform one image towards another one?</p> <p><strong><u>Before we go into mathematical concepts, let's look at one of the main applications of image registrations.</u></strong></p> <h5 id="atlas-based-segmentation">Atlas-based segmentation</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/BJ0T49b.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. Atlas registration </div> <p>Itâ€™s <strong>atlas-based segmentation</strong>.</p> <p>Before deep learning, that was specification was the main like every time when people use registration almost in 80% or something. They used it for atlas-based registration.</p> <p>So what is the idea of atlas-based registration? What we do is we have a collection of images <u>maunally segmented</u>. And we get a new image, we <strong><u>register every image from our collection towards our image</u></strong>. During this registration, we get <strong>transformation matrix</strong>.</p> <p>So basically we get some map which says which pixels went where during the registration, how we changed during the process. And the way you can use this transformation metrics or we can apply it to the segmentation from our atlas.</p> <p>æˆ‘çš„ç†è§£æ˜¯æˆ‘ä»¬åŸæœ¬æœ‰ä¸€ç³»åˆ—æ ‡å‡†çš„äººå·¥åˆ†å‰²å¥½çš„å›¾ç‰‡ï¼Œå°†æ¯ä¸€å¼ å›¾ç‰‡éƒ½ä»¥æˆ‘ä»¬çš„æ–°å›¾åƒä½œä¸º Target è¿›è¡Œé…å‡†ï¼Œåœ¨é…å‡†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†å˜æ¢çŸ©é˜µï¼Œè€Œè¿™äº›çŸ©é˜µå¯ä»¥åº”ç”¨åˆ°åˆ†å‰²ä¸Šä»è€Œè·å¾—æ–°å›¾åƒçš„ segmentationã€‚</p> <p>We deform this image towards our target and then, we use the transformation to deform the segmentation towards our target, so that our segmentation kind of stretches and changes that tries to capture our new image.</p> <p>We can do it for all images in our atlas and then we get many versions of segmentations. And then we average them somehow (label fusion), and we just take this average result as a registered segmentation result.</p> <p>This is one of the main idea for using registration.</p> <p>What are the <strong>advantages</strong> and <strong>disadvantages</strong> of atlas-based segmentation?</p> <ul> <li> <p>Segmentation speed linearly depends on the number of training images $\implies adv$</p> <p>For any other algorithms, like deep learning algorithms, the number of training images does not affect the speed of segmentation itself. It can affect training but does not affect the segmentation. If we have million training images, the speed is the same.</p> <p>Because we registered every atlas image individually to our target, the speed directly linearly increases with the number of training images. $\implies disadv$ So the people investigate how we can use not all the training images but also some specific training images which are most similar to our target. So they apply some preprocessing and figure out which training images are most similar and then only register them to our target.</p> </li> <li> <p>Segmentation speed does not depend on the number of target structures</p> <p>Thereâ€™s a couple of <strong>advantages</strong>. One advantage is that the <strong>speed of atlas-based segmentation</strong> <strong>does not depend on the number of structures we have</strong>. We can have one million different regions in the object but the speed will be the same. We just take training images, register them, take the transformation metrics and then deform the mask.</p> <p>But many alternative algorithms, maybe like random walker, their <strong>speed grows linearly with the number of objects we have</strong>.</p> </li> <li> <p>Needs way less training samples than deep learning</p> <p>Another advantage is that we need way less training images for algorithm to work.</p> <p>In contrast to now deep learning, we need thousands or hundreds images, we can use 20 segmented brains to have a reasonably good atlas-based segmentation.</p> </li> </ul> <h5 id="detecting-differences">Detecting differences</h5> <p><strong>Diffcult to see and examine differences/changes</strong> if the images are <strong>not oriented</strong> the same or <strong>on top of each other</strong>.</p> <p>There are many different reasons for wanting to register images:</p> <ul> <li> <p><u>Get different contrasts</u> from different modalities superimposed</p> <p>This refers to the ability to <u>overlay or combine</u> <u>images obtained from different imaging modalities</u>, such as MRI, CT, or PET. <u>Each modality provides unique information or contrast about the same anatomical region</u>. <u>Registering</u> these images allows you to see <u>the anatomical structures in relation to each other</u>, providing a <u>more comprehensive view</u> of the patientâ€™s condition.</p> <p>(ä»ä¸åŒæ¨¡å¼çš„å åŠ ä¸­è·å¾—ä¸åŒçš„å¯¹æ¯”)</p> <ul> <li> <p>Get an image with poor resolution or with little anatomical detail superimposed on an image with good anatomical information</p> <p>Sometimes, medical images may have <u>low resolution</u> or <u>lack fine anatomical details</u>. <u>Registering</u> such images with <u>higher-resolution</u> or <u>more detailed images</u> can help in <u>better visualization</u> and <u>understanding of the region of interest</u>. This process essentially <u>enhances the quality of the lower-quality image by aligning it with a reference image with better anatomical information</u>.</p> <p>(å°†åˆ†è¾¨ç‡ä½æˆ–è§£å‰–ç»†èŠ‚å°‘çš„å›¾åƒå åŠ åˆ°è§£å‰–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒä¸Š)</p> </li> </ul> </li> <li> <p>Inverstigate <u>changes over time</u> (e.g., before-after treatment)</p> <p>Image registration is used to <u>compare images acquired at different time points</u>, such as <u>before and after medical treatment</u> or surgery. By <u>aligning</u> these images <u>accurately</u>, you can <u>precisely quantify changes in size</u>, <u>shape</u>, or <u>intensity</u> of structures over time. This is valuable for <u>monitoring disease progression</u> or the <u>effectiveness of a treatment</u>.</p> <ul> <li> <p>See if there are differences in <u>size, shape</u> or <u>intensity</u> over time - more sensitively</p> <p>Image registration helps in <u>detecting subtle changes</u> in the size, shape, or intensity of anatomical structures over time. It <u>enhances the sensitivity of your analysis by ensuring that the images are aligned correctly</u>, allowing for <u>precise measurement of changes that might otherwise go unnoticed</u>.</p> </li> </ul> </li> <li> <p>To compare different individuals</p> <p>Image registration allows you to <u>align and compare images from different individuals</u>, enabling a quantitative and objective assessment of anatomical structures, pathology, or physiological variations <u>across a population</u>. This is especially valuable in research studies and clinical trials.</p> <ul> <li> <p>Analyze the images to see <u>differences between individuals</u></p> <p>Image registration facilitates the <u>automated analysis of images</u> to <u>identify and quantify differences between individuals</u>. This can include <u>variations in organ size, shape, location, or pathology</u>. <u>Automated analysis</u> saves time and reduces the potential for subjective bias that may occur in <u>manual comparisons</u>.</p> </li> <li> <p>Can <u>save a lot of time</u> - for example, rather than having to <u>manually draw the structures and compare them</u></p> <p>Image registration <u>significantly reduces the need for time-consuming manual tasks</u>, such as <u>manually outlining or drawing structures for comparison</u>. By automating the alignment and analysis of images, it streamlines the research and diagnostic process, making it <u>more efficient and less prone to human error</u>.</p> </li> <li> <p><u>Register to atals</u>, and <u>extract specific region</u></p> <p>Registering images to a common anatomical atlas allows for <u>precise localization and extraction of specific anatomical regions or structures</u>. This is <u>useful for conducting region-specific analyses</u> or for <u>guiding surgical procedures with high precision</u>. It also aids in standardizing data across different individuals for research purposes.</p> </li> </ul> </li> <li> <p>Important to check that the <u>registrations are correct</u> - be sure to <u>compare the same anatomical structure</u>.</p> </li> </ul> <h4 id="ingredients-of-image-registration">Ingredients of image registration</h4> <p>The registration algorithms have different aspects which they work with.</p> <p>One aspect is information type. What information they use are <strong>intrinsic</strong> or <strong>extrinsic</strong> information. Another one thing which actually depends on information type is the <strong>similarity measure</strong>. How to understand that images become more similar or less similar during the registration. And another thing is <strong>transformation</strong>. So for some applications that have <strong>rigid</strong> transformations, only translation, rotation for example. Or for some application and most often we need <strong>non-rigid</strong>. So we need to somehow change the relative position of pixels, how expansive areas fix some other areas to get the new object.</p> <ul> <li><u>What type of information</u> do we utilise?</li> <li><u>How do we quantify</u> similarity?</li> <li><u>What kind of transformations</u> can we use?</li> <li><u>What kind of interpolations</u> can we use?</li> </ul> <h5 id="sources-of-information">Sources of information</h5> <ul> <li> <p>Intrinsic information</p> <ul> <li> <p>The images are visually similar (non-necessarily by absolute intensities):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oEvhMvl.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Intrinsic information </div> </li> </ul> </li> </ul> <p>The intrinsic information is used when these images are visually similar to each other. The intensities are similar most important. Fro example, here you can see two same images and there is a histogram how good they match to each other. And now this image will be rotated and this histogram is moving and itâ€™s becoming more like a straight line.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/9eJMBdI.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 5. Illustration of Histogram </div> <p>The histogram pixel reflects how much these two pixels are similar to each other. And the higher color is the more similar. So if you take the next pixel, how similar is the first pixel to second pixel and so on. And when we go through all pixels of first row, we compare the first pixel the first image to all pixels in the second image. And we just measure how similar they are.</p> <p>So if these images are very similar to each other, what we would expect is to see somehow the high values on the diagonal when we compare the corresponding pixels, and to see a low values outside the diagnoal, and we donâ€™t compare the corresponding pixels.</p> <p>å¯¹è§’çº¿ä¸Šçš„å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤å¹…å›¾ç‰‡åƒç´ å€¼è§†è§‰ä¸Šè¶Šç›¸è¿‘ã€‚</p> <p>So this is the idea of the histogram.</p> <p><strong>Extrinsic information</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/wDl0rkq.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 6. Example of extrinsic information </div> <ul> <li>The images are too different from each other visually</li> <li>We need to <strong>help registration by providing</strong> <strong>correspondences</strong></li> </ul> <p>The extinsic information, we usually work with images which are not similar to each other. So you can see these two images, one of them is CT and one of them is PET. And the intensities here are completely different.</p> <p>For example, we can see that the mandible is more or less visible, but in PET, the mandiable is not visible at all and the bone is invisible. But tumor is very much visible at PET, very poor visible in CT.</p> <p>So in that case, if the images have two different visually from each other. What we need to do is we need to provide some additional correspondences. And one of the most common thing to do is automated landmarks.</p> <p>We train algorithm to the specific points on both images, and we use these specific points to align the images. So we detect these points individually on PET, and detect these points on CT individually. And then we use these points to alignt he images to non-register form. So thereâ€™s a point measure to each other.</p> <h4 id="different-types-of-transformations">Different types of transformations</h4> <ul> <li>Rigid</li> <li>Affine</li> <li>Non-rigid</li> </ul> <h5 id="rigid-and-affine-transformations">Rigid and Affine transformations</h5> <p>See details in my previous blog post from <a href="https://liuying-1.github.io/assets/pdf/geometry.pdf">Signal and Image Processing - Transformations</a>. :point_left:</p> <div align="center"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UqtiM6p.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h5 id="non-rigid-transformations">Non-rigid transformations</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/aHW956w.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>Now letâ€™s define this <strong>mathematically</strong>.</p> <p>How is a 2D <strong>transformation or rotation</strong> defined mathematically?</p> <p>Can we extend this to 3D?</p> <h5 id="rigid-registration---mathematically">Rigid registration - mathematically</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/cAO4SD8.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p><u>In what kind of applications to we want to use rigid registrations?</u></p> <p>E.g., <strong>Intra-subject rigid body</strong></p> <p><strong>Intra-Subject:</strong> â€œIntraâ€ means <strong>within</strong>, and â€œsubjectâ€ typically <strong>refers to an individual or a patient</strong>. Intra-subject, in this context, means that <strong>you are dealing with multiple images of the same individual or patient</strong>. These <strong>images are acquired at different time points</strong> or <strong>using different imaging modalities</strong> while <strong>focusing on the same anatomical region</strong> within the <strong>same person</strong>.</p> <p><strong>Rigid Body</strong>: In the context of image registration, a rigid body transformation is <strong>a mathematical transformation</strong> that includes <strong>translation</strong>, <strong>rotation</strong>, and <strong>scaling</strong> but does <strong>not allow for any deformation or stretching</strong> of the image. In other words, the <strong>relative positions and orientations of structures within the image can change</strong>, but the <strong>shapes and sizes of the structures remain the same</strong>.</p> <p><strong>Intra-Subject Rigid Body Registration</strong>:</p> <ul> <li>This refers to the process of <strong>aligning</strong> and <strong>registering</strong> images of the <strong>same individual or patient</strong> that have been obtained under <strong>different conditions</strong> or at <strong>different times</strong>.</li> <li>For example, if <strong>a patient undergoes a series of medical imaging scans over time</strong> (e.g., MRI or CT scans) to monitor a condition or treatment <strong>progress</strong>, you may use <strong>intra-subject rigid body registration to align these images</strong>.</li> <li>The purpose is to ensure that <strong>corresponding anatomical structures in each image are in the same spatial position and orientation</strong>, even if the patientâ€™s position or the imaging conditions have changed slightly between scans.</li> <li>The primary goal is typically to <strong>track changes in the patientâ€™s anatomy</strong>, such as tumor growth, organ movement, or any other anatomical variations, with precision and accuracy.</li> </ul> <p>Applications of <strong>intra-subject rigid body registration</strong> can be found in various medical fields, including <strong>radiation therapy</strong>, where itâ€™s crucial to <strong>ensure that the radiation beam is accurately targeted at the same area during multiple treatment sessions</strong>, or in <strong>longitudinal studies to monitor disease progression</strong> or <strong>response to treatment within the same patient</strong>.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JpzfBT9.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In summary, <strong>intra-subject rigid body registration</strong> involves <strong>aligning and comparing images of the same individual taken at different times</strong> or <strong>under different conditions</strong>. Itâ€™s a fundamental technique in medical image analysis used for <strong>monitoring, diagnosis, and treatment planning</strong>, where precise <strong>spatial alignment of anatomical structures</strong> is essential.</p> <h5 id="affine-registration---mathematcically">Affine registration - mathematcically</h5> <p>However, basically, the scaling is considered as affine transformation.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/CRsio4B.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In what kind of applications to we want to use affine registrations?</p> <p>E.g. <strong>inter-subject</strong></p> <p><strong>Inter-Subject</strong></p> <ul> <li>â€œInterâ€ means <strong>between or among</strong>, and â€œsubjectâ€ typically refers to <strong>different individuals</strong> or <strong>patients</strong> in the context of medical imaging.</li> <li>Therefore, â€œinter-subjectâ€ means that <strong>you are dealing with images acquired from different individuals or patients</strong>.</li> </ul> <p><strong>Affine Transformation</strong>:</p> <ul> <li>An affine transformation is a type of transformation that <strong>includes translation, rotation, scaling</strong>, and <strong>shearing</strong> but does not allow for <strong>deformation or non-linear warping of the image</strong>. Essentially, itâ€™s a more flexible transformation than rigid body transformations (which only include translation and rotation) but still <strong>preserves the basic shape of objects</strong>.</li> </ul> <p><strong>Inter-Subject Affine Transformation</strong>:</p> <ul> <li>This refers to the process of <strong>aligning and registering images acquired from different individuals or patient</strong>s.</li> <li>In medical imaging, you might have a set of images from different patients, such as MRI scans of different brains, CT scans of different chests, or X-rays of different limbs.</li> <li>Inter-subject affine transformation allows you to <strong>align and compare these images</strong>, despite variations in size, orientation, and scaling between individuals.</li> <li>The primary goal is to <strong>establish a common spatial coordinate system that enables meaningful comparisons</strong>, such as in population-based studies, group analyses, or template-based approaches.</li> </ul> <p>Applications of <strong>inter-subject affine transformation</strong> can be found in various areas of medical research and clinical practice, such as <strong>comparing anatomical structures across a population</strong>, <strong>creating population-based atlases or templates</strong>, and <strong>performing group-level statistical analyses</strong>. Itâ€™s particularly useful when you need to make comparisons or perform analyses that involve images from <strong>different individuals</strong> while accounting for variations in size, orientation, and scaling.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/ngmeDyx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In summary, inter-subject affine transformation involves <strong>aligning and registering images from different individuals or patients</strong>, allowing for <strong>meaningful comparisons and analyses across a population</strong>. This technique plays a crucial role in various medical image analysis applications, particularly in group studies and population-based research.</p> <h5 id="example-of-affine-registration-across-several-subjects">Example of affine registration across several subjects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/XNxsaN5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h5 id="non-rigid-registration---mathematically">Non-rigid registration - mathematically</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Vw4yIRX.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In what kind of applications to we want to use non-rigid registrations?</p> <p>E.g. <strong>intra or inter-subject non-rigid body</strong></p> <p>A 3x3 matrix that represents <strong>a non-rigid transformation</strong>. <strong>Each element in the matrix affects the deformation</strong> of the image in different ways. The exact values of these elements determine <strong>how the image is deformed locally</strong>. Hereâ€™s a brief explanation of the elements:</p> <ul> <li><code class="language-plaintext highlighter-rouge">m1</code>, <code class="language-plaintext highlighter-rouge">m5</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control <strong>scaling</strong> factors in the x, y, and z directions, respectively.</li> <li><code class="language-plaintext highlighter-rouge">m2</code>, <code class="language-plaintext highlighter-rouge">m4</code>, <code class="language-plaintext highlighter-rouge">m6</code>, and <code class="language-plaintext highlighter-rouge">m8</code> control <strong>shearing</strong> or skewing effects.</li> <li><code class="language-plaintext highlighter-rouge">m3</code>, <code class="language-plaintext highlighter-rouge">m7</code>, and <code class="language-plaintext highlighter-rouge">m9</code> control the <strong>actual deformation or warping</strong>.</li> </ul> <p><strong>1. Why Use Non-Rigid Transformations?</strong></p> <ul> <li><strong>Non-rigid transformations</strong> are used when you need to account for <strong>more complex changes in the shape and structure of objects or regions within the image</strong>. Rigid and affine transformations are too restrictive for scenarios where local deformations occur, such as changes in the shape of organs or tissues, as in medical image analysis.</li> </ul> <p><strong>2. Applications of Non-Rigid Registrations (Intra or Inter-Subject):</strong></p> <ul> <li><strong>Intra-Subject Non-Rigid Registration</strong>: In this case, you are dealing with images of the <strong>same individual or patient at different time points</strong>. <strong>Non-rigid registration</strong> can be used to account for <strong>anatomical changes over time</strong>, such as organ deformation during respiration, muscle contractions, or gradual changes in tissue structures due to disease progression.</li> <li><strong>Inter-Subject Non-Rigid Registration</strong>: When you have images from <strong>different individuals (inter-subject)</strong>, <strong>non-rigid registration</strong> becomes valuable for <strong>aligning images that exhibit significant anatomical variations</strong>. Examples include aligning MRI scans of different brains, which have <strong>varying shapes and sizes</strong>, or comparing images of different patients with congenital or acquired deformities.</li> </ul> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/JPahdsm.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <p>In both <strong>intra-subject</strong> and <strong>inter-subject</strong> scenarios, <strong>non-rigid registration allows you to account for the complex and local variations in anatomy</strong>, enabling <strong>accurate comparisons, analysis, and medical interventions</strong>. Itâ€™s particularly important when dealing with deformable structures and when rigid or affine transformations are insufficient to capture the necessary deformations. Applications range from disease monitoring to surgical planning and population studies.</p> <p><strong>1. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨éåˆšæ€§å˜æ¢ï¼Ÿ</strong></p> <ul> <li>å½“æ‚¨éœ€è¦è€ƒè™‘<strong>å›¾åƒä¸­æ›´å¤æ‚çš„ç‰©ä½“æˆ–åŒºåŸŸçš„å½¢çŠ¶å’Œç»“æ„å‘ç”Ÿå˜åŒ–</strong>æ—¶ï¼Œå°±éœ€è¦ä½¿ç”¨<strong>éåˆšæ€§å˜æ¢</strong>ã€‚å¯¹äºåŒ»å­¦å›¾åƒåˆ†æç­‰é¢†åŸŸï¼Œåˆšæ€§å’Œä»¿å°„å˜æ¢å¯¹äºå±€éƒ¨å˜å½¢åœºæ™¯å¤ªè¿‡å—é™åˆ¶ã€‚</li> </ul> <p><strong>2. éåˆšæ€§é…å‡†çš„åº”ç”¨ï¼ˆä¸ªä½“å†…æˆ–ä¸ªä½“é—´ï¼‰ï¼š</strong></p> <ul> <li><strong>ä¸ªä½“å†…éåˆšæ€§é…å‡†ï¼š</strong> åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¤„ç†çš„æ˜¯<strong>åŒä¸€äººæˆ–æ‚£è€…åœ¨ä¸åŒæ—¶é—´ç‚¹çš„å›¾åƒ</strong>ã€‚éåˆšæ€§é…å‡†å¯ç”¨äºè€ƒè™‘éšæ—¶é—´å‘ç”Ÿçš„è§£å‰–å˜åŒ–ï¼Œä¾‹å¦‚å‘¼å¸è¿‡ç¨‹ä¸­çš„å™¨å®˜å˜å½¢ã€è‚Œè‚‰æ”¶ç¼©æˆ–ç”±äºç–¾ç—…è¿›å±•å¼•èµ·çš„ç»„ç»‡ç»“æ„é€æ¸å˜åŒ–ã€‚</li> <li><strong>ä¸ªä½“é—´éåˆšæ€§é…å‡†ï¼š</strong> å½“æ‚¨æœ‰æ¥è‡ªä¸åŒä¸ªä½“çš„å›¾åƒï¼ˆä¸ªä½“é—´ï¼‰æ—¶ï¼Œéåˆšæ€§é…å‡†å˜å¾—é‡è¦ï¼Œä»¥å¯¹é½å±•ç¤ºæ˜æ˜¾è§£å‰–å·®å¼‚çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œå¯¹é½å…·æœ‰ä¸åŒå½¢çŠ¶å’Œå¤§å°çš„ä¸åŒå¤§è„‘çš„MRIæ‰«æå›¾åƒï¼Œæˆ–æ¯”è¾ƒä¸åŒæ‚£è€…å…·æœ‰å…ˆå¤©æ€§æˆ–åå¤©æ€§ç•¸å½¢çš„å›¾åƒã€‚</li> </ul> <p>åœ¨ä¸ªä½“å†…å’Œä¸ªä½“é—´çš„æƒ…å†µä¸‹ï¼Œéåˆšæ€§é…å‡†å…è®¸æ‚¨è€ƒè™‘è§£å‰–çš„å¤æ‚å’Œå±€éƒ¨å˜åŒ–ï¼Œä»è€Œå®ç°å‡†ç¡®çš„æ¯”è¾ƒã€åˆ†æå’ŒåŒ»ç–—å¹²é¢„ã€‚å®ƒåœ¨å¤„ç†å¯å˜å½¢ç»“æ„å’Œåˆšæ€§æˆ–ä»¿å°„å˜æ¢æ— æ³•æ•æ‰å¿…è¦å˜å½¢çš„æƒ…å†µä¸‹å°¤ä¸ºé‡è¦ã€‚åº”ç”¨èŒƒå›´ä»ç–¾ç—…ç›‘æµ‹åˆ°æ‰‹æœ¯è§„åˆ’å’Œäººç¾¤ç ”ç©¶ã€‚</p> <h5 id="example-of-non-rigid-registration-across-several-subjects">Example of non-rigid registration across several subjects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vY3vg4N.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> <h4 id="similarity-measures">Similarity measures</h4> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6imIdry.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 7. MSE </div></div> <p>So letâ€™s talk about <strong>similarity measures</strong>.</p> <p>What could we use as <strong>similarity measure</strong>?</p> <ul> <li>Absolute difference</li> <li><strong>Sum of squared difference</strong></li> <li><strong>Cross-correlation</strong></li> <li>Mutual information</li> </ul> <p>There are two types of similarity measures described above? What are the two types?</p> <h5 id="mean-sum-of-squared-differences">Mean sum of squared differences</h5> \[MSE = \frac{1}{n}\frac{1}{m}\sum^{n}_{x=1}\sum^{m}_{y=1}\left(I(x, y)-J(x,y)\right)^2\] <p>We have one of the most obvious similarity measure is the mean sum of squared differences. So what we do would take the same pixel in each image, and compare the intensity difference and we square it and we sum up all squared differences and we divided by the number of pixels we have.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vaWcNyY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 8. MSE calculation </div></div> <p>So here is an example of these two images and the result of similar measure calculation. And the smaller the value, the most similar the images are. So we want to <strong>minimize</strong> it.</p> <h5 id="normalized-sum-of-squared-differences">Normalized sum of squared differences</h5> <p>The sum of squared differences works well if the images have the same intensity ranges. What if they have <strong>a different intensity ranges</strong>?</p> <p>One of the good idea which is used almost always when the pixel-wise comparison is used is <strong>normalization</strong>.</p> <p>What we do is we take our intensities in our image, we compute the mean intensity, we compute the standard deviation. We can test this and we normalise them. From the original images subtract the mean that the new mean will become $0$. And we divide it by the standard deviation.</p> \[I^* = \frac{I-\overline{I}}{\sigma(I)}\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/3w7cKY7.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 9. NMSE </div></div> <p>By doing this, everything is the same except the <strong>intensity range</strong>. And it is more good useful in practical applications.</p> <p>So <strong>normalization</strong> is one of the things which is very commonly used and this is <strong>pixel-wise similarity measure</strong>.</p> <h5 id="normalized-cross-correlation">Normalized cross-correlation</h5> <p>Another idea which we can go further in this area, what we can do is we can do the <strong>cross-correlation</strong>.</p> \[NCC = \frac{\sum_{x, y}\left(\left(I(x, y) - \overline{I}) \cdot(J(x, y)-\overline{J}\right)\right)}{\sqrt{\sum_{x, y}(I(x, y) - \overline{I})^2\sum_{x, y}(J(x, y)-\overline{J})^2}}\] <p>So what we do is to <strong>find the normalized pixel values</strong> and then <strong>multiply each other</strong>, and <strong>find the sum of them</strong>. And then we divide it by the squared root of multiplication of squared difference. So to see how these things behaves, letâ€™s go to the mirror board to see some examples.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/p8i5spT.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 10. Cross-correlation </div></div> <p>So, what we would like to do is would like this expression to get good values when we compare the upper two patches, and to have bad values when compared to the other two. We are going to quickly compute them to see what happens.</p> \[NCC_{good} = \frac{3\cdot3 + 3\cdot(-1)\cdot(-1)}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{12}{12}=1\] <p>What about the bad patches?</p> \[NCC_{bad} = \frac{-3+1+1-3}{\sqrt{\left(9+1+1+1\right)^2}} = \frac{-4}{12}=-\frac{1}{3}\] <p>What we would like the normal cross correlation to be as high as possible because we saw that for the <strong>same patch</strong>, we get $1$. And for two different patches, we get $-\frac{1}{3}$. And we actually see one of the good property of the normal cross-correlation. It seems to be that its values are actually restricted into the $-1$ and $+1$ range. So when we get to same patches, basically the best possible scenario. We get the result with $1$. So probably if we get the opposite patch, we will probably get $-1$.</p> <p>The good thing about is normalise as cross correlation is, thatâ€™s why the name comes that our values. They <strong>get normalized</strong> into the $-1$ to $+1$ range. This is very <strong>useful</strong> when we compare two patches.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/M4pqEGG.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 11. Maximization </div></div> <p><strong><em>Will MSE and NCC work for CT-MR image registration?</em></strong></p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/SCQfK8H.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 12. CT-MR </div></div> <p>We want to perform <strong>registration</strong> on <strong>CT-MR</strong> image on two images, which are <strong>very different intenstity patterns</strong>. The edges in these images are kind of <strong>similar</strong>.</p> <p>You can see that there is a <strong>mandible</strong> on the right chart, which is <strong>black</strong> but on the left side, it is <strong>white</strong>. You can see the edges over human skin. They are also kind of more or less visible. But what is important is that the intensities now is completely different. So the intensitity plan is completely different.</p> <p><strong><em>Now the question is if you can use MSE or NCC or something like this for this registration.</em></strong></p> <p>Actually turns out we <strong><u>cannot</u></strong> use them because what MSE and what all of these metrics for what they tried to do is try to <strong><u>match high intensity with high intensity, and low intensity with low intensity</u></strong>.</p> <p>But in our particular case, what we would like to do is we want to <strong>match somehow the patterns the edges within the pixels</strong>. For example, we want to match a bone, where a black and white intensities in different patches. <strong>Itâ€™s very difficult to just in advance define this, so we can know what exactly we want to match</strong>. =&gt; <em>But we want to match patterns not individual intensities.</em></p> <h5 id="mutual-image-information">Mutual image information</h5> <p>For this reason, there is another metric which is called <strong>mutual image information</strong>.</p> <p>The metric is most commonly used in <strong>image registration</strong>. It requires a lot of computation resources, but at the same time, it is a very <strong>versatile</strong> can be applied for many different images. I do not need to know many things about images in advance. We can just apply it and it will most likely work.</p> <p>Here is an equation how mutual information works.</p> \[MI = E(I, J)\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6FN99jJ.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 13. Joint entropy </div></div> <p>So we need to compute the entropy issues and we will compute in histogram manner.</p> <p>Below is the illustration.</p> <p>Letâ€™s say we have different images which we want to match each other. Letâ€™s assume these are the images of some object.</p> <p>What we do is we create a 2D plot where at a lower dimension.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/oosMzEx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 14. Illustration </div></div> <p>Then, we will do the same thing for the next pixel, and â€¦.. it will finally come with a cloud of points. And we are interested to how <strong>compact</strong> is this cloud.</p> <p>From this, if there are two images just inversion of each other, our plots will be very compact. It means, we just need very small regions to cover all the possible pixels. But when the images were random, the pixels which would plot it on this 2D plot, they would distribute to the space in a completely manner. They could be occupied a lot of area.</p> <p>So what we can do and the idea of the mutual information is to complete a histogram of the pixels in both axises $x$ and $y$ and which is a probability histogram. And then compute probability histogram for joint probability.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/K8wN4pA.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 15. Illustration </div></div> <p><strong><u>So the more compact the more higher will be mutual information.</u></strong></p> <p>And here is basically how they look like for this particular image.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Wuk9Ahc.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 16. Mutual information </div></div> <p>These are with the mutual information for these two images which are just inversion of each other.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/GFL1GWk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 17. Rotation mutual information </div></div> <p>Here is the rotated version. And you can see that the plot now is no longer compact as it was. It occupies a lot space. And thatâ€™s why the mutual information is way lower. And our ideas is to <strong>maximize mutual information</strong>.</p> <h4 id="interpolation">Interpolation</h4> <p>Another concept which we need to know is the concept of <strong>interpolation</strong>.</p> <p>What different interpolation techniques do you know?</p> <ul> <li>Nearest neighbour</li> <li>Linear interpolation</li> <li>Barycentric interpolation</li> <li>Spline interpolation</li> </ul> <h5 id="interpolation-effects">Interpolation effects</h5> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/1EVjwqL.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 18. Interpolation effects </div></div> <p>Different interpolation techniques work on diff. images, the choice of interpolation technique can have varying effects or perform differently depending on the characteristics and content of the input images.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YanUjxl.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 19. Effects of different interpolation techniques </div></div> <p>ä¸åŒæ’å€¼æŠ€æœ¯ä¸­æåˆ°çš„ RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰å€¼å¯ä»¥ä½œä¸ºä¸€ç§é‡åŒ–æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ¯ç§æ’å€¼æ–¹æ³•åº”ç”¨äºç‰¹å®šå›¾åƒæ—¶çš„è´¨é‡æˆ–å‡†ç¡®æ€§ã€‚RMSE æ˜¯ä¸€ç§å¸¸ç”¨æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–åŸå§‹å›¾åƒä¸æ’å€¼ï¼ˆè½¬æ¢ï¼‰å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚</p> <p>The <strong>interpolation</strong> in image registration plays two roles.</p> <p>One of them is for <strong>computing the values of pixels</strong>. Letâ€™s say if the pixels has a coordinate which do not match original coordinates. For example, one pixel has a coordinate $(0, 0)$. But if we would like to move it a little bit, $(-0.1, 0.3)$. <strong>What kind of value of this pixel have?</strong> Should we just around these numbers to get $(0, 0)$ or should we do something more intellectual.</p> <p>æ‰€ä»¥å°±æ˜¯å½“æˆ‘ä»¬åšé…å‡†æ—¶ï¼Œæˆ‘ä»¬ç§»åŠ¨äº†ä¸€ç‚¹ç‚¹å›¾åƒï¼Œé‚£ä¹ˆåŸå…ˆç‚¹ä¸Šçš„åƒç´ åº”è¯¥æ˜¯å˜æˆå¤šå°‘å‘¢ï¼Ÿä½¿ç”¨æ’å€¼æ³•æ¥è®¡ç®—ã€‚</p> <p>Actually we take account the neighboring pixels if they can play some role in the value of a new pixel. There is one way how we can compute it.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YYy2YJ4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 20. Example of interpolation </div></div> <p>We have four points $(0,0), (1,0), (0,1)$ and $(1,1)$. Letâ€™s say there are pixels $1$, $2$, $3$ and $4$. And would like to find the value of a pixel which will be located here. And then the question is <strong>what will be the value of the pixel</strong>? We know the intensities of these four pixels, and also the corresponding $(x, y)$ of the target pixel.</p> <p>So the way to do it is to <strong>calculate the contribution</strong>. The contribution of this pixel to our new pixel. Letâ€™s just imagine that this $x, y$ will be very close to $1$.</p> \[x, y\approx 0.999\] <p>So how similar should be the contribution of the pixel? Each of these four pixels or contribute something to the intensity.</p> <p>The contribution of $I(0, 0)$:</p> <ul> <li>Proportional to the opposite â€œrectangleâ€.</li> </ul> \[(1-x)\cdot(1-y)\cdot I(0, 0)\] <p>The same logical goal for other four pixels. So for pixel $(1,1)$, itâ€™s contribution to the intensity of our new pixel through the proportional to this.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/J0lWvji.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 21. Contribution illustration </div></div> \[\begin{align} I(\cdot) &amp;= (1-x)\cdot(1-y)\cdot I(0. 0) \\ &amp; + x\cdot (1-y)\cdot I(1, 0) \\ &amp; + (1-x)\cdot y \cdot I(0, 1) \\ &amp; + x \cdot y \cdot I(1, 1) \end{align}\] <p>So the rule is that we compute the <strong>size of these boxes</strong> and then <strong>multiplying them</strong> by the opposite pixel. Then, sum them up and get the final new value of the pixel.</p> <h4 id="image-registration-is-optimization">Image registration is optimization</h4> <p>We need to try to apply registration algorithm, try to optimise registration process.</p> <p>So what we can learn until now is we now know two pixels in the space and their neighborhood of them, how to compute how similar are they using different similarity measures based on pixel based comparison, like sum of quare differences or normalized cross correlation, or based on some advanced techniques like mutual information.</p> <p>We also know how this image moving will change if we move it 7.5 pixels up, how many intensities will change. =&gt; Interpolation</p> <p>So we know all this and we can try to formulate the registration, how the registration should be done.</p> <p>So what we need to do is we need to <strong>minimize</strong> depending on what kind of similarity measure we use. We want to minimize a distance metric.</p> \[\min\sum_{i, j}d\left(I(i, j) - J\left(x(i, j, \theta), y(i, j, \theta\right)\right)\] <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/fgLvTG6.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 22. Similarity measure </div></div> <p>So here is our formulation of for registration optimization. What we need to find is to find $\theta$. <strong><em>The question is how to do it.</em></strong></p> <p>The common way to solve it is using an alogrithm called <strong><u>gradient descent algorithm</u></strong>. So letâ€™s just choose the most simple similarity measures we have. So, for $d$, letâ€™s choose the squared intensity difference. So basically, we just take a pixel and find the difference between each pixel individually, and thatâ€™s it. No mutual information or anything complex like that.</p> \[d(I(i, j), J(x, y)) = (I(i, j)-J(x, y))^2\] <p>And $\theta$ will only use one transformation - translation. So basically $F$ from $I$ to $J$ is basically a translation we would translate and the $I$ to some value that the $x$ and $J$ to some value.</p> \[f(I, J, \theta) - \text{translation using }\theta \\ f(i, j, \theta) = [i+\theta_x, j+\theta_y]\] <p>And this is one of the most simple configuration we can have, and now the question is <strong>how we get optimized solution</strong>? How we can find the optimal $\theta$? And the way to do it is to use the <strong>gradient descent algorithm</strong>.</p> <p>Then, the formulation of the problem is, <strong><u>we want to optimize</u></strong>:</p> \[E(\theta) = \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta))^2\] <h5 id="gradient-decent-algorithm">Gradient decent algorithm</h5> <p>By using <strong>gradient decent algorithm</strong>:</p> \[\nabla E(\theta) = \frac{\partial E(\theta)}{\partial \theta}\] <p>And move our solution move like take some $\theta$, change our $\theta$ according to the value of the gradient.</p> \[\theta_{t+1} = \theta_{t} + \eta\nabla E(\theta)\] <p>Usually weighting factor is something very small, like $0.01$. In deep learning, it is called, learning rate. So the question is how to find the gradient of the expression?</p> <p>In our case,</p> \[\nabla E(\theta) = \nabla \sum_{i, j}(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))^2 = \\ -\sum_{i, j} 2(I(i, j) - J(x(i, j, \theta), y(i, j, \theta)))\cdot\frac{\partial J}{\partial \theta}\] <p>Now, the question is how to differentiate the $J$ against $\theta$.</p> \[\frac{\partial J}{\partial \theta} = \frac{\partial J}{\partial X} \cdot \frac{\partial X}{\partial \theta}\] <p>And this is again a complex function, we need to first differentiate it against its coordinates (transformed coordinates), basically aginst $x$ and $y$.</p> <p>So if you want to differeniate an image against its coordinates, what it means is actually very simple thing.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Lx6vlHx.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> </div> \[\frac{\partial J}{\partial x} = \frac{1}{2}(I(x+1, y) - I(x-1, y))\] <p>Find two neighboring values for every pixel, subtract them from each other, and then multiply by 0.5. And then we do the same thing for the vertical direction.</p> \[\frac{\partial J}{\partial y} = \frac{1}{2}(I(x, y+1) - I(x, y-1))\] <p>Below is the illustration.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YQxD2o5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 23. Understanding </div></div> <p>This part is easy, but the question is how to find the last part on the abovementioned differentiation, the gradient of our coordinates against our transformation.</p> \[T(x, y) = \begin{bmatrix}1 \quad 0 \quad \theta_x\\0 \quad 1 \quad \theta_y\\ 0 \quad 0\quad 1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix} = [x+\theta_x, y+\theta_y]\\ \frac{\partial X}{\partial \theta} = \begin{bmatrix} \frac{\partial X}{\partial \theta_x}; \frac{\partial X}{\partial \theta_y}\end{bmatrix} = [1, 1]\] <p>Now, we have everything.</p> \[\frac{\partial J}{\partial \theta} = [\frac{1}{2}\left(I(x+1, y\right) - I(x-1, y), \frac{1}{2}(I(x, y+1) - I(x, y-1))]\] <p>This is just differentiaion of the image because our transformation is just translation. Then, we can compute the whole graient of our system.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/RkzH4GA.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 24. Whole gradient </div></div> <p>Iteration and computation until it stops.</p> <h5 id="problem-with-optimization">Problem with optimization</h5> <p>This is a very expensive procedure to do as these similiarty measures $d(I, J)$ can be complex and expensive to compute (mutual information - regions around pixels).</p> <p>Another issue is that $\theta$ can contain many variables. In reality, $\theta$ can have thousands of variables, so each individual pixel can move individually. So each pixel will have transformations. Unlike in our case, only two variables.</p> <ul> <li> <p>9 for similarity transformations</p> \[\begin{bmatrix} \theta_1 \quad \theta _2 \quad \theta_3\\ \theta_4 \quad \theta_5 \quad \theta_6 \\ \theta_7 \quad \theta_8 \quad \theta_9 \end{bmatrix}\] </li> <li> <p>thousands for non-rigid transformations</p> </li> </ul> <p><strong>How to simplify this?</strong></p> <p>What people usually do is they use so-called <strong>multi-resolution pyramid registration</strong>.</p> <h5 id="multi-resolution-pyramid-registration">Multi-resolution pyramid registration</h5> <ul> <li>Downscale (and smooth) reference and moving images x16 (types)</li> <li>Register them and memorize transformation</li> <li>Downscale (and smooth) reference and moving images x8, apply memorized transformation</li> <li>Register images and combine transformations from both steps</li> <li>Continue</li> </ul> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/PiCMkeP.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 25. Multi-resolution pyramid registration </div></div> <p>â€œå¤šåˆ†è¾¨ç‡é‡‘å­—å¡”é…å‡†â€ æ˜¯ä¸€ç§åœ¨å›¾åƒé…å‡†ä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯ç”¨äºå°†ä¸¤ä¸ªå…·æœ‰ä¸åŒåˆ†è¾¨ç‡æˆ–å°ºåº¦çš„å›¾åƒå¯¹å‡†ã€‚å®ƒé‡‡ç”¨åˆ†å±‚æ–¹æ³•é€æ­¥å¯¹é½å’Œä¼˜åŒ–ä¸¤å¹…å›¾åƒä¹‹é—´çš„é…å‡†ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹æ‚¨æä¾›çš„è¦ç‚¹ï¼Œä»¥è¯´æ˜è¿™ä¸ªæŠ€æœ¯çš„å·¥ä½œåŸç†ï¼š</p> <ol> <li><strong>å°†å‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒé™é‡‡æ ·ï¼ˆå¹¶å¹³æ»‘ï¼‰16å€ï¼ˆç±»å‹ï¼‰</strong>ï¼š <ul> <li>é¦–å…ˆï¼Œå‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒéƒ½ä¼šé™ä½åˆ†è¾¨ç‡ï¼Œé™ä½16å€ï¼Œå¹¶å¯é€‰æ‹©å¹³æ»‘ä»¥å‡å°‘å™ªéŸ³ã€‚</li> <li>è¿™ä¸€æ­¥åˆ›å»ºäº†å›¾åƒçš„ä½åˆ†è¾¨ç‡ç‰ˆæœ¬ã€‚</li> </ul> </li> <li><strong>å°†å®ƒä»¬è¿›è¡Œé…å‡†å¹¶è®°å¿†å˜æ¢</strong>ï¼š <ul> <li>å¯¹é™é‡‡æ ·çš„å‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒè¿›è¡Œé…å‡†ï¼Œå³è°ƒæ•´å®ƒä»¬çš„ç©ºé—´å¯¹é½ä»¥æ‰¾åˆ°æœ€ä½³åŒ¹é…ã€‚</li> <li>è®°å¿†æˆ–ä¿å­˜è°ƒæ•´è¿™äº›ä½åˆ†è¾¨ç‡å›¾åƒçš„å˜æ¢ã€‚</li> </ul> </li> <li><strong>å°†å‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒé™é‡‡æ ·ï¼ˆå¹¶å¹³æ»‘ï¼‰8å€ï¼Œåº”ç”¨è®°å¿†çš„å˜æ¢</strong>ï¼š <ul> <li>å‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒå†æ¬¡é™ä½åˆ†è¾¨ç‡ï¼Œè¿™æ¬¡é™ä½8å€ã€‚</li> <li>å…ˆå‰è®°å¿†çš„å˜æ¢ï¼ˆæ¥è‡ªæ­¥éª¤2ï¼‰è¢«åº”ç”¨äºè¿™äº›ä½åˆ†è¾¨ç‡å›¾åƒã€‚</li> <li>è¿™ä¸€æ­¥åˆ›å»ºäº†æ›´é«˜åˆ†è¾¨ç‡ä½†ä»ç„¶å‡å°æ¯”ä¾‹çš„å¯¹é½å›¾åƒã€‚</li> </ul> </li> <li><strong>å¯¹å›¾åƒè¿›è¡Œé…å‡†å¹¶ç»“åˆæ¥è‡ªä¸¤ä¸ªæ­¥éª¤çš„å˜æ¢</strong>ï¼š <ul> <li>æ¥è‡ªæ­¥éª¤3çš„æ›´é«˜åˆ†è¾¨ç‡å‚è€ƒå›¾åƒå’Œç§»åŠ¨å›¾åƒè¿›ä¸€æ­¥é…å‡†ï¼Œè¿›ä¸€æ­¥ç²¾ç»†è°ƒæ•´å®ƒä»¬çš„å¯¹é½ã€‚</li> <li>ä»è¿™ä¸ªé…å‡†æ­¥éª¤è·å¾—çš„å˜æ¢ä¸æ­¥éª¤2ä¸­çš„è®°å¿†çš„å˜æ¢ç»„åˆåœ¨ä¸€èµ·ã€‚</li> <li>è¿™äº›å˜æ¢çš„ç»„åˆç¡®ä¿å›¾åƒåœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹å‡†ç¡®å¯¹é½ã€‚</li> </ul> </li> <li><strong>ç»§ç»­</strong>ï¼š <ul> <li>è¿™ä¸ªè¿‡ç¨‹ä»¥è¿­ä»£æ–¹å¼ç»§ç»­ï¼Œå›¾åƒé€æ¸é™ä½åˆ†è¾¨ç‡ï¼Œé€æ­¥å¯¹é½å’Œå˜æ¢ã€‚</li> <li>åœ¨æ¯ä¸ªçº§åˆ«ä¸Šï¼Œå˜æ¢å¾—åˆ°è¿›ä¸€æ­¥çš„ç²¾ç»†è°ƒæ•´å’Œç»„åˆã€‚</li> <li>è¿™ä¸€è¿‡ç¨‹æŒç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°æœ€é«˜æ‰€éœ€åˆ†è¾¨ç‡æˆ–è¾¾åˆ°æ”¶æ•›æ¡ä»¶ã€‚</li> </ul> </li> </ol> <p><strong>å…³é”®ç‚¹</strong>ï¼š</p> <ul> <li>å¤šåˆ†è¾¨ç‡é‡‘å­—å¡”é…å‡†æ˜¯ä¸€ç§å¤„ç†å…·æœ‰ä¸åŒç»†èŠ‚æ°´å¹³å’Œåˆ†è¾¨ç‡çš„å›¾åƒçš„å¸¸è§ç­–ç•¥ã€‚</li> <li>å®ƒä»ä½åˆ†è¾¨ç‡å¼€å§‹è¿›è¡Œç²—ç•¥å¯¹é½ï¼Œç„¶åéšç€è€ƒè™‘åˆ°æ›´é«˜åˆ†è¾¨ç‡é€æ­¥ä¼˜åŒ–å¯¹é½ã€‚</li> <li>è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾ç€åŠ å¿«é…å‡†è¿‡ç¨‹ï¼Œæé«˜å¯¹é½çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å‹æˆ–å¤æ‚å›¾åƒæ—¶ã€‚</li> </ul> <p>æ€»çš„æ¥è¯´ï¼Œå¤šåˆ†è¾¨ç‡é‡‘å­—å¡”é…å‡†æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œç”¨äºç¨³å¥åœ°å¯¹é½å…·æœ‰ä¸åŒå°ºåº¦æˆ–ç»†èŠ‚çº§åˆ«çš„å›¾åƒï¼Œå°¤å…¶åœ¨åŒ»å­¦å½±åƒã€è®¡ç®—æœºè§†è§‰å’Œé¥æ„Ÿç­‰é¢†åŸŸï¼Œå…¶ä¸­å›¾åƒå¯èƒ½å…·æœ‰ä¸åŒçš„åˆ†è¾¨ç‡æˆ–åŒ…å«ç»†èŠ‚ä¿¡æ¯ã€‚</p> <p>When you even do pyramid approach, there is still a problem in computation.</p> <p>In the pyramid approach, people donâ€™t do it for every pixel, people do it for a grid. So you just define a grid of points. For example, the size of the image is $100 \times 100$, but the number of grid points is $10 \times 10$. So we just place great place grid points at $10 \times 10$, skip every pixels and base with grids. And what we do is we compute our similarity measure only for this points. And we compute the optimization only for these points. And then we somehow inerpolate the transformation for the internal points.</p> <div align="center"><div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DV1yHKh.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 26. Grid-based registration </div></div> <ul> <li>Do not perform registration per pixel but use a sparse grid</li> </ul>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Learning note for medical image registration.]]></summary></entry><entry><title type="html">Segmentation Basics</title><link href="https://liuying-1.github.io/blog/2023/segmentation-basics/" rel="alternate" type="text/html" title="Segmentation Basics"/><published>2023-09-21T08:00:00+00:00</published><updated>2023-09-21T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/segmentation-basics</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/segmentation-basics/"><![CDATA[<p><b><i>Disclaimer: All the material below is refered to the course, Medical Image Analysis, delievered by the University of Copenhagen.</i></b></p> <h4 id="todays-learning-objectives">Todayâ€™s Learning Objectives</h4> <ul> <li>Define what segmentation means</li> <li>What kind of segmentation methods exist</li> <li>Explain how connected component decomposition works</li> <li>How can you use dilation-erosion operations</li> </ul> <p>First, I will tell you <u>what segmentation means</u> and then I will familliarize you with some <u>existing simple segmentation methods</u>. <u>These methods can be used as</u> and are used very often despite very simple, not usually as independent solutions, but actually, as <u>auxiliary methods</u>. And I will also show you a couple of useful tools, and one of them will be the connected component decomposition and another one will be more for logical operations. And I will show you how we use them in practice, how you can augment medical image segmentation with these methods.</p> <h4 id="topics-of-interest">Topics of Interest</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/iFssnJ1.png" alt="image-20230923063424696" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Topics of interest in segmentation conference </div> <p>In this chart, I tried to recover from my own memory, and one of the medical image conferences the organizer presented a separation of all submissions into different topics. Of course, you can separate this submissions separately, but as far as I remeber, the separation is approximately like this. You can see that, <u>segmentation</u> actually occupies the majority of submissions which are presented at conferences. So <u>this is a topic which attracts the most interest in the community</u>.</p> <h5 id="reconstruction">Reconstruction</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/5ctIAih.png" alt="image-20230923064608513" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Topics of interest - Reconstruction </div> <p>When the 3D images of human body are required, the human body are typically positioned inside a tubular room, and the detector goes around the human body, spins around and acquire a series of 2D image projections. <u>And these images are stacked one after another into 3D volume</u>, which further doesnâ€™t look like a 3D volume of human body. It looks like the image on the left, which is the <u>spectrum</u> of the image. In CT, itâ€™s called <u>sinogram</u>. And there is an <u>algorithm, which can then reconstruct</u> the true 3D volume from this sinogram, and there is a theory which says, <u>it is always possible for a perfect accurate sinogram to reconstruct a perfect accurate 3D volume</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/G33TMox.png" alt="image-20230923070541523" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 3. Spectrum </div> <p>However, in real life, we <u>never have a perfect smooth and complete image of spectrum or sinogram</u>. And the reason is very simple, <u>we can not acquire images at every possible step</u> (degree) when the detector spins around the human body. And we have to do some sacrifices. We have to acquire images from some steps, and due to these steps, <u>due to a lower number of images acquired</u>, we first of all <u>speed up the acquisition time</u> where reduce the radiation dose delievered to the patient. For example, we require CT images, and at the same time, we kind of <u>make our CT scanners less expensive</u>.</p> <p>And <u>the problem what we are facing is</u>, <u>what is the minimum number of images need to reconstruct to another anatomy</u>. It turns out that currently, if we cannot use really record high-quality motion life images which we may need sometimes. For example, to record how heart moves, how human breaths, and so on. And this problem <u>results in two options</u>. Either we have <u>a poor quality image with motions</u>, or we cannot acquire motion at all. We require <u>static images, which can be affected by motion artifacts</u>. The researchers start to think, what if we acquire less images, <u>what if we start from a poor solution</u>, poor quality images, and try to develop algorithms. Knowing something about human anatomy, try to <u>develop algorithms to improve image reconstruction</u>. Maybe we can use the existing scanners, poor quality scanners, and see if we can get good images with using computational methods. And here is some example from work from Nvidia where they only take 10% of 2D projection images generated by a MR scanner, and apply <u>deep learning to generate a reconstruction</u> (center above). So you can see that, by using existing techniques, it can work well.</p> <h5 id="synthesis">Synthesis</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/bJBcsRU.png" alt="image-20230923072252122" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Topics of interest - Synthesis </div> <p>The next topic of interest is <u>image synthesis</u>. The idea of image synthesis is to <u>generate artificial image modalities from other modalities</u> of the same patient.</p> <p>Letâ€™s say we have <u>a CT image of a patient</u>. And <u>image synthesis algorithm try to generate a MR image of a same patient in the same position having in the CT image</u>. Why is this useful?</p> <p>Letâ€™s say we have <u>a solution with toolbox</u>, which require CT and MRI images to <u>diagnose the disease</u>. And letâ€™s say we install this toolbox in a place where there is no MRI scanner. In such a place, we may <u>either wait when the MRI scanner gets installed too so that we start to use this toolbox</u>. Or we can <u>apply the synthesis to generate the artificial MR scanners</u>, and try to use this toolbox. This is one of the way how synthesis can be used.</p> <p>Another way of synthesis where synthesis is useful is <u>detection of abnormalities</u>. Letâ€™s say we have a CT image of a patient. As you can see from the illustration (above), some of the structures of the brain, some of them are soft tissues are very poorly visible in CT. And this means, if there is a small tumor, itâ€™s possible that we will not even see it in the CT image. At the same time, this tumor will be very well visible in MR. And what we can try to do is we can try to use CT image if we have both CT and MR images of the same patient, we can try to take a CT image and generate a diffierential MR image. During this generation, that newly generated MR, will have no tumor because the tumor is almost invisible in CT, because the algorithm doesnâ€™t actually know where is the tumor. And it will generate us a more or less clean image of overhead. And when we subtract our true MR from the artificially synthesised MR, it is possible that tumor will be highlighted because there is no tumor under artificial synthesised MR image, there is one in the true one.</p> <h5 id="image-guided-interventions">Image-guided interventions</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/use2VlQ.png" alt="intenvention" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 4. Topics of interest - Synthesis </div> <p>Another topic of interest is a very large one, it is <u>Image-guided procedures</u>. Usually, image procedures require the use of many combinational tools, including image segmentation, and image registration. The key think that we separate this topic of interest is that they are much centered at a specific procedure. They are very target. Actually protocal is procedure. And one good example is image-guided procedure, image intervention is insertion of medical screws into the patient vertebra.</p> <p>Why do we even need this procedure? Some patients have very severe form of scoliosis or we have spine damage. In order to fix the spine curvature, it might be necessary to insert screws into the patientâ€™s spine. And put a back cord for the screws so that the spine will be tilted and straighted. According to the anatomical requirements of human body, and insert screws is a procedure will actually actively straighten this spine and allow the patient to function properly. When we do this insertion, we need to be very careful that the screws do not go into wrong place. First of all, the screws should not break the vertebra. If we break the vertebra, it is possible that we instead of help patient but more serverely than he was already injured. Another problem is that the screws should not go into the spinal cords. We should be very careful that screws go into the spinal cords, otherwise, the outcome might be devastating. And also, the screws inserted properly may get loosen, so the procedure should be repeated again. To answer all these questions, we need to analyse images, we need to see how the anatomy of the patient is organized. In order to position these screws perfectly through particles of vertebra.</p> <p>There are many other applications of Image-guided interventions, for example, epidemic treatment, femoral head replacement, and some surgeries.</p> <h5 id="computer-aided-diagnose">Computer-aided diagnose</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/R7C89xq.png" alt="image-20230923090634240" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 5. Topics of interest - Computer aided diagnose </div> <p>One of the topics that has been gaining a lot of interest recently is computer-aided diagnoses. For example, it could be a paper which was published in nature on skin image analysis by stanford. The idea here is that they take an image of the skin, take photos of them, and pass them for neural network, try to predict different skin dieases. Other applications could be prediction of diabetes, and some other applications like prediction of long pathologies from just xrays.</p> <h5 id="registration">Registration</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/eKF7zGm.png" alt="image-20230923091229536" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 6. Topics of interest - Image Registration </div> <p>During our course, you will have several lectures of image registration, because if not knowing what image registration is or how it works, it is impossible to work in the field of medical image analysis.</p> <p>And the image registration, what it tries to do is, it tries to find a transformation from one image to another. As you can see in this example, what we try to do is weâ€™ll try to take the image which is located at the left up top corner, and slowly step by step with deform it so that the it can come to the image located at the right bottom corner. And we slowly step by step deform it until it turns into again into the image at the left top. And the practical accountability of image registration is significant. One of the ways we can use registration is to monitor disease progression.</p> <p>Letâ€™s say the patient has been imaged, and the doctor prescribed him a treatment. And after some time, after several months, he would like to follow up how this diease is managed by treatment. And letâ€™s say these dieases affect some structures in the human body, maybe itâ€™s a tumor, which should shrink. Letâ€™s see if the treatment is chemotherapy or regular therapy or maybe it is a disease, like brain, some degenerative diease in the brain like dementia, which results in degradation of certain brain structures. And what you would like to know is quantatively assess how much the tumor or some brain starts just change. What we would do is to take the image of the patient which came at the time of diagnosis. Annotate the structure there, and deform it towards a patient image which we see after at the follow-up time. A lot of changes will tell us how fast it is progressing and whether itâ€™s a treatment has been affected. Another way to use image registratin is to segment structures of interest through registration.</p> <p>Letâ€™s say we have a patient whose structure inside his body has been segmentated manually and very lightly. Weâ€™re interested to segment the same structure on a new image. What we can do is we can manually do it take a instrument like paint brush in some toolbox, and manually outline the structure which is very time-consuming and might be imprecise. But the other way of doing it is to take an image which we have and deform this image of other patients towards our new image and simultaneously deform the structures. Use these deformation to understand how structures are depicted.</p> <h4 id="segmentation-applications">Segmentation Applications</h4> <p>After familiarizing you with a main topics of interest in the field, let me go directly to image segmentation.</p> <hr/> <ul> <li>Process of partitionig an image into distinct regions</li> <li>Why do we need segmentation in computer vision or medical imaging</li> </ul> <hr/> <p>So topic which receives the most attention by researchers, the topic which used the most and contribute the most, and different applications in the field.</p> <p>What definition for segmentation is basically personalization of the image into meaninful regions. And if you think about it, such personalization is extremely valuable, not only in medical imaging. Everytime when we analyze image, when we try to understand what is the depicted in the image, we perform segmentation. At least we can perform segmentation at least what is depicted.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/6UVfnkP.png" alt="image-20230925192552726" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 7. Segmentation </div> <p>Letâ€™s say here is an image of a dog. To understand this is a dog, what we can actually apply an algorithm can develop analogy which will segment which will basically contour the dog on even images, and <u>this contouring can be used to actually find out where the object is located</u>, what is in the background and so on.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/TFxtpS1.png" alt="image-20230925203509741" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 8. Self-driving </div> <p>Another application a more close to industry application is <u>self-driving cars</u>. These days like self-driving cars, is really flying there and not much time will pass before the self-driving cars will become the main way of transportation. And the way the car, the algorithm inside the car understands where to turn and how to interact. It usually requires <u>segmentation</u>, the algorithm needs to know <u>what happens around it</u>. Where are other cars? Where are the traffic lights? Where is the pedestrians? Where obstacles like shops or buildings. To do this, we need <u>segmentation</u>. We need to take an image around us and partition into meaningful regions in order to understand what happens around us.</p> <h5 id="automation-of-manual-work">Automation of manual work</h5> <p>One of the ways we can use segmentation is for <strong>automation of manual work</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/ZYZB2Mz.png" alt="image-20230925203703772" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 9. Automation of manual work </div> <p>In medical image, it is sometimes needed to segment organs to understand somethimg about human anatomy. And if you can see that many different areas, this is one area where this is done the constantly old time and takes a lot of time over for human. And this area is <u>radiation therapy planning</u>.</p> <p><u>Why the segmentation is needed there?</u></p> <p>Here is an example of two radiant therapist, one is having neuron therapy and one is liver radiant therapy. What we do when we deliver radiation to patient? What we do is we donâ€™t only radiate tumor but actually radiation <u>does not differentiate between tumor and healthy structures</u>.</p> <p>So if we shoot radiation beam, it will pass through the human body and it will radiate everything on its way. <u>How can we be sure that we kill the tumors selves but actually do not kill or minimally damage the surrounding issues?</u></p> <p>The way to do it is actually to <u>use several beams to shoot from different directions</u>. In such way that <u>all of these beams face centered to the tumor</u>, so that radiation delivered to surrounding structures is somehow really distributed. We <u>have high dose delivered to the tumor</u> and <u>not such a high dose delivered to all surrounding structures</u>.</p> <p>When we do this, we still want to be sure that <u>no critical structure is damaged</u>. This is something very difficult to achieve. Letâ€™s say a tumor is located very close to some critical structures like in the brain or close to spinal cord.</p> <p>Does it mean that we can simply position beams equidistantly around the human body and shoot as long as we deliver enough dose to the tumor, do not care how much dose is distributed surrounding structures?</p> <p>Well actually, thatâ€™s not true. <u>It turns out that different human organs have different radiation doses activity</u>. Letâ€™s say if we irritate spinal cord, itâ€™s actually way worse in contrast to irrigating the same volume of the ear. All of these relationships is divided of different organs has been carefully evaluated during the last couple of decades by oncologists and they established a set of rules saying that 30% of liver who received those below certian level or tge maximum dose delivered to spinal cord should be below such and such level.</p> <p>But how we can be sure that this constraints are satisfied?</p> <p><u>The way to do it is to segment all organs which are at risk to receive a dose.</u> In the image, segment tumor and plan the treatment before I actually shooting the doses plan treatment and see, what is the volume? How much dose each organ receives? Have the segmentation, we can measure what is the volume in certain millimeters of inhibitors of what is the volume? What is the dose each organ receives and this segmentation of organs is performed continously in the medical field is doing every day now and it takes hours to do this. And many companies like Philips, they focus on developing solutions for <strong><u>segmentation of organs for radiation therapy planing</u></strong>.</p> <h5 id="computer-aided-diagnosis">Computer-aided diagnosis</h5> <p>Another area of interest is computer-aided diagnosis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pojc5mU.png" alt="image-20230925213407477" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 10. Heart segmentation </div> <p>In before, the radiation therapy is treament planning. Here, we donâ€™t know if patient is sick or not, but we can use segmentation to do that. We can use segmentation to estimate if the patient is sick. For example, a cardiacally enlargement of heart can be detected on X-ray images. To do that, what they do is they matter these two lines like basically you can see in the center of an image. A measure the length of this, the height of a heart and the width of the heart in X-ray and from this measurements. They can say if a patient has a normal heart. <u>However, instead of measuring this two lines, it's way better and it might even be more precise and useful to actually segment the whole heart and estimate this volume extremely from segmentation instead of measuring it's height and width</u>. And here, we need segmentation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/FJH1ETW.png" alt="image-20230925213407477" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 11. Computer-aided diagnosis </div> <p><u>Another example is from detection of a vertebra block fractures</u>. When we age, our bones weaken and some vertebra these can collapse. And in some cases, we need to perform a treatment, we need to maybe strengthen the vertebra. If the patient is going to develop very serious complication affect the vertebra about to break or itâ€™s too weak.</p> <p>The question is how do we estimate the state of the vertebra?</p> <p>What the doctors do is they take 3D image of a patient and they measure some metrics, they measure height of vertebral points at the front area in the middle and at the back. And there are tables which says what should be the height and what should be the propulsion of these heights respect to each other for healthy vertebra and for pathological vertebra.</p> <p><u>Instead of measuring these distances manually, what we can do is we can apply an algorithm for measuring them automatically.</u> And such algorithms actually perform better than human because instead of taking one point in the image, they can segment the whole vertebra. And measure the height of the level of compression more comprehensively, not just from one distance.</p> <p>Another application of computer aided diagnosis is, for example, <u>writing an automated reports</u>. Letâ€™s say we have a lung field image. And the doctor or maybe a machine annotated pathologist on this image, for example, annotated regions which are suspicious to be cancer or some other opacities or pneumatics or pneumonia or covid. And they would like to generate an automated report from this, what will may need is actually segmentation of lungs in order to say where exactly the pathology is located. From segmentation, we can say that, the pathology is located at the lower part of our right lock or at the upper lobe of a left clunk. And for this, we need segmentation.</p> <h5 id="image-guided-procedures">Image-guided procedures</h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/YfisbTL.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 12. Image-guided procedures </div> <p>Here is the screenshot from the software from <u>planning dental implants</u>.</p> <p>When a patient comes for the dental implant, itâ€™s especially if implant requires insertion of screws into paitentâ€™s jaw and the 3D image of patient jaw is required. And my doctor, when he sees how a patientâ€™s jaw looks like, and he sees the place for the implant to be inserted.</p> <p>He faces a couple of challenges, one of them is he needs to design an implant wihch will fit patients jaw perfectly. One way to do is, to have a collection of teeth, basically, image collection of different teeth. And take an average teeth from population, average shape for a specific tooth from population, and put it artificially into the jaw, adjust the shape of his tooth towards to the patient. Unfortunately this procedure is very lengthy because average tooth will never fit very well to a specific patient. We have very different tooth.</p> <p>The way better strategy is to <u>use the patients anatomy to generate an implant</u>. What we can do is we can use a <u>symmetry of patient's jaw</u>. It is known that a specific tooth will be very similar to the same tooth on the other side of the jaw. So what we can do is we can apply segmentation algorithm, extract the same tooth from other parts of the jaw. And then we can reflect it and position, then start to adjust. This usually require way of less time.</p> <p>Another thing which should take into account is when we insert screws into the jaw, we should not injure the nerves. Otherwise, the patient will have serious complications, or his muscles in jaw get paralyzed. And here we can do segmentation to be sure to understand how this nerve is going.</p> <h5 id="motion-analysis">Motion analysis</h5> <p>Another area of significant interest is motion analysis.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/TkAscm9.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 13. Motion analysis </div> <p>It is often the case then we image some moving structures like heart or lungs, we would like to know how exactly they move. We donâ€™t only want their static appearance but also dynamic appearance. And there are image modalities which allows us to do that. For example, we can do that with MR images which captures the motion.</p> <p>ANd when we acquire such an image, it might do very beneficial to understand the anatomy, to understand the move patterns, and with the segmentation for that. For example, here on the right there is segmentation of heart, then we use heart segmentation to model how blood travels inside of a heart so that to understand if there is any problem of heart. If any vessel is weakened, and then we need segmentation to do that.</p> <h4 id="segmentation-methods">Segmentation Methods</h4> <p>There are already a lot different versions of segmentation methods, and segmentation has been developed for many many years now. The common and best way to separate the methods into two categories is,</p> <ul> <li>Supervised -&gt; Annotated Database</li> <li>Unsupervised -&gt; Using common sense without use annotation manually</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/RT3wiJ2.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 14. Segment Inspiration </div> <h5 id="thresholding">Thresholding</h5> <p>This is some technique involved in the course <a href="https://liuying-1.github.io/blog/2023/histogram"><em>Signal and Image Processing</em></a>, check it in detail to help recall.</p> <ul> <li>Histogram of intensity distribution</li> <li>One or multiple thresholds are used for classifying image pixels into sub partitions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/C7h6mPg.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 15. Thresholding </div> <p><strong>Thresholding: gaussian mixture model</strong></p> <ul> <li>Usually, you need to know in advance how many gaussians you need</li> <li>A good initial guess can help significantly</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/Dv1bS5L.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/QHBgmXz.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 16. Intensity Histogram and original image </div> <p><strong>Gaussian Mixture Model (GMM)</strong></p> <p>A Gaussian Mixture Model is a probabilistic model that represents <u>a mixture of multiple Gaussian (normal) distributions.</u> In image segmentation, GMM is used to <u>model the distribution of pixel intensities in an image</u>. <u>Instead of assuming a single global threshold</u> like in simple thresholding, GMM assumes that <u>pixel intensities</u> in an image <u>come from a mixture of several Gaussian distributions</u>.</p> <p><strong>Why use GMM?</strong></p> <ul> <li><strong>Complex Distributions</strong>: GMM is <u>more robust</u> when dealing with complex, <u>multi-modal intensity distributions</u> in an image. It can <u>model situations where the pixel intensities don't follow a simple bimodal distribution</u>.</li> <li><strong>Flexibility</strong>: It allows for a more flexible and probabilistic approach to modeling image data, which can be particularly useful when objects in an image have varying intensity characteristics.</li> <li><strong>Adaptive Thresholding</strong>: GMM can adapt to local variations in intensity, making it useful for images with non-uniform lighting conditions.</li> <li><strong>Soft Segmentation</strong>: GMM can <u>provide soft (probabilistic) segmentations</u>, where each pixel is assigned a probability of belonging to different classes. This can be useful in cases where hard segmentation may be ambiguous.</li> </ul> <p>In summary, Gaussian Mixture Models are used when the intensity distribution in an image is not well-suited for a <u>simple global thresholding approach</u>. GMM provides a more flexible and probabilistic way to model the pixel intensity distribution, which can lead to better segmentation results, especially in cases with complex or non-uniform intensity distributions. However, GMM is <u>computationally more expensive</u> than thresholding, and the choice between these techniques depends on the specific characteristics of the image and the requirements of the segmentation task.</p> <ul> <li>Expectation Maximization (EM) algorithm</li> <li>Iterative improvement of log-likelihood function</li> </ul> <p>The Expectation-Maximization (EM) algorithm is <u>a statistical iterative optimization technique used for finding maximum likelihood estimates of parameters in probabilistic models</u>, especially when dealing with incomplete or missing data. The EM algorithm consists of <u>two main steps</u>, the <u>Expectation</u> (E-step) and the <u>Maximization</u> (M-step), which are <u>iteratively performed until convergence</u>. Hereâ€™s an overview of how the EM algorithm works:</p> <ol> <li><strong>Initialization</strong>: Start with an <u>initial guess</u> for the parameters of the model.</li> <li><strong>Expectation (E-step)</strong>: <ul> <li>Compute <u>the expected value (expectation) of the log-likelihood of the data</u> with respect to the <u>current estimate of the parameters</u>. This step involves estimating the values of hidden or unobserved variables, given the observed data and the current parameter estimates. It is called the â€œE-stepâ€ because it computes the expected values of these hidden variables.</li> <li>These expected values are also called the â€œposterior probabilitiesâ€ or â€œresponsibilitiesâ€ and represent the probability that each data point belongs to a particular component of the mixture model.</li> </ul> </li> <li><strong>Maximization (M-step)</strong>: <ul> <li>Update the model parameters to maximize the expected log-likelihood computed in the E-step. This involves <u>finding new parameter estimates</u> that make the observed data more likely given the estimated hidden variables.</li> <li>In many cases, this step involves solving optimization problems to find the best-fitting parameters.</li> </ul> </li> <li><strong>Iteration</strong>: <ul> <li>Repeat the E-step and M-step iteratively until the algorithm converges. Convergence typically occurs when the change in the estimated parameters between iterations becomes small.</li> </ul> </li> <li><strong>Final Estimates</strong>: <ul> <li>Once the EM algorithm converges, the final estimates of the model parameters are obtained.</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DmxDXO0.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/P7i535h.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 17. EM algorithm and Resulting segmentation </div> <h5 id="region-growing">Region growing</h5> <p>What we do with thresholding? We actually do not take into account the neighboring pixels. So the region growing is based on that if two pieces are neighboring, two neighboring pixels <u>have similar intensities</u>, they probably belong to the same object.</p> <p>And what <u>region growing</u> does is <u>it's simply adds pixels one by one and merge together in order to get homogeneous regions</u>. Despite being very simple, it can be very useful and relatively accruate in some specific situations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DoWc2Az.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 18. Region growing Lung Fields </div> <p>Letâ€™s see we have an example of this lung fields. We can try to <u>have some seeds</u>, see the regions which are marked in green and <u>slowly add pieces of the pixel surrounding lungs</u> and <u>add into the mask fluxes</u>.</p> <p><strong>Neighboring Pixels</strong></p> <hr/> <ul> <li>You will <u>need seed pixels</u> that are known to belong to the object (objects)</li> <li>Add <u>neighboring pixels</u> $p_t$ as long as $P(p_{t-1}, p_t)=1$</li> <li><u>Iterate</u> until no further pixels could be added</li> <li><u>Move to new seed pixels</u> if there is any</li> </ul> <hr/> <p>The basic idea of region growing is connectivity. In 2D, there are two types of connectivity,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/NDoHGoW.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 19. Neighboring Pixel </div> <ul> <li> <p>4-connectivity</p> </li> <li> <p>8-connectivity</p> </li> </ul> <p>We can extend the connectivity into this <u>distance-based connectivity</u>, you can see in the same way that pixels are connected <u>if a distance between them is less than 2 millimeters</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/sxsnJ2U.png" alt="image-20230925223731903" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 20. Distance-based connectivity </div> <p>In this way, we will <u>add more pixels</u> into our neighborhood.</p> <p>The original region growing is extremely simple. We have <u>seed points</u>, and we have a rule which says that <u>if two neighboring pixels have distance less than $x$</u>. They belong to the same class. And what we do is we <u>slowly iteratively expand our seed pixels</u> until we cannot expand any longer.</p> <p>Letâ€™s look at this toy example.</p> <ul> <li>$P(x, y) = \mid x-y\mid \leq 3$</li> <li>We do not need to have both seeds; the result will be the same</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/UBP64xH.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 21. Toy Example </div> <p>In the first, we have a pixel $1$ with intensity $1$, marked as dark gray. And pixel $7$ marked as light gray. And the expansion rule is that if neighboring pixels have distance less than $3$, then we connected together.</p> <p>In the first iteration we add all neighbors to pixel $7$ because the distance betweem them is less than $3$, and we also add some numbers to pixel $1$. And we can just continuous process until we cannot add any more pixels and we get a pretty logical separation of darker object at the left of our image and brighter object at the right of our image.</p> <p><u>Region growing works good only in very specific circumstances</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/AnDvxYO.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 22. Decent region growing </div> <p>For example, segmentation of lungs. You can see that when we give some seeds inside lungs, and we apply region growing and the results are pretty decent.</p> <p>Of course, <u>region growing is imperfect</u>. It can <u>not work well for complex anatomy</u>.</p> <p>For example, here is an attempt of segmenting lung cells and vertebra by using region growing. Due to its similicity alogirthms, it cannot usually produces somooth anatomically relevant borders. So you can see <u>it suffer from leaks</u>, and it also has a <u>problem of noise</u>. If there is a noisy pixel, it cannot penetrate it.</p> <hr/> <p>Problems:</p> <ul> <li>Leaks</li> <li>Noise</li> </ul> <hr/> <p><strong>Split/merge</strong></p> <p>Another way of doing region growing is approach it from different direction instead of adding pixels to our seeds.</p> <p>What we can try to do is we can <u>separate image into pieces slowly into most dissimilar pieces</u>.</p> <p>For example, we label our whole image as one object. And the first iteration, what we try to do is <u>separating this image into four squares</u>. And we see <u>how similar with this average intensity individual squares.</u> If average intensity is <u>similar, we keep the squares to be the same object</u>. But if is <u>too different, we separate them into pieces</u>. And then we go into each of the squares, and try to <u>separate it further</u>. And we <u>continue until we cannot separate any longer</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pFz9xlY.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 23. Quadtree </div> <hr/> <ul> <li>First, the whole image is one initial region</li> <li>In each iteration: <ul> <li>Merge neighboring regions that are too similar $P(R, \hat) = 1$</li> <li>Split regions that are too different $P(R,\hat{R})=0$</li> </ul> </li> <li>Stop when no split or merge can be performed</li> </ul> <p>$P(R, \hat{R})$ can be based on the standard deviation of the pixel intensities.</p> <hr/> <p>For the reason of simplicity split/merge happens according to the image quadtree:</p> <ul> <li>For pixels in a $2\times 2$ square belong to a split node of a higher level</li> <li>For squares belong to a split node of a higher level etc.</li> </ul> <p>In such a way, we can get a three potential separations at three of different connected and disconected regions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/x78VJvd.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 24. Split and Merge Example </div> <p>Here is an example of how this algorithm work.</p> <p>On the left is the result of applying of starting from the labeling the whole image to be one object. And then starting separating the image into squares, and connected each other. However, the problem with such idea is that <u>once we separated to rectangle regions, they cannot anymore merge together.</u> And it might be needed, for example, our rectangle boxes, they never pass perfectly at the border of different structures and it might be needed to go back into some levels and merge some smaller regions together.</p> <p><u>We call it split and merge region growing where we not only separate them into smaller pieces but we can merge them back some of the smaller pieces back if needed.</u></p> <p>And you can see that a separation with split and merge region growing is better than the one with only split region growing.</p> <p><strong><em>You do not have to use thresholding or region growing on raw image intensities.</em></strong></p> <p>The thing about the region growing and thresholding is that these days we donâ€™t use them on raw image intensities because organs are much more complicated than just regions of homogeneous region of specific intensity. But what we can do is we can apply a more sophisticated algorithms, for example, deep learning to enhance organs which we are interested about.</p> <p>For example, hereâ€™s is the enhancement of kidney. And then when we enhance the kidney, we can apply thresholding. To analyse this enhanced image, itâ€™s very easier to separate the background from object on the enhanced image instead of separating the kidney on the raw image.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/85DDZnK.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 25. Auxiliary method </div> <p>And this is how thresholding is currently used. <u>It's more used to analyse results of more sophisticated algorithms as an auxiliary method to analyse this.</u></p> <h5 id="other-similar-segmentation-methods">Other similar segmentation methods</h5> <ul> <li>Watersheds</li> <li>Level sets</li> <li>K-means</li> <li>Various thresholding (Otsu, Huang, etc)</li> </ul> <h4 id="example">Example</h4> <p>Letâ€™s have a problem of lung fields segmentation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/EuGOPR4.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/W4rM9tt.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 26. Lungs Segmentation </div> <p>Letâ€™s say we have a 3D image of lung fields of a human body and our aim is to segment lung fields from this image. And the lung fields are darker than other structures around. Considering this fact, what we can do is we can use <u>thresholding to separate the lungs</u>. So we can take a histogram and use this histogram to separate <u>according to the first valley which separates air from soft tissues</u>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/MdVwUIk.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> <img src="https://i.imgur.com/RuvoEre.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 27. Resulting image of first valley </div> <p>And here is what we get, itâ€™s perfect but itâ€™s already somehow decent trade. Letâ€™s see if we can improve it by using some simple logical ideas which come down in mind.</p> <p>So here is an illustration of 3D render of our segmentation. Although it looked very nice on the slice we saw on a cross section, weâ€™ve tried to visualize the threshold of mask in 3D, what we see is that apart from the lung fields which are full with air, we also segment that the air outside the human body which doesnâ€™t belong to human body.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/NzGLYB0.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 28. 3D illustration </div> <p>So the first thing we need to think about is how to remove this air outside the human body?</p> <h4 id="must-have-tools">Must-have tools</h4> <h5 id="connected-component-decomposition">Connected component decomposition</h5> <p>And the way to do is by using a tool which is called <u>connected component decomposition</u>.</p> <p>So if you look again to our example fo segmentation of lung fields, what you can see is that the air outside the human body is a very large connected object. So when all areas connected to each other always the air outside and its size is quite significant. And at the same time, lungs are also significant size. And when you look more closely into the image, you find out that not only the air outside the lung fields a human body is also segmentated by thresholding but also air in the stomach is selected. Also air around some image artifacts is also segmented.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/pv7Nlcw.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 29. 3D illustration with useless air </div> <p>And the way to solve it is to use anatomical knowledge which we have prior to solving, to addressing this problem. And what we know is that the air outside with very large volume. And the air outside which was supposed to the largest which we will see in the image. Then, the next largest object is lungs. So for lungs, as the second largest acquistion of air inside of a human body. So what we can do is to find the object which is the second largest and hopefully this object will be lung fields.</p> <p>To do that, we need an algorithm which is called <u>connected component decomposition</u>.</p> <p>What the algorithm tries to do is tries to connect and label the same label all pixels in the mask of we have valued $1$ and connect with which are connected to each other using 8 or 4 connectivity in 2D or 27 or 6 connectivity in 3D.</p> <hr/> <p>Separating segmentation results into regions:</p> <ul> <li>Removes noise from segmentation</li> <li>Separates segmentation into regions that can be further analysed</li> </ul> <p>We have a binary image (0-background)</p> <p>Connected component:</p> <ul> <li>All pixels have intensity value 1</li> <li>There is a path between any two pixels using 4-connectivity or 8-connectivity</li> <li>No pixel can be added</li> </ul> <hr/> <p>Here is a little bit of algorithmical depth of connected component decomposition.</p> <p>It will be skipped as it is not listed in the learning outcomes.</p> <h5 id="morphological-dilationerosion">Morphological dilation/erosion</h5> <p>The results probably not that satisfied because the lung fields are not only the air. The lungs also contain vessels inside of them. And these vessels will not be segmented by thresholding. However, we still would likt to add these vessels into our mask, then the segmentation will be complete.</p> <p>And the problem is that in some cases, especially in X-ray images, the air in the stomach comes very close to lung fields and sometimes it may come be very connected to lung fields and our lung fields mask can sometimes leak inside of a stomach. And we donâ€™t want these to happen.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/R9YtBEe.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 30. Problems </div> <p>So here is an example of the vessels inside of lung fields, and also example of artificial leak. We donâ€™t want them to be present in our mask. And the way to do it is to apply mathematical operations.</p> <p>There are two types of operations, one is morphological <strong>erosion</strong> and the other one is morphological <strong>dilation</strong>.</p> <p>So the erosion is <u>removes one layer of pixels from our mask</u>. So <u>our masks shrinks one layer inside</u>. So <u>we cut one layer from our mask, for every pixel which has at least one neighbour which is not mask will remove this pixel.</u></p> <p>For dilation is opposite, what it does is, it adds a few layer of pixels into our mask. So for every known mask pixel, which has a mask pixel in our original mask for every such pixel, we include this pixel into our mask.</p> <p>What happens when we apply the erosion and dilation, we donâ€™t return to the original mask.</p> <hr/> <p>Informally:</p> <ul> <li>Dilation - expanding of binary mask</li> <li>Erosion - shrinking of binary mask</li> </ul> <p>Dilation/erosion is useful:</p> <ul> <li>To remove all internal noise pixels in segmentation mask</li> <li>To remove all boundary artifacts</li> <li>To smooth the boundaries</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/cnTll6C.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 31. Explaination </div> <hr/> <p><strong>Algorithms</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/2R64uYF.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 32. Erosion </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/DsRTkbD.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 32. Dilation </div> <p>So, letâ€™s look at this example.</p> <p>We have an original mask which looks like this.There is a hole inside, value $0$. Letâ€™s see what will happen if we apply the dilation around this mask.</p> <p>All pixels which had value $0$ in our original image $I$, all pixels which had value $0$ and at least one neighbor which has 1 in the 4 neighborhood, they all turn into $1$ into our new mask. =&gt; Expand our original mask a little bit.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/jFbdP0K.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 33. Dilation -&gt; Erosion </div> <p>So for every pixel which has a label 1 and at least one pixel is neighborhood is 0, and also consider the image border to have 0 labels. We turn this pixel into 0. If it has one neighborhood which is 0 and then we turn it into 0. So all these pixels at the border may turn into 0 because we have at least on neighborhood which has 0. But the pixel which it didnâ€™t turn back into 0 is the one which is in the middle. It was 0 in the very begining, but after the dilation, all its neighbors now are 1. So it cannot anymore turn into 0.</p> <p><u>So what we get is after applying dilation over, we've got almost the same mask except one pixel in the middle, except for one noisy pixel in the middle, which we turn into one by these operation.</u></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/LUP1lR5.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 34. Erosion -&gt; Dilation </div> <p>Letâ€™s see what happens if we look at the opposite situation.</p> <p>All the pixels which have at least one pixel with label 0 as neighbor will be removed, including this guy who is close to border. After this, we will only have two pixels left as label 1. Then, we play dilation. The result shows that compared to the original one, something bottom is not needed for us will be removed.</p> <p>In some situations, we also lost some pixels and for such a small mask, itâ€™s a significant loss. But you can imagine in our real life, our mask is very larger. So losing only some pixels at the borders will not be that critical in comparison to removing the noise and the comparison to remove the leaks which we donâ€™t want to have.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/vIo7Jsn.png" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 35. Result of dilation -&gt; erosion </div> <p>5 times dilation and then 5 times erosion. =&gt; we add all the vessels into our lung field mask, and then the borders of our mask get smoothed. Some inhomogeneity of border is disappeared.</p> <p><u>So we not only included the vessels into our mask, which we wanted the beginning, but we also get an opportunity to remove noisy at the borders.</u></p> <p>This note will be fixed in soon once I am not that busy.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="mia"/><summary type="html"><![CDATA[Learning note for medical image segmentation.]]></summary></entry><entry><title type="html">Inpainting</title><link href="https://liuying-1.github.io/blog/2023/inpainting/" rel="alternate" type="text/html" title="Inpainting"/><published>2023-09-19T08:00:00+00:00</published><updated>2023-09-19T08:00:00+00:00</updated><id>https://liuying-1.github.io/blog/2023/inpainting</id><content type="html" xml:base="https://liuying-1.github.io/blog/2023/inpainting/"><![CDATA[<p>Inpainting, as known as <strong>predict missing pixels</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/KLK2ZIN.png" alt="image-20230919201623320" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 1. Context Encoders: Feature Learning by Inpainting (Pathak et al., 2016) </div> <p><strong><em>Disclaimer: Below are the extraction from the original paper.</em></strong> It will be reposted once I finished read all of them when I am available.</p> <p><strong><em>Reference: Context Encoders: Feature Learning by Inpainting</em></strong></p> <hr/> <h4 id="abstraction">Abstraction</h4> <p>We present <strong><u>an unsupervised visual feature learning algorithm</u></strong> driven by context-based pixel prediction.</p> <p>Context Encoders - a <strong>convolutional neural network</strong> trained to <strong>generate the contents of an arbitrary image region conditioned on its surroundings</strong>.</p> <p>To succeed at this task, context encoders need to both <strong>understand the content of the entire image,</strong> as well as <strong>produce a plausible hypothesis for the missing parts</strong>.</p> <p>When training context encoders, we have experimented with both <strong>a standard pixel-wise reconstruction loss</strong>, as well as <strong>a reconstruction plus an adversarial loss</strong>.</p> <p>The <strong>latter produces much sharper results</strong> because it can <strong>better handle multiple modes</strong> in the output.</p> <p>A context encoder learns <strong>a representation that captures not just appearance</strong> but also the <strong>semantics of visual structures</strong>. ==&gt; <em>Question</em></p> <p>Context encoders can be used for <strong>semantic inpainting tasks</strong>, either <strong>stand-alone</strong> or <strong>as initialization for non-parametric methods</strong>.</p> <h4 id="introduction">Introduction</h4> <p>Our visual world is very diverse, yet <strong>highly structured</strong>, and humans have an uncanny ability to make sense of this structure.</p> <p>This work is to explore <strong>whether state-of-the-art computer vision algorithms can do the sam</strong>e.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://i.imgur.com/nFaEBX3.png" alt="image-20230919205428308" class="img-fluid rounded z-depth-1" data-zoomable=""/> </div> </div> <div class="caption"> Figure 2. Demonstration </div> <p>Although the center part of the image is missing, most of us can easily imaging its content from the surrounding pixels, without having ever seen that exact scene. Some of us can even draw it.</p> <p>This ability comes from the fact that <strong>natural images, despite their diversity, are high structured</strong>. In this case, the regular pattern of windows on the facade. We <strong>humans</strong> are able to <strong>understand this structure</strong> and <strong>make visual predictions</strong> even when seeing <strong>only parts of the scene</strong>.</p> <p>In this paper, we show that <strong>it is possible to learn and predict this structure</strong> using <strong>convolutional neural networks</strong> (CNNs), a class of models that have recently shown success across a variety of <strong>image understanding tasks</strong>.</p> <p>Given <strong>an image with a missing region</strong>, we train a convolutional neural network to <strong>regress to the missing pixel values</strong>. <em>Question</em></p> <p>We call our model <strong>context encoder</strong>, as it consists of <strong>an encoder capturing the context of an image into a compact latent feature representation</strong> and <strong>a decoder which uses that representation to produce the missing image content</strong>.</p> <p>The context encoder is closely related to auto encoders, as it shares a <strong>similar encoder-decoder architecture</strong>.</p> <p><strong>Autoencoders</strong> <strong>take an input image</strong> and try to <strong>reconstruct it</strong> after it passes through a low-dimensional â€œbottleneckâ€ layer, with the aim of <strong>obtaining a compact feature representation</strong> is likely to <strong>just compresses the image content without learning a semantically meaningful representation</strong>. ==&gt; Encoder has done the abovementioned, the decoder is to map back to the original datashape.</p> <hr/> <p>ChatGPT gives the following explaination to autoencoder.</p> <ol> <li>â€œ<strong>Autoencoders</strong>â€: Autoencoders are a class of neural network architectures used for unsupervised learning. They consist of an encoder and a decoder, and their <u>primary purpose is to learn a compressed representation of the input data</u>.</li> <li>â€œ<strong>Take an input image</strong>â€: Autoencoders typically operate on input data, which in this case is an image. The image is passed through the network.</li> <li>â€œ<strong>Reconstruct it</strong>â€: The <u>core task of an autoencoder is to take the input image</u> and attempt to <strong><u>generate an output (reconstruction) that is as close as possible to the original input</u></strong>. <strong><u>In essence, it tries to replicate the input</u></strong>.</li> <li>â€œ<strong>After it passes through a low-dimensional â€˜bottleneckâ€™ layer</strong>â€: Autoencoders have an intermediate layer called the bottleneck or latent space, which has a lower dimension than the input data. This layer forces the network to <strong><u>learn a compressed representation of the input</u></strong> data.</li> <li>â€œ<strong>With the aim of obtaining a compact feature representation</strong>â€: The primary objective of this compression process is to <strong><u>obtain a compact feature representation of the input</u></strong> data. This means that the autoencoder is trying to <u>capture the most important and salient features of the input</u> data in a condensed form.</li> <li>â€œ<strong>Is likely to just compresses the image content without learning a semantically meaningful representation</strong>â€: This part of the statement suggests <u>a potential limitation of traditional autoencoders</u>. In some cases, when using basic autoencoder architectures and simple loss functions like mean squared error, the learned compressed representation may indeed <strong><u>focus more on compressing the raw image content</u></strong> (e.g., pixel values) rather than <strong><u>capturing higher-level, semantically meaningful features</u></strong> (e.g., object shapes or meaningful abstractions).</li> </ol> <p>In other words, <strong>traditional autoencoders</strong> may prioritize achieving a low reconstruction error (<strong><u>faithfully reproducing the input</u></strong>) over learning abstract, semantic information. However, this limitation can be addressed or mitigated by modifying the architecture, loss functions, or training strategies, as mentioned in the previous response. <strong><u>More advanced autoencoder</u></strong> variants, like variational autoencoders (VAEs) or denoising autoencoders, <strong><u>are designed to encourage the learning of semantically meaningful representations</u></strong>.</p> <hr/> <p><strong>Denoising autoencoders</strong> address this issue by <strong>corrupting the input image</strong> and requirng the network to <strong>undo</strong> the damage. However, this corruption process is typically very <strong>localized and low-level</strong>, and does <strong>not require much semantic information</strong> to undo.</p> <hr/> <p>ChatGPT explains what are localized and low-level. :point_down:</p> <ol> <li>â€œ<strong>Localized</strong>â€: When it is said that the corruption process is â€œlocalized,â€ it means that the noise or distortion introduced to the input data <strong>tends to affect only a small portion of the data</strong>. In the context of images, for example, localized corruption might involve adding noise to individual pixels or small regions of the image. It does not involve large-scale changes or alterations to the entire image.</li> <li>â€œ<strong>Low-level</strong>â€: The term â€œlow-levelâ€ refers to the nature of the corruption. Low-level corruptions are simple and typically involve basic operations on the data, such as adding random noise, blurring, or altering individual data points (e.g., pixel values in an image). These corruptions do not carry complex semantic meanings or involve high-level abstractions. They are often applied at the level of raw data elements.</li> </ol> <p>In the context of denoising autoencoders, the idea is that <strong>the noise or distortions introduced to the input</strong> data are <strong>relatively simple</strong> and <strong>affect only small parts</strong> of the data. Because these <strong>corruptions are not complex or semantically rich</strong>, the network may <strong>not need to learn deep</strong>, <strong>high-level semantics to undo</strong> the effects of the corruption. Instead, it can focus on relatively shallow and low-level operations to recover the clean data. This is why the statement suggests that denoising autoencoders may not necessarily learn highly meaningful semantic representations since the corruption process itself is not very demanding in terms of semantic understanding.</p> <hr/> <p>In contrast, our context encoder needs to solve <strong><u>a much harder task</u></strong>: <strong><u>to fill in large missing areas of the image, where it can't get "hints" from nearby pixels.</u></strong></p> <p>This requires <strong>a much deeper semantic understanding of the scene</strong>, and the <strong>ability to synthesize high-level features over large spatial extents</strong>.</p> <p>For example, an entire window needs to be <strong>conjured up â€œout of thin airâ€</strong> in Figure 2 above.</p> <p>This is similar in spirit to wrd2vec which <strong><u>learns word representation from natural language sentences by predicting a word given its context</u></strong>.</p> <p>Like autoencoders, <strong><u>context encoders</u></strong> are <strong><u>trained in a completely unsupervised manner</u></strong>.</p> <p>Our results demonstrate that in order to succeed at this task, a model needs to <strong><u>both understand the content of an image</u></strong>, as well as <strong><u>produce a plausible hypothesis for the missing parts</u></strong>.</p> <p>This task, however, is inherently <strong>multi-modal</strong> as there are <strong>multiple ways</strong> to <strong>fill the missing region while also maintaining coherence with the given context</strong>.</p> <p>We <strong>decouple this burden</strong> in our loss function by <strong>jointly training our context encoders</strong> to <strong>minimize both a reconstruction loss and adversarial loss</strong>.</p> <p>The reconstruction loss (L2) <strong>captures the overall structure</strong> of the missing region in relation to the context, while the adversarial loss has the effect of <strong>picking a particular mode</strong> from the distribution.</p> <ul> <li>â€œ<strong>Reconstruction loss (L2)</strong>â€: This loss function measures how well the generated output (the completed or inpainted region) matches the original data. Itâ€™s often calculated as the mean squared difference between the generated output and the ground truth (original data). In other words, it captures how well the generated content resembles the actual missing region in terms of overall structure.</li> <li>â€œ<strong>Adversarial loss</strong>â€: This type of loss function is typically associated with GANs. It involves a discriminator network that tries to distinguish between real (ground truth) data and generated data. The adversarial loss <strong>encourages the generated data to be more realistic</strong> and closer to the distribution of real data. In this context, it is mentioned that the adversarial loss has the effect of selecting a particular mode from the distribution, which means <strong>it encourages the generated content to be more similar to specific instances of real data</strong> (modes) rather than just any possible output.</li> </ul> <p>So, the overall strategy described here is to <strong>simultaneously train context encoders</strong> to <strong>generate missing regions</strong> in a way that <strong>not only captures the overall structure</strong> (reconstruction loss) but also <strong>encourages the generated content to resemble specific instances of real data</strong> (adversarial loss). This approach aims to <strong>strike a balance between maintaining coherence with the context and producing realistic</strong>, data-driven details in the inpainted or completed regions.</p> <p>Figure 2 shows that using only the <strong>reconstruction loss produces blurry results</strong>, whereas <strong>adding the adversarial loss results in much sharper predictions</strong>.</p> <p><strong><u>Evaluate the encoder and the decoder independently.</u></strong></p> <p>On the encoder, we show that <strong>encoding</strong> just the <strong>context of an image patch</strong> and using the <strong>resulting feature</strong> to <strong>retrieve nearest neighbor contexts from a dataset</strong> produces patches which are semantically similar to the original (unseen) patch.</p> <ul> <li>After encoding the context of an image patch, the authors take the <strong>resulting feature representation</strong> and <strong>use it to find other image patches in a dataset that have similar feature representations</strong>. In other words, they <strong>look for other patches that share similar contextual characteristics</strong> based on the extracted features.</li> <li>The key result or observation here is that when they retrieve image patches from the dataset based on the feature representation of the context, the <strong>retrieved patches are found to be â€œsemantically similarâ€ to the original, unseen patch</strong>. This means that <strong>the retrieved patches share meaningful visual or structural characteristics with the original patch</strong>, even though the encoder processed only the context of the patch.</li> </ul> <p><strong>By encoding only the context of an image patch</strong> and using the <strong>resulting features to find similar contexts in a dataset</strong>, they can <strong>retrieve other patches that semantically similar to the original patch</strong>. This suggests that the context encoder is <strong>effective at capturing meaningful contextual information from images</strong>, even without considering the patch itself.</p> <p>We further validate the quality of the learned feature representation by fine-tuning the encoder for a variety of image understanding tasks, including classfication, object detection, and semantic segmentation.</p> <p>We are competitive with the state-of-the-art unsupervised/self-supervised methods on those tasks.</p> <p>On the decoder side, we <strong>show that our method is often able to fill in realistic image content</strong>.</p> <p>Indeed, to the best of our knowledge, ours is the <strong>first parametric inpainting algorithm</strong> that is able to <strong>give resonable results for semantic hole-filling</strong> (large missing regions).</p> <p>The context encoder can also be useful as a better visual feature for computing nearest neighbors in non-parametric inpainting methods.</p> <h4 id="related-work">Related work</h4> <p>Computer vision has made tremendous progress on <strong>semantic image understanding tasks</strong> such as classification, object detection, and segmentation in the past decade.</p> <p>Recently, <strong>Convolutional Neural Networks (CNNs)</strong> have greatly advanced the performance in these tasks. The success of such models on image classification paved the way to tackle harder problems, including unsupervised understanding and generation of natural images.</p> <p>We briefly review the related work in each of the sub-fields pertaining to this paper.</p> <h5 id="unsupervised-learning"><strong>Unsupervised learning</strong></h5> <p><strong>CNNs trained for ImageNet classification</strong> with over a million <strong>labeled examples</strong> learn features which generalize very well across tasks. However, <strong>whether such semantically informative and generalizable features can be learned from raw images alone, without any labels</strong>, remains an open question. Some of the earliest work in deep unsupervised learning are autoencoders. Along similar lines, denoising autoencoders reconstruct the image from local corruptions, to make encoding robust to such corruptions. While context encoders <strong>could be thought of as a variant of denoising autoencoders</strong>, the corruption applied to the modelâ€™s input <strong>is spatially much larger</strong>, requiring <strong>more semantic information</strong> to undo.</p> <h5 id="weakly-supervised-and-self-supervised-learning">Weakly-supervised and self-supervised learning</h5> <p>Very recently, there has been significant interesting in learning meaningful representations using weakly-supervised and self-supervised learning. One useful source of supervision is to <strong>use the temporal information</strong> contained in videos. <strong>Consistency across temporal frames</strong> has been used as supervision to learn embeddings which perform well on a number of tasks. Another way to use consistency is to track patches in frames of video containing task-relevant attributes and use the coherence of tracked patches to guide the training.</p> <p>Most closely related to the present paper are efforts at <strong>exploiting spatial context as a source of free and plentiful supervisory signal</strong>. Recently, Doersch et al. used the task of <strong>predicting the relative positions of neighboring patches within an image</strong> as a way to train an unsupervised deep feature representations.</p> <p>Our context encoder solves <strong>a pure prediction problem</strong> (what pixel intensities should go in the hole?).</p> <p>In contrast, <strong>context encoders can be applied to any unlabeled image database</strong> and <strong>learn to generate images based on the surrounding context</strong>.</p> <p>The later part in the article is not that related to my interests. I will repost this blog as soon as possible to form my own interpretation focus on the technique, <strong>inpainting</strong>.</p>]]></content><author><name></name></author><category term="study"/><category term="ucph"/><category term="ssl"/><summary type="html"><![CDATA[Learning note for inpainting.]]></summary></entry></feed>